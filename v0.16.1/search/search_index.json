{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#externaldns","title":"ExternalDNS","text":"<p>ExternalDNS synchronizes exposed Kubernetes Services and Ingresses with DNS providers.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>This README is a part of the complete documentation, available here.</p>"},{"location":"#what-it-does","title":"What It Does","text":"<p>Inspired by Kubernetes DNS, Kubernetes\u2019 cluster-internal DNS server, ExternalDNS makes Kubernetes resources discoverable via public DNS servers. Like KubeDNS, it retrieves a list of resources (Services, Ingresses, etc.) from the Kubernetes API to determine a desired list of DNS records. Unlike KubeDNS, however, it\u2019s not a DNS server itself, but merely configures other DNS providers accordingly\u2014e.g. AWS Route 53 or Google Cloud DNS.</p> <p>In a broader sense, ExternalDNS allows you to control DNS records dynamically via Kubernetes resources in a DNS provider-agnostic way.</p> <p>The FAQ contains additional information and addresses several questions about key concepts of ExternalDNS.</p> <p>To see ExternalDNS in action, have a look at this video or read this blogpost.</p>"},{"location":"#the-latest-release","title":"The Latest Release","text":"<ul> <li>current release process</li> </ul> <p>ExternalDNS allows you to keep selected zones (via <code>--domain-filter</code>) synchronized with Ingresses and Services of <code>type=LoadBalancer</code> and nodes in various DNS providers:</p> <ul> <li>Google Cloud DNS</li> <li>AWS Route 53</li> <li>AWS Cloud Map</li> <li>AzureDNS</li> <li>Civo</li> <li>CloudFlare</li> <li>DigitalOcean</li> <li>DNSimple</li> <li>OpenStack Designate</li> <li>PowerDNS</li> <li>CoreDNS</li> <li>Exoscale</li> <li>Oracle Cloud Infrastructure DNS</li> <li>Linode DNS</li> <li>RFC2136</li> <li>NS1</li> <li>TransIP</li> <li>OVH</li> <li>Scaleway</li> <li>Akamai Edge DNS</li> <li>GoDaddy</li> <li>Gandi</li> <li>IBM Cloud DNS</li> <li>TencentCloud PrivateDNS</li> <li>TencentCloud DNSPod</li> <li>Plural</li> <li>Pi-hole</li> </ul> <p>ExternalDNS is, by default, aware of the records it is managing, therefore it can safely manage non-empty hosted zones. We strongly encourage you to set <code>--txt-owner-id</code> to a unique value that doesn\u2019t change for the lifetime of your cluster. You might also want to run ExternalDNS in a dry run mode (<code>--dry-run</code> flag) to see the changes to be submitted to your DNS Provider API.</p> <p>Note that all flags can be replaced with environment variables; for instance, <code>--dry-run</code> could be replaced with <code>EXTERNAL_DNS_DRY_RUN=1</code>.</p>"},{"location":"#new-providers","title":"New providers","text":"<p>No new provider will be added to ExternalDNS in-tree.</p> <p>ExternalDNS has introduced a webhook system, which can be used to add a new provider. See PR #3063 for all the discussions about it.</p> <p>Known providers using webhooks:</p> Provider Repo Abion https://github.com/abiondevelopment/external-dns-webhook-abion Adguard Home Provider https://github.com/muhlba91/external-dns-provider-adguard Anexia https://github.com/ProbstenHias/external-dns-anexia-webhook Bizfly Cloud https://github.com/bizflycloud/external-dns-bizflycloud-webhook ClouDNS https://github.com/rwunderer/external-dns-cloudns-webhook Dreamhost https://github.com/asymingt/external-dns-dreamhost-webhook Efficient IP https://github.com/EfficientIP-Labs/external-dns-efficientip-webhook Gcore https://github.com/G-Core/external-dns-gcore-webhook GleSYS https://github.com/glesys/external-dns-glesys Hetzner https://github.com/mconfalonieri/external-dns-hetzner-webhook Huawei Cloud https://github.com/setoru/external-dns-huaweicloud-webhook IONOS https://github.com/ionos-cloud/external-dns-ionos-webhook Infoblox https://github.com/AbsaOSS/external-dns-infoblox-webhook Mikrotik https://github.com/mirceanton/external-dns-provider-mikrotik Netcup https://github.com/mrueg/external-dns-netcup-webhook Netic https://github.com/neticdk/external-dns-tidydns-webhook OpenStack Designate https://github.com/inovex/external-dns-designate-webhook OpenWRT https://github.com/renanqts/external-dns-openwrt-webhook RouterOS https://github.com/benfiola/external-dns-routeros-provider STACKIT https://github.com/stackitcloud/external-dns-stackit-webhook Unifi https://github.com/kashalls/external-dns-unifi-webhook Vultr https://github.com/vultr/external-dns-vultr-webhook"},{"location":"#status-of-in-tree-providers","title":"Status of in-tree providers","text":"<p>ExternalDNS supports multiple DNS providers which have been implemented by the ExternalDNS contributors. Maintaining all of those in a central repository is a challenge, which introduces lots of toil and potential risks.</p> <p>This mean that <code>external-dns</code> has begun the process to move providers out of tree. See #4347 for more details. Those who are interested can create a webhook provider based on an in-tree provider and after submit a PR to reference it here.</p> <p>We define the following stability levels for providers:</p> <ul> <li>Stable: Used for smoke tests before a release, used in production and maintainers are active.</li> <li>Beta: Community supported, well tested, but maintainers have no access to resources to execute integration tests on the real platform and/or are not using it in production.</li> <li>Alpha: Community provided with no support from the maintainers apart from reviewing PRs.</li> </ul> <p>The following table clarifies the current status of the providers according to the aforementioned stability levels:</p> Provider Status Maintainers Google Cloud DNS Stable AWS Route 53 Stable AWS Cloud Map Beta Akamai Edge DNS Beta AzureDNS Stable Civo Alpha @alejandrojnm CloudFlare Beta DigitalOcean Alpha DNSimple Alpha OpenStack Designate Alpha PowerDNS Alpha CoreDNS Alpha Exoscale Alpha Oracle Cloud Infrastructure DNS Alpha Linode DNS Alpha RFC2136 Alpha NS1 Alpha TransIP Alpha OVH Alpha Scaleway DNS Alpha @Sh4d1 UltraDNS Alpha GoDaddy Alpha Gandi Alpha @packi IBMCloud Alpha @hughhuangzh TencentCloud Alpha @Hyzhou Plural Alpha @michaeljguarino Pi-hole Alpha @tinyzimmer"},{"location":"#kubernetes-version-compatibility","title":"Kubernetes version compatibility","text":"<p>A breaking change was added in external-dns v0.10.0.</p> ExternalDNS &lt;= 0.9.x &gt;= 0.10.0 Kubernetes &lt;= 1.18 Kubernetes &gt;= 1.19 and &lt;= 1.21 Kubernetes &gt;= 1.22"},{"location":"#running-externaldns","title":"Running ExternalDNS","text":"<p>The are two ways of running ExternalDNS:</p> <ul> <li>Deploying to a Cluster</li> <li>Running Locally</li> </ul>"},{"location":"#deploying-to-a-cluster","title":"Deploying to a Cluster","text":"<p>The following tutorials are provided:</p> <ul> <li>Akamai Edge DNS</li> <li>Alibaba Cloud</li> <li>AWS</li> <li>AWS Load Balancer Controller</li> <li>Route53<ul> <li>Same domain for public and private Route53 zones</li> </ul> </li> <li>Cloud Map</li> <li>Kube Ingress AWS Controller</li> <li>Azure DNS</li> <li>Azure Private DNS</li> <li>Civo</li> <li>Cloudflare</li> <li>CoreDNS</li> <li>DigitalOcean</li> <li>DNSimple</li> <li>Exoscale</li> <li>ExternalName Services</li> <li>Google Kubernetes Engine</li> <li>Using Google\u2019s Default Ingress Controller</li> <li>Using the Nginx Ingress Controller</li> <li>Headless Services</li> <li>Istio Gateway Source</li> <li>Linode</li> <li>NS1</li> <li>NS Record Creation with CRD Source</li> <li>MX Record Creation with CRD Source</li> <li>TXT Record Creation with CRD Source</li> <li>OpenStack Designate</li> <li>Oracle Cloud Infrastructure (OCI) DNS</li> <li>PowerDNS</li> <li>RFC2136</li> <li>TransIP</li> <li>OVH</li> <li>Scaleway</li> <li>UltraDNS</li> <li>GoDaddy</li> <li>Gandi</li> <li>IBM Cloud</li> <li>Nodes as source</li> <li>TencentCloud</li> <li>Plural</li> <li>Pi-hole</li> </ul>"},{"location":"#running-locally","title":"Running Locally","text":"<p>See the contributor guide for details on compiling from source.</p>"},{"location":"#setup-steps","title":"Setup Steps","text":"<p>Next, run an application and expose it via a Kubernetes Service:</p> <pre><code>kubectl run nginx --image=nginx --port=80\nkubectl expose pod nginx --port=80 --target-port=80 --type=LoadBalancer\n</code></pre> <p>Annotate the Service with your desired external DNS name. Make sure to change <code>example.org</code> to your domain.</p> <pre><code>kubectl annotate service nginx \"external-dns.alpha.kubernetes.io/hostname=nginx.example.org.\"\n</code></pre> <p>Optionally, you can customize the TTL value of the resulting DNS record by using the <code>external-dns.alpha.kubernetes.io/ttl</code> annotation:</p> <pre><code>kubectl annotate service nginx \"external-dns.alpha.kubernetes.io/ttl=10\"\n</code></pre> <p>For more details on configuring TTL, see here.</p> <p>Use the internal-hostname annotation to create DNS records with ClusterIP as the target.</p> <pre><code>kubectl annotate service nginx \"external-dns.alpha.kubernetes.io/internal-hostname=nginx.internal.example.org.\"\n</code></pre> <p>If the service is not of type Loadbalancer you need the \u2013publish-internal-services flag.</p> <p>Locally run a single sync loop of ExternalDNS.</p> <pre><code>external-dns --txt-owner-id my-cluster-id --provider google --google-project example-project --source service --once --dry-run\n</code></pre> <p>This should output the DNS records it will modify to match the managed zone with the DNS records you desire. It also assumes you are running in the <code>default</code> namespace. See the FAQ for more information regarding namespaces.</p> <p>Note: TXT records will have the <code>my-cluster-id</code> value embedded. Those are used to ensure that ExternalDNS is aware of the records it manages.</p> <p>Once you\u2019re satisfied with the result, you can run ExternalDNS like you would run it in your cluster: as a control loop, and not in dry-run mode:</p> <pre><code>external-dns --txt-owner-id my-cluster-id --provider google --google-project example-project --source service\n</code></pre> <p>Check that ExternalDNS has created the desired DNS record for your Service and that it points to its load balancer\u2019s IP. Then try to resolve it:</p> <pre><code>dig +short nginx.example.org.\n104.155.60.49\n</code></pre> <p>Now you can experiment and watch how ExternalDNS makes sure that your DNS records are configured as desired. Here are a couple of things you can try out:</p> <ul> <li>Change the desired hostname by modifying the Service\u2019s annotation.</li> <li>Recreate the Service and see that the DNS record will be updated to point to the new load balancer IP.</li> <li>Add another Service to create more DNS records.</li> <li>Remove Services to clean up your managed zone.</li> </ul> <p>The tutorials section contains examples, including Ingress resources, and shows you how to set up ExternalDNS in different environments such as other cloud providers and alternative Ingress controllers.</p>"},{"location":"#note","title":"Note","text":"<p>If using a txt registry and attempting to use a CNAME the <code>--txt-prefix</code> must be set to avoid conflicts.  Changing <code>--txt-prefix</code> will result in lost ownership over previously created records.</p> <p>If <code>externalIPs</code> list is defined for a <code>LoadBalancer</code> service, this list will be used instead of an assigned load balancer IP to create a DNS record. It\u2019s useful when you run bare metal Kubernetes clusters behind NAT or in a similar setup, where a load balancer IP differs from a public IP (e.g. with MetalLB).</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Are you interested in contributing to external-dns? We, the maintainers and community, would love your suggestions, contributions, and help! Also, the maintainers can be contacted at any time to learn more about how to get involved.</p> <p>We also encourage ALL active community participants to act as if they are maintainers, even if you don\u2019t have \u201cofficial\u201d write permissions. This is a community effort, we are here to serve the Kubernetes community. If you have an active interest and you want to get involved, you have real power! Don\u2019t assume that the only people who can get things done around here are the \u201cmaintainers\u201d. We also would love to add more \u201cofficial\u201d maintainers, so show us what you can do!</p> <p>The external-dns project is currently in need of maintainers for specific DNS providers. Ideally each provider would have at least two maintainers. It would be nice if the maintainers run the provider in production, but it is not strictly required. Provider listed here that do not have a maintainer listed are in need of assistance.</p> <p>Read the contributing guidelines and have a look at the contributing docs to learn about building the project, the project structure, and the purpose of each package.</p> <p>For an overview on how to write new Sources and Providers check out Sources and Providers.</p>"},{"location":"#heritage","title":"Heritage","text":"<p>ExternalDNS is an effort to unify the following similar projects in order to bring the Kubernetes community an easy and predictable way of managing DNS records across cloud providers based on their Kubernetes resources:</p> <ul> <li>Kops\u2019 DNS Controller</li> <li>Zalando\u2019s Mate</li> <li>Molecule Software\u2019s route53-kubernetes</li> </ul>"},{"location":"#user-demo-how-to-blogs-and-examples","title":"User Demo How-To Blogs and Examples","text":"<ul> <li>A full demo on GKE Kubernetes. See How-to Kubernetes with DNS management (ssl-manager pre-req)</li> <li>Run external-dns on GKE with workload identity. See Kubernetes, ingress-nginx, cert-manager &amp; external-dns</li> <li>ExternalDNS integration with Azure DNS using workload identity</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing Guidelines","text":"<p>Welcome to Kubernetes. We are excited about the prospect of you joining our community! The Kubernetes community abides by the CNCF code of conduct. Here is an excerpt:</p> <p>In the interest of fostering an open and welcoming community, we pledge to respect all people who contribute through reporting issues, posting feature requests, updating documentation, submitting pull requests or other activities.</p>"},{"location":"CONTRIBUTING/#getting-started","title":"Getting Started","text":"<p>We have full documentation on how to get started contributing here:</p> <ul> <li>Contributor License Agreement Kubernetes projects require that you sign a Contributor License Agreement (CLA) before we can accept your pull requests</li> <li>Kubernetes Contributor Guide - Main contributor documentation, or you can just jump directly to the contributing section</li> <li>Contributor Cheat Sheet - Common resources for existing developers</li> </ul>"},{"location":"CONTRIBUTING/#mentorship","title":"Mentorship","text":"<ul> <li>Mentoring Initiatives - We have a diverse set of mentorship programs available that are always looking for volunteers!</li> </ul>"},{"location":"CONTRIBUTING/#contact-information","title":"Contact Information","text":"<ul> <li>Slack channel</li> <li>Mailing list</li> </ul>"},{"location":"LICENSE/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\u201cLicense\u201d shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\u201cLicensor\u201d shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\u201cLegal Entity\u201d shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \u201ccontrol\u201d means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\u201cYou\u201d (or \u201cYour\u201d) shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\u201cSource\u201d form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\u201cObject\u201d form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\u201cWork\u201d shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\u201cDerivative Works\u201d shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\u201cContribution\u201d shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \u201csubmitted\u201d   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \u201cNot a Contribution.\u201d</p> <p>\u201cContributor\u201d shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>\u00a9 You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \u201cNOTICE\u201d text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \u201cAS IS\u201d BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"{}\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright {yyyy} {name of copyright owner}</p> <p>Licensed under the Apache License, Version 2.0 (the \u201cLicense\u201d);    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \u201cAS IS\u201d BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"code-of-conduct/","title":"Kubernetes Community Code of Conduct","text":"<p>Please refer to our Kubernetes Community Code of Conduct</p>"},{"location":"charts/external-dns/","title":"external-dns","text":"<p>ExternalDNS synchronizes exposed Kubernetes Services and Ingresses with DNS providers.</p> <p>Homepage: https://github.com/kubernetes-sigs/external-dns/</p>"},{"location":"charts/external-dns/#maintainers","title":"Maintainers","text":"Name Email Url stevehipwell steve.hipwell@gmail.com"},{"location":"charts/external-dns/#source-code","title":"Source Code","text":"<ul> <li>https://github.com/kubernetes-sigs/external-dns/</li> </ul>"},{"location":"charts/external-dns/#installing-the-chart","title":"Installing the Chart","text":"<p>Before you can install the chart you will need to add the <code>external-dns</code> repo to Helm.</p> <pre><code>helm repo add external-dns https://kubernetes-sigs.github.io/external-dns/\n</code></pre> <p>After you\u2019ve installed the repo you can install the chart.</p> <pre><code>helm upgrade --install external-dns external-dns/external-dns --version 1.15.2\n</code></pre>"},{"location":"charts/external-dns/#providers","title":"Providers","text":"<p>Configuring the ExternalDNS provider should be done via the <code>provider.name</code> value with provider specific configuration being set via the <code>provider.&lt;name&gt;.&lt;key&gt;</code> values, where supported, and the <code>extraArgs</code> value. For legacy support <code>provider</code> can be set to the name of the provider with all additional configuration being set via the <code>extraArgs</code> value. See documentation for more info on available providers and tutorials.</p>"},{"location":"charts/external-dns/#providers-with-specific-configuration-support","title":"Providers with Specific Configuration Support","text":"Provider Supported <code>webhook</code> \u2705"},{"location":"charts/external-dns/#other-providers","title":"Other Providers","text":"<p>For set up for a specific provider using the Helm chart, see the following links:</p> <ul> <li>AWS</li> <li>akamai-edgedns</li> <li>cloudflare</li> <li>digitalocean</li> <li>godaddy</li> <li>ns1</li> <li>plural</li> </ul>"},{"location":"charts/external-dns/#namespaced-scoped-installation","title":"Namespaced Scoped Installation","text":"<p>external-dns supports running on a namespaced only scope, too. If <code>namespaced=true</code> is defined, the helm chart will setup <code>Roles</code> and <code>RoleBindings</code> instead <code>ClusterRoles</code> and <code>ClusterRoleBindings</code>.</p>"},{"location":"charts/external-dns/#limited-supported","title":"Limited Supported","text":"<p>Not all sources are supported in namespaced scope, since some sources depends on cluster-wide resources. For example: Source <code>node</code> isn\u2019t supported, since <code>kind: Node</code> has scope <code>Cluster</code>. Sources like <code>istio-virtualservice</code> only work, if all resources like <code>Gateway</code> and <code>VirtualService</code> are present in the same namespaces as <code>external-dns</code>.</p> <p>The annotation <code>external-dns.alpha.kubernetes.io/endpoints-type: NodeExternalIP</code> is not supported.</p> <p>If <code>namespaced</code> is set to <code>true</code>, please ensure that <code>sources</code> my only contains supported sources (Default: <code>service,ingress</code>).</p>"},{"location":"charts/external-dns/#support-matrix","title":"Support Matrix","text":"Source Supported Infos <code>ingress</code> \u2705 <code>istio-gateway</code> \u2705 <code>istio-virtualservice</code> \u2705 <code>crd</code> \u2705 <code>kong-tcpingress</code> \u2705 <code>openshift-route</code> \u2705 <code>skipper-routegroup</code> \u2705 <code>gloo-proxy</code> \u2705 <code>contour-httpproxy</code> \u2705 <code>service</code> \u26a0\ufe0f\ufe0f NodePort not supported <code>node</code> \u274c <code>pod</code> \u274c"},{"location":"charts/external-dns/#values","title":"Values","text":"Key Type Default Description affinity object <code>{}</code> Affinity settings for <code>Pod</code> scheduling. If an explicit label selector is not provided for pod affinity or pod anti-affinity one will be created from the pod selector labels. automountServiceAccountToken bool <code>nil</code> Set this to <code>false</code> to opt out of API credential automounting for the <code>Pod</code>. commonLabels object <code>{}</code> Labels to add to all chart resources. deploymentAnnotations object <code>{}</code> Annotations to add to the <code>Deployment</code>. deploymentStrategy object <code>{\"type\":\"Recreate\"}</code> Deployment Strategy. dnsConfig object <code>nil</code> DNS config for the pod, if not set the default will be used. dnsPolicy string <code>nil</code> DNS policy for the pod, if not set the default will be used. domainFilters list <code>[]</code> Limit possible target zones by domain suffixes. env list <code>[]</code> Environment variables for the <code>external-dns</code> container. excludeDomains list <code>[]</code> Intentionally exclude domains from being managed. extraArgs list <code>[]</code> Extra arguments to provide to ExternalDNS. extraContainers object <code>{}</code> Extra containers to add to the <code>Deployment</code>. extraVolumeMounts list <code>[]</code> Extra volume mounts for the <code>external-dns</code> container. extraVolumes list <code>[]</code> Extra volumes for the <code>Pod</code>. fullnameOverride string <code>nil</code> Override the full name of the chart. global.imagePullSecrets list <code>[]</code> Global image pull secrets. image.pullPolicy string <code>\"IfNotPresent\"</code> Image pull policy for the <code>external-dns</code> container. image.repository string <code>\"registry.k8s.io/external-dns/external-dns\"</code> Image repository for the <code>external-dns</code> container. image.tag string <code>nil</code> Image tag for the <code>external-dns</code> container, this will default to <code>.Chart.AppVersion</code> if not set. imagePullSecrets list <code>[]</code> Image pull secrets. initContainers list <code>[]</code> Init containers to add to the <code>Pod</code> definition. interval string <code>\"1m\"</code> Interval for DNS updates. labelFilter string <code>nil</code> Filter resources queried for endpoints by label selector livenessProbe object See values.yaml Liveness probe configuration for the <code>external-dns</code> container. logFormat string <code>\"text\"</code> Log format. logLevel string <code>\"info\"</code> Log level. managedRecordTypes list <code>[]</code> Record types to manage (default: A, AAAA, CNAME) nameOverride string <code>nil</code> Override the name of the chart. namespaced bool <code>false</code> if <code>true</code>, ExternalDNS will run in a namespaced scope (<code>Role`` and</code>Rolebinding`` will be namespaced too). nodeSelector object <code>{}</code> Node labels to match for <code>Pod</code> scheduling. podAnnotations object <code>{}</code> Annotations to add to the <code>Pod</code>. podLabels object <code>{}</code> Labels to add to the <code>Pod</code>. podSecurityContext object See values.yaml Pod security context, this supports full customisation. policy string <code>\"upsert-only\"</code> How DNS records are synchronized between sources and providers; available values are <code>sync</code> &amp; <code>upsert-only</code>. priorityClassName string <code>nil</code> Priority class name for the <code>Pod</code>. provider.name string <code>\"aws\"</code> ExternalDNS provider name; for the available providers and how to configure them see README. provider.webhook.args list <code>[]</code> Extra arguments to provide for the <code>webhook</code> container. provider.webhook.env list <code>[]</code> Environment variables for the <code>webhook</code> container. provider.webhook.extraVolumeMounts list <code>[]</code> Extra volume mounts for the <code>webhook</code> container. provider.webhook.image.pullPolicy string <code>\"IfNotPresent\"</code> Image pull policy for the <code>webhook</code> container. provider.webhook.image.repository string <code>nil</code> Image repository for the <code>webhook</code> container. provider.webhook.image.tag string <code>nil</code> Image tag for the <code>webhook</code> container. provider.webhook.livenessProbe object See values.yaml Liveness probe configuration for the <code>external-dns</code> container. provider.webhook.readinessProbe object See values.yaml Readiness probe configuration for the <code>webhook</code> container. provider.webhook.resources object <code>{}</code> Resources for the <code>webhook</code> container. provider.webhook.securityContext object See values.yaml Pod security context for the <code>webhook</code> container. provider.webhook.service.port int <code>8080</code> Webhook exposed HTTP port for the service. provider.webhook.serviceMonitor object See values.yaml Optional Service Monitor configuration for the <code>webhook</code> container. rbac.additionalPermissions list <code>[]</code> Additional rules to add to the <code>ClusterRole</code>. rbac.create bool <code>true</code> If <code>true</code>, create a <code>ClusterRole</code> &amp; <code>ClusterRoleBinding</code> with access to the Kubernetes API. readinessProbe object See values.yaml Readiness probe configuration for the <code>external-dns</code> container. registry string <code>\"txt\"</code> Specify the registry for storing ownership and labels. Valid values are <code>txt</code>, <code>aws-sd</code>, <code>dynamodb</code> &amp; <code>noop</code>. resources object <code>{}</code> Resources for the <code>external-dns</code> container. revisionHistoryLimit int <code>nil</code> Specify the number of old <code>ReplicaSets</code> to retain to allow rollback of the `Deployment``. secretConfiguration.data object <code>{}</code> <code>Secret</code> data. secretConfiguration.enabled bool <code>false</code> If <code>true</code>, create a <code>Secret</code> to store sensitive provider configuration (DEPRECATED). secretConfiguration.mountPath string <code>nil</code> Mount path for the <code>Secret</code>, this can be templated. secretConfiguration.subPath string <code>nil</code> Sub-path for mounting the <code>Secret</code>, this can be templated. securityContext object See values.yaml Security context for the <code>external-dns</code> container. service.annotations object <code>{}</code> Service annotations. service.ipFamilies list <code>[]</code> Service IP families (e.g. IPv4 and/or IPv6). service.ipFamilyPolicy string <code>nil</code> Service IP family policy. service.port int <code>7979</code> Service HTTP port. serviceAccount.annotations object <code>{}</code> Annotations to add to the service account. Templates are allowed in both the key and the value. Example: <code>example.com/annotation/{{ .Values.nameOverride }}: {{ .Values.nameOverride }}</code> serviceAccount.automountServiceAccountToken string <code>nil</code> Set this to <code>false</code> to opt out of API credential automounting for the <code>ServiceAccount</code>. serviceAccount.create bool <code>true</code> If <code>true</code>, create a new <code>ServiceAccount</code>. serviceAccount.labels object <code>{}</code> Labels to add to the service account. serviceAccount.name string <code>nil</code> If this is set and <code>serviceAccount.create</code> is <code>true</code> this will be used for the created <code>ServiceAccount</code> name, if set and <code>serviceAccount.create</code> is <code>false</code> then this will define an existing <code>ServiceAccount</code> to use. serviceMonitor.additionalLabels object <code>{}</code> Additional labels for the <code>ServiceMonitor</code>. serviceMonitor.annotations object <code>{}</code> Annotations to add to the <code>ServiceMonitor</code>. serviceMonitor.bearerTokenFile string <code>nil</code> Provide a bearer token file for the <code>ServiceMonitor</code>. serviceMonitor.enabled bool <code>false</code> If <code>true</code>, create a <code>ServiceMonitor</code> resource to support the Prometheus Operator. serviceMonitor.interval string <code>nil</code> If set override the Prometheus default interval. serviceMonitor.metricRelabelings list <code>[]</code> Metric relabel configs to apply to samples before ingestion. serviceMonitor.namespace string <code>nil</code> If set create the <code>ServiceMonitor</code> in an alternate namespace. serviceMonitor.relabelings list <code>[]</code> Relabel configs to apply to samples before ingestion. serviceMonitor.scheme string <code>nil</code> If set overrides the Prometheus default scheme. serviceMonitor.scrapeTimeout string <code>nil</code> If set override the Prometheus default scrape timeout. serviceMonitor.targetLabels list <code>[]</code> Provide target labels for the <code>ServiceMonitor</code>. serviceMonitor.tlsConfig object <code>{}</code> Configure the <code>ServiceMonitor</code> TLS config. shareProcessNamespace bool <code>false</code> If <code>true</code>, the <code>Pod</code> will have process namespace sharing enabled. sources list <code>[\"service\",\"ingress\"]</code> Kubernetes resources to monitor for DNS entries. terminationGracePeriodSeconds int <code>nil</code> Termination grace period for the <code>Pod</code> in seconds. tolerations list <code>[]</code> Node taints which will be tolerated for <code>Pod</code> scheduling. topologySpreadConstraints list <code>[]</code> Topology spread constraints for <code>Pod</code> scheduling. If an explicit label selector is not provided one will be created from the pod selector labels. triggerLoopOnEvent bool <code>false</code> If <code>true</code>, triggers run loop on create/update/delete events in addition of regular interval. txtOwnerId string <code>nil</code> Specify an identifier for this instance of ExternalDNS wWhen using a registry other than <code>noop</code>. txtPrefix string <code>nil</code> Specify a prefix for the domain names of TXT records created for the <code>txt</code> registry. Mutually exclusive with <code>txtSuffix</code>. txtSuffix string <code>nil</code> Specify a suffix for the domain names of TXT records created for the <code>txt</code> registry. Mutually exclusive with <code>txtPrefix</code>. <p>Autogenerated from chart metadata using helm-docs.</p>"},{"location":"charts/external-dns/CHANGELOG/","title":"ExternalDNS Helm Chart Changelog","text":"<p>All notable changes to this project will be documented in this file.</p> <p>The format is based on Keep a Changelog, and this project adheres to Semantic Versioning.</p>"},{"location":"charts/external-dns/CHANGELOG/#unreleased","title":"UNRELEASED","text":""},{"location":"charts/external-dns/CHANGELOG/#added","title":"Added","text":"<ul> <li>Added helm testing framework <code>helm plugin unittest</code>. (#5137) @ivankatliarchuk</li> <li>Added ability to generate schema with <code>helm plugin schema</code>. (#5075) @ivankatliarchuk</li> <li>Added <code>docs/contributing/dev-guide.md#helm-values</code> guide. (#5075) @ivankatliarchuk</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#changed","title":"Changed","text":"<ul> <li>Regenerate JSON schema with `helm-values-schema-json\u2019 plugin. (#5075) @ivankatliarchuk</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#v1152-2025-02-14","title":"v1.15.2 - 2025-02-14","text":""},{"location":"charts/external-dns/CHANGELOG/#changed_1","title":"Changed","text":"<ul> <li>Added <code>transportservers</code> resource to ClusterRole when specifying <code>f5-transportserver</code> or <code>f5-virtualserver</code> as a source. (#5066) @visokoo</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#fixed","title":"Fixed","text":"<ul> <li>Fixed handling of non-string types in <code>serviceAccount.metadata.annotations</code> field. (#5067) @hjoshi123</li> <li>Fixed regression where <code>affinity.nodeAffinity</code> was being ignored. (#5046) @mkhpalm</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#v1151-2025-01-27","title":"v1.15.1 - 2025-01-27","text":""},{"location":"charts/external-dns/CHANGELOG/#added_1","title":"Added","text":"<ul> <li>Added ability to configure <code>imagePullSecrets</code> via helm <code>global</code> value. (#4667) @jkroepke</li> <li>Added options to configure <code>labelFilter</code> and <code>managedRecordTypes</code> via dedicated helm values. (#4849) @abaguas</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#changed_2","title":"Changed","text":"<ul> <li>Allow templating <code>serviceaccount.annotations</code> keys and values, by rendering them using the <code>tpl</code> built-in function. (#4958) @fcrespofastly</li> <li>Updated ExternalDNS OCI image version to v0.15.1. (#5028) @stevehipwell</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#fixed_1","title":"Fixed","text":"<ul> <li>Fixed automatic addition of pod selector labels to <code>affinity</code> and <code>topologySpreadConstraints</code> if not defined. (#4666) @pvickery-ParamountCommerce</li> <li>Fixed missing Ingress permissions when using Istio sources. (#4845) @joekhoobyar</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#v1150-2024-09-11","title":"v1.15.0 - 2024-09-11","text":""},{"location":"charts/external-dns/CHANGELOG/#changed_3","title":"Changed","text":"<ul> <li>Updated ExternalDNS OCI image version to v0.15.0. (#4735) @stevehipwell</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#fixed_2","title":"Fixed","text":"<ul> <li>Fixed <code>provider.webhook.resources</code> behavior to correctly leverage resource limits. (#4560) @crutonjohn</li> <li>Fixed <code>provider.webhook.imagePullPolicy</code> behavior to correctly leverage pull policy. (#4643) @kimsondrup</li> <li>Fixed to add correct webhook metric port to <code>Service</code> and <code>ServiceMonitor</code>. (#4643) @kimsondrup</li> <li>Fixed to no longer require the unauthenticated webhook provider port to be exposed for health probes. (#4691) @kimsondrup &amp; @hatrx</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#v1145-2024-06-10","title":"v1.14.5 - 2024-06-10","text":""},{"location":"charts/external-dns/CHANGELOG/#added_2","title":"Added","text":"<ul> <li>Added support for <code>extraContainers</code> argument. (#4432) @omerap12</li> <li>Added support for setting <code>excludeDomains</code> argument.  (#4380) @bford-evs</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#changed_4","title":"Changed","text":"<ul> <li>Updated ExternalDNS OCI image version to v0.14.2. (#4541) @stevehipwell</li> <li>Updated <code>DNSEndpoint</code> CRD. (#4541) @stevehipwell</li> <li>Changed the implementation for <code>revisionHistoryLimit</code> to be more generic. (#4541) @stevehipwell</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#fixed_3","title":"Fixed","text":"<ul> <li>Fixed the <code>ServiceMonitor</code> job name to correctly use the instance label. (#4541) @stevehipwell</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#v1144-2024-04-05","title":"v1.14.4 - 2024-04-05","text":""},{"location":"charts/external-dns/CHANGELOG/#added_3","title":"Added","text":"<ul> <li>Added support for setting <code>dnsConfig</code>. (#4265) @davhdavh</li> <li>Added support for <code>DNSEndpoint</code> CRD. (#4322) @onedr0p</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#changed_5","title":"Changed","text":"<ul> <li>Updated ExternalDNS OCI image version to v0.14.1. (#4357) @stevehipwell</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#v1143-2024-01-26","title":"v1.14.3 - 2024-01-26","text":""},{"location":"charts/external-dns/CHANGELOG/#fixed_4","title":"Fixed","text":"<ul> <li>Fixed args for webhook deployment. (#4202) @webwurst</li> <li>Fixed support for <code>gateway-grpcroute</code>, <code>gateway-tlsroute</code>, <code>gateway-tcproute</code> &amp; <code>gateway-udproute</code>. (#4205) @orenlevi111</li> <li>Fixed incorrect implementation for setting the <code>automountServiceAccountToken</code>. (#4208) @stevehipwell</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#v1142-2024-01-22","title":"v1.14.2 - 2024-01-22","text":""},{"location":"charts/external-dns/CHANGELOG/#fixed_5","title":"Fixed","text":"<ul> <li>Restore template support in <code>.Values.provider</code> and <code>.Values.provider.name</code></li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#v1141-2024-01-12","title":"v1.14.1 - 2024-01-12","text":""},{"location":"charts/external-dns/CHANGELOG/#fixed_6","title":"Fixed","text":"<ul> <li>Fixed webhook install failure: <code>\"http-webhook-metrics\": must be no more than 15 characters</code>. (#4173) @gabe565</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#v1140-2024-01-10","title":"v1.14.0 - 2024-01-10","text":""},{"location":"charts/external-dns/CHANGELOG/#added_4","title":"Added","text":"<ul> <li>Added the option to explicitly enable or disable service account token automounting. (#3983) @gilles-gosuin</li> <li>Added the option to configure revisionHistoryLimit on the K8s Deployment resource. (#4008) @arnisoph</li> <li>Added support for webhook providers, as a sidecar. (#4032 @mloiseleur</li> <li>Added the option to configure ipFamilyPolicy and ipFamilies of external-dns Service.  (#4153) @dongjiang1989</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#changed_6","title":"Changed","text":"<ul> <li>Avoid unnecessary pod restart on each helm chart version. (#4103) @jkroepke</li> <li>Updated ExternalDNS OCI image version to v0.14.0. (#4073) @appkins</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#deprecated","title":"Deprecated","text":"<ul> <li>The <code>secretConfiguration</code> value has been deprecated in favour of creating secrets external to the Helm chart and configuring their use via the <code>extraVolumes</code> &amp; <code>extraVolumeMounts</code> values. (#4161) @stevehipwell</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#v1131-2023-09-08","title":"v1.13.1 - 2023-09-08","text":""},{"location":"charts/external-dns/CHANGELOG/#added_5","title":"Added","text":"<ul> <li>Added RBAC for Traefik to ClusterRole. (#3325) @ThomasK33</li> <li>Added support for init containers. (#3325) @calvinbui</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#changed_7","title":"Changed","text":"<ul> <li>Disallowed privilege escalation in container security context and set the seccomp profile type to <code>RuntimeDefault</code>. (#3689) @nrvnrvn</li> <li>Updated ExternalDNS OCI image version to v0.13.6. (#3917) @stevehipwell</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#removed","title":"Removed","text":"<ul> <li>Removed RBAC rule for already removed <code>contour-ingressroute</code> source. (#3764) @johngmyers</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#v1130-2023-03-30","title":"v1.13.0 - 2023-03-30","text":""},{"location":"charts/external-dns/CHANGELOG/#all-changes","title":"All Changes","text":"<ul> <li>Updated ExternalDNS version to v0.13.5. (#3661) @GMartinez-Sisti</li> <li>Adding missing gateway-httproute cluster role permission. (#3541) @nicon89</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#v1122-2023-03-30","title":"v1.12.2 - 2023-03-30","text":""},{"location":"charts/external-dns/CHANGELOG/#all-changes_1","title":"All Changes","text":"<ul> <li>Added support for ServiceMonitor relabelling. (#3366) @jkroepke</li> <li>Updated chart icon path. (#3492) kundan2707</li> <li>Added RBAC for Gateway-API resources to ClusterRole. (#3499) @michaelvl</li> <li>Added RBAC for F5 VirtualServer to ClusterRole. (#3503) @mikejoh</li> <li>Added support for running ExternalDNS with namespaced scope. (#3403) @jkroepke</li> <li>Updated ExternalDNS version to v0.13.4. (#3516) @stevehipwell</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#v1121-2023-02-06","title":"v1.12.1 - 2023-02-06","text":""},{"location":"charts/external-dns/CHANGELOG/#all-changes_2","title":"All Changes","text":"<ul> <li>Updated ExternalDNS version to v0.13.2. (#3371) @stevehipwell</li> <li>Added <code>secretConfiguration.subPath</code> to mount specific files from secret as a sub-path. (#3227) @jkroepke</li> <li>Changed to use <code>registry.k8s.io</code> instead of <code>k8s.gcr.io</code>. (#3261) @johngmyers</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#v1120-2022-11-29","title":"v1.12.0 - 2022-11-29","text":""},{"location":"charts/external-dns/CHANGELOG/#all-changes_3","title":"All Changes","text":"<ul> <li>Added ability to provide ExternalDNS with secret configuration via <code>secretConfiguration</code>. (#3144) @jkroepke</li> <li>Added the ability to template <code>provider</code> &amp; <code>extraArgs</code>. (#3144) @jkroepke</li> <li>Added the ability to customise the service account labels. (#3145) @jkroepke</li> <li>Updated ExternalDNS version to v0.13.1. (#3197) @stevehipwell</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#v1110-2022-08-10","title":"v1.11.0 - 2022-08-10","text":""},{"location":"charts/external-dns/CHANGELOG/#added_6","title":"Added","text":"<ul> <li>Added support to configure <code>dnsPolicy</code> on the Helm chart deployment. @michelzanini</li> <li>Added ability to customise the deployment strategy. mac-chaffee</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#changed_8","title":"Changed","text":"<ul> <li>Updated ExternalDNS version to v0.12.2. @stevehipwell</li> <li>Changed default deployment strategy to <code>Recreate</code>. mac-chaffee</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#v1101-2022-07-11","title":"v1.10.1 - 2022-07-11","text":""},{"location":"charts/external-dns/CHANGELOG/#fixed_7","title":"Fixed","text":"<ul> <li>Fixed incorrect addition of <code>namespace</code> to <code>ClusterRole</code> &amp; <code>ClusterRoleBinding</code>. @stevehipwell</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#v1100-2022-07-08","title":"v1.10.0 - 2022-07-08","text":""},{"location":"charts/external-dns/CHANGELOG/#added_7","title":"Added","text":"<ul> <li>Added <code>commonLabels</code> value to allow the addition of labels to all resources. @stevehipwell</li> <li>Added support for Process Namespace Sharing via the <code>shareProcessNamespace</code>  value. (#2715) @wolffberg</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#changed_9","title":"Changed","text":"<ul> <li>Update ExternalDNS version to v0.12.0. @vojtechmares</li> <li>Set resource namespaces to <code>{{ .Release.Namespace }}</code> in the templates instead of waiting until apply time for inference. @stevehipwell</li> <li>Fixed <code>rbac.additionalPermissions</code> default value.(#2796) @tamalsaha</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#v190-2022-04-19","title":"v1.9.0 - 2022-04-19","text":""},{"location":"charts/external-dns/CHANGELOG/#changed_10","title":"Changed","text":"<ul> <li>Update ExternalDNS version to v0.11.0. (#2690) @stevehipwell</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#v180-2022-04-13","title":"v1.8.0 - 2022-04-13","text":""},{"location":"charts/external-dns/CHANGELOG/#added_8","title":"Added","text":"<ul> <li>Add annotations to Deployment. (#2477) @beastob</li> </ul>"},{"location":"charts/external-dns/CHANGELOG/#changed_11","title":"Changed","text":"<ul> <li>Fix RBAC for <code>istio-virtualservice</code> source when <code>istio-gateway</code> isn\u2019t also added. (#2564) @mcwarman</li> </ul>"},{"location":"docs/20190708-external-dns-incubator/","title":"Move ExternalDNS out of Kubernetes incubator","text":"<ul> <li>Move ExternalDNS out of Kubernetes incubator</li> <li>Summary</li> <li>Motivation<ul> <li>Goals</li> </ul> </li> <li>Proposal</li> <li>Details<ul> <li>Graduation Criteria</li> <li>Maintainers</li> <li>Release process, artifacts</li> <li>Risks and Mitigations</li> </ul> </li> </ul>"},{"location":"docs/20190708-external-dns-incubator/#summary","title":"Summary","text":"<p>ExternalDNS is a project that synchronizes Kubernetes\u2019 Services, Ingresses and other Kubernetes resources to DNS backends for several DNS providers.</p> <p>The projects was started as a Kubernetes Incubator project in February 2017 and being the Kubernetes incubation initiative officially over, the maintainers want to propose the project to be moved to the kubernetes GitHub organization or to kubernetes-sigs, under the sponsorship of sig-network.</p>"},{"location":"docs/20190708-external-dns-incubator/#motivation","title":"Motivation","text":"<p>ExternalDNS started as a community project with the goal of unifying several existing projects that were trying to solve the same problem: create DNS records for Kubernetes resources on several DNS backends.</p> <p>When the project was proposed (see the original discussion), there were at least 3 existing implementations of the same functionality:</p> <ul> <li> <p>Mate - https://github.com/linki/mate</p> </li> <li> <p>DNS-controller from kops - https://github.com/kubernetes/kops/tree/HEAD/dns-controller</p> </li> <li> <p>Route53-kubernetes - https://github.com/wearemolecule/route53-kubernetes</p> </li> </ul> <p>ExternalDNS\u2019 goal from the beginning was to provide an officially supported solution to those problems.</p> <p>After two years of development, the project is still in the kubernetes-sigs.</p> <p>The incubation has been officially discontinued and to quote @thockin \u201cIncubator projects should either become real projects in Kubernetes, shut themselves down, or move elsewhere\u201d (see original thread here).</p> <p>This KEP proposes to move ExternalDNS to the main Kubernetes organization or kubernetes-sigs. The \u201cProposal\u201d section details the reasons behind it.</p>"},{"location":"docs/20190708-external-dns-incubator/#goals","title":"Goals","text":"<p>The only goal of this KEP is to establish consensus regarding the future of the ExternalDNS project and determine where it belongs.</p>"},{"location":"docs/20190708-external-dns-incubator/#proposal","title":"Proposal","text":"<p>This KEP is about moving External DNS out of the Kubernetes incubator. This section will cover the reasons why External DNS is useful and what the community would miss in case the project would be discontinued or moved under another organization.</p> <p>External DNS\u2026</p> <ul> <li> <p>Is the de facto solution to create DNS records for several Kubernetes resources.</p> </li> <li> <p>Is a vital component to achieve an experience close to a PaaS that many Kubernetes users try to replicate on top of Kubernetes, by allowing to automatically create DNS records for web applications.</p> </li> <li> <p>Supports already 18 different DNS providers including all major public clouds (AWS, Azure, GCP).</p> </li> </ul> <p>Given that the kubernetes-sigs organization will eventually be shut down, the possible alternatives to moving to be an official Kubernetes project are the following:</p> <ul> <li> <p>Shut down the project</p> </li> <li> <p>Move the project elsewhere</p> </li> </ul> <p>We believe that those alternatives would result in a worse outcome for the community compared to moving the project to the any of the other official Kubernetes organizations. In fact, shutting down ExternalDNS can cause:</p> <ul> <li> <p>The community to rebuild the same solution as already happened multiple times before the project was launched. Currently ExternalDNS is easy to be found, referenced in many articles/tutorials and for that reason not exposed to that risk.</p> </li> <li> <p>Existing users of the projects to be left without a future proof working solution.</p> </li> </ul> <p>Moving the ExternalDNS project outside of Kubernetes projects would cause:</p> <ul> <li> <p>Problems (re-)establishing user trust which could eventually lead to fragmentation and duplication.</p> </li> <li> <p>It would be hard to establish in which organization the project should be moved to.</p> </li> <li> <p>Lack of resources to test, lack of issue management via automation.</p> </li> </ul> <p>For those reasons, we propose to move ExternalDNS out of the Kubernetes incubator, to live either under the kubernetes or kubernetes-sigs organization to keep being a vital part of the Kubernetes ecosystem.</p>"},{"location":"docs/20190708-external-dns-incubator/#details","title":"Details","text":""},{"location":"docs/20190708-external-dns-incubator/#graduation-criteria","title":"Graduation Criteria","text":"<p>ExternalDNS is a two years old project widely used in production by many companies. The implementation for the three major cloud providers (AWS, Azure, GCP) is stable, not changing its logic and the project is being used in production by many company using Kubernetes.</p> <p>We have evidence that many companies are using ExternalDNS in production, but it is out of scope for this proposal to collect a comprehensive list of companies.</p> <p>The project was quoted by a number of tutorials on the web, including the official tutorials from AWS.</p> <p>ExternalDNS can\u2019t be consider to be \u201cdone\u201d: while the core functionality has been implemented, there is lack of integration testing and structural changes that are needed.</p> <p>Those are identified in the project roadmap, which is roughly made of the following items:</p> <ul> <li> <p>Decoupling of the providers</p> </li> <li> <p>Implementation proposal</p> </li> <li> <p>Development</p> </li> <li> <p>Bug fixing and performance optimization (i.e. rate limiting on cloud providers)</p> </li> <li> <p>Integration testing suite, to be implemented at least for the \u201cstable\u201d providers</p> </li> </ul> <p>For those reasons, we consider ExternalDNS to be in Beta state as a project. We believe that once the items mentioned above will be implemented, the project can reach a declared GA status.</p> <p>There are a number of other factors that need to be covered to fully describe the state of the project, including who are the maintainers, the way we release and manage the project and so on.</p>"},{"location":"docs/20190708-external-dns-incubator/#maintainers","title":"Maintainers","text":"<p>The project has the following maintainers:</p> <ul> <li> <p>hjacobs</p> </li> <li> <p>Raffo</p> </li> <li> <p>linki</p> </li> <li> <p>njuettner</p> </li> </ul> <p>The list of maintainers shrunk over time as people moved out of the original development team (all the team members were working at Zalando at the time of project creation) and the project required less work.</p> <p>The high number of providers contributed to the project pose a maintainability challenge: it is hard to bring the providers forward in terms of functionalities or even test them. The maintainers believe that the plan to transform the current Provider interface from a Go interface to an API will allow for enough decoupling and to hand over the maintenance of those plugins to the contributors themselves, see the risk and mitigations section for further details.</p>"},{"location":"docs/20190708-external-dns-incubator/#release-process-artifacts","title":"Release process, artifacts","text":"<p>The project uses the free quota of TravisCI to run tests for the project.</p> <p>The release pipeline for the project is currently fully owned by Zalando. It runs on the internal system of the company (closed source) which external maintainers/users can\u2019t access and that pushes images to the publicly accessible docker registry available at the URL <code>registry.opensource.zalan.do</code>.</p> <p>The docker registry service is provided as best effort with no sort of SLA and the maintainers team openly suggests the users to build and maintain their own docker image based on the provided Dockerfiles.</p> <p>Providing a vanity URL for the docker images was consider a non goal till now, but the community seems to be wanting official images from a GCR domain, similarly to what is available for other parts of official Kubernetes projects.</p> <p>ExternalDNS does not follow a specific release cycle. Releases are made often when there are major contributions (i.e. new providers) or important bug fixes. That said, the default branch is considered stable and can be used as well to build images.</p>"},{"location":"docs/20190708-external-dns-incubator/#risks-and-mitigations","title":"Risks and Mitigations","text":"<p>The following are risks that were identified:</p> <ul> <li> <p>Low number of maintainers: we are currently facing issues keeping up with the number of pull requests and issues giving the low number of maintainers. The list of maintainers already shrunk from 8 maintainers to 4.</p> </li> <li> <p>Issues maintaining community contributed providers: we often lack access to external providers (i.e. InfoBlox, etc.) and this means that we cannot verify the implementations and/or run regression tests that go beyond unit testing.</p> </li> <li> <p>Somewhat low quality of releases due to lack of integration testing.</p> </li> </ul> <p>We think that the following actions will constitute appropriate mitigations:</p> <ul> <li>Decoupling the providers via an API will allow us to resolve the problem of the providers. Being the project already more than 2 years old and given that there are 18 providers implemented, we possess enough information to define an API that we can be stable in a short timeframe.</li> <li> <p>Once this is stable, the problem of testing the providers can be deferred to be a provider\u2019s responsibility. This will also reduce the scope of External DNS core code, which means that there will be no need for a further increase of the maintaining team.</p> </li> <li> <p>We added integration testing for the main cloud providers to the roadmap for the 1.0 release to make sure that we cover the mostly used ones.</p> </li> <li> <p>We believe that this item should be tackled independently from the decoupling of providers as it would be capable of generating value independently from the result of the decoupling efforts.</p> </li> <li> <p>With the move to the Kubernetes incubation, we hope that we will be able to access the testing resources of the Kubernetes project.</p> </li> </ul>"},{"location":"docs/deprecation/","title":"External DNS Deprecation Policy","text":"<p>This document defines the Deprecation Policy for External DNS.</p> <p>Kubernetes is a dynamic system driven by APIs, which evolve with each new release. A crucial aspect of any API-driven system is having a well-defined deprecation policy. This policy informs users about APIs that are slated for removal or modification. Kubernetes follows this principle and periodically refines or upgrades its APIs or capabilities. Consequently, older features are marked as deprecated and eventually phased out. To avoid breaking existing users, we should follow a simple deprecation policy for behaviors that a slated to be removed.</p> <p>The features and capabilities either to evolve or need to be removed.</p>"},{"location":"docs/deprecation/#deprecation-policy","title":"Deprecation Policy","text":"<p>We follow the Kubernetes Deprecation Policy and API Versioning Scheme: alpha, beta, GA. It is therefore important to be aware of deprecation announcements and know when API versions will be removed, to help minimize the effect.</p>"},{"location":"docs/deprecation/#scope","title":"Scope","text":"<ul> <li>CRDs and API Objects and fields: <code>.Spec</code>, <code>.Status</code> and <code>.Status.Conditions[]</code></li> <li>Annotations objects or it\u2019s values</li> <li>Controller Configuration: CLI flags &amp; environment variables</li> <li>Metrics as defined in the Kubernetes docs</li> <li>Revert a specific behavior without an alternative (flag,crd or annotation)</li> </ul>"},{"location":"docs/deprecation/#non-scope","title":"Non-Scope","text":"<p>Everything not listed in scope is not subject to this deprecation policy and it is subject to breaking changes, updates at any point in time, and deprecation - as long as it follows the Deprecation Process listed below.</p> <p>This includes, but isn\u2019t limited to:</p> <ul> <li>Any feature/specific behavior not in Scope.</li> <li>Source code imports</li> <li>Source code refactorings</li> <li>Helm Charts</li> <li>Release process</li> <li>Docker Images (including multi-arch builds)</li> <li>Image Signature (including provenance, providers, keys)</li> </ul>"},{"location":"docs/deprecation/#including-features-and-behaviors-to-the-deprecation-policy","title":"Including features and behaviors to the Deprecation Policy","text":"<p>Any <code>maintainer</code> or <code>contributor</code> may propose including a feature, component, or behavior out of scope to be in scope of the deprecation policy.</p> <p>The proposal must clearly outline the rationale for inclusion, the impact on users, stability, long term maintenance plan, and day-to-day activities, if such.</p> <p>The proposal must be formalized by submitting a <code>docs/proposal/EDP-XXX.md</code> document in a Pull Request. Pull request must be labeled with <code>kind/proposal</code>.</p> <p>The proposal template location is here. The template is quite complete, one can remove any unnecessary or irrelevant section on a specific proposal.</p>"},{"location":"docs/deprecation/#deprecation-process","title":"Deprecation Process","text":""},{"location":"docs/deprecation/#nomination-of-deprecation","title":"Nomination of Deprecation","text":"<p>Any maintainer may propose deprecating a feature, component, or behavior (both in and out of scope). In Scope changes must abide to the Deprecation Policy above.</p> <p>The proposal must clearly outline the rationale for deprecation, the impact on users, and any alternatives, if such.</p> <p>The proposal must be formalized by submiting a <code>design</code> document as a Pull Request.</p>"},{"location":"docs/deprecation/#showcase-to-maintainers","title":"Showcase to Maintainers","text":"<p>The proposing maintainer must present the proposed deprecation to the maintainer group. This can be done synchronously during a community meeting or asynchronously, through a GitHub Pull Request.</p>"},{"location":"docs/deprecation/#voting","title":"Voting","text":"<p>A majority vote of maintainers is required to approve the deprecation. Votes may be conducted asynchronously, with a reasonable deadline for responses (e.g., one week). Lazy Consensus applies if the reasonable deadline is extended, with a minimal of at least one other maintainer approving the changes.</p>"},{"location":"docs/deprecation/#implementation","title":"Implementation","text":"<p>Upon approval, the proposing maintainer is responsible for implementing the changes required to mark the feature as deprecated. This includes:</p> <ul> <li>Updating the codebase with deprecation warnings where applicable.</li> <li>log.Warn(\u201cThe XXX is on the path of DEPRECATION. We recommend that you use YYY (link to docs)\u201d)</li> <li>Documenting the deprecation in release notes and relevant documentation.</li> <li>Updating APIs, metrics, or behaviors per the Kubernetes Deprecation Policy if in scope.</li> <li>If the feature is entirely deprecated, archival of any associated repositories (external provider as example).</li> </ul>"},{"location":"docs/deprecation/#deprecation-notice-in-release","title":"Deprecation Notice in Release","text":"<p>Deprecation must be introduced in the next release. The release must follow semantic versioning:</p> <ul> <li>If the project is in the 0.x stage, a <code>minor</code> version <code>bump</code> is required.</li> <li>For projects 1.x and beyond, a major version bump is required. For the features completely removed.</li> <li>If it\u2019s a flag change/flip, the <code>minor</code> version <code>bump</code> is acceptable</li> </ul>"},{"location":"docs/deprecation/#full-deprecation-and-removal","title":"Full Deprecation and Removal","text":"<p>The removal must follow standard Kubernetes deprecation timelines if the feature is in scope.</p>"},{"location":"docs/faq/","title":"Frequently asked questions","text":""},{"location":"docs/faq/#how-is-externaldns-useful-to-me","title":"How is ExternalDNS useful to me?","text":"<p>You\u2019ve probably created many deployments. Typically, you expose your deployment to the Internet by creating a Service with <code>type=LoadBalancer</code>. Depending on your environment, this usually assigns a random publicly available endpoint to your service that you can access from anywhere in the world. On Google Kubernetes Engine, this is a public IP address:</p> <pre><code>$ kubectl get svc\nNAME      CLUSTER-IP     EXTERNAL-IP     PORT(S)        AGE\nnginx     10.3.249.226   35.187.104.85   80:32281/TCP   1m\n</code></pre> <p>But dealing with IPs for service discovery isn\u2019t nice, so you register this IP with your DNS provider under a better name\u2014most likely, one that corresponds to your service name. If the IP changes, you update the DNS record accordingly.</p> <p>Those times are over! ExternalDNS takes care of that last step for you by keeping your DNS records synchronized with your external entry points.</p> <p>ExternalDNS\u2019 usefulness also becomes clear when you use Ingresses to allow external traffic into your cluster. Via Ingress, you can tell Kubernetes to route traffic to different services based on certain HTTP request attributes, e.g. the Host header:</p> <pre><code>$ kubectl get ing\nNAME         HOSTS                                      ADDRESS         PORTS     AGE\nentrypoint   frontend.example.org,backend.example.org   35.186.250.78   80        1m\n</code></pre> <p>But there\u2019s nothing that actually makes clients resolve those hostnames to the Ingress\u2019 IP address. Again, you normally have to register each entry with your DNS provider. Only if you\u2019re lucky can you use a wildcard, like in the example above.</p> <p>ExternalDNS can solve this for you as well.</p>"},{"location":"docs/faq/#which-dns-providers-are-supported","title":"Which DNS providers are supported?","text":"<p>Please check the provider status table for the list of supported providers and their status.</p> <p>As stated in the README, we are currently looking for stable maintainers for those providers, to ensure that bugfixes and new features will be available for all of those.</p>"},{"location":"docs/faq/#which-kubernetes-objects-are-supported","title":"Which Kubernetes objects are supported?","text":"<p>Services exposed via <code>type=LoadBalancer</code>, <code>type=ExternalName</code>, <code>type=NodePort</code>, and for the hostnames defined in Ingress objects as well as headless hostPort services.</p>"},{"location":"docs/faq/#how-do-i-specify-a-dns-name-for-my-kubernetes-objects","title":"How do I specify a DNS name for my Kubernetes objects?","text":"<p>There are three sources of information for ExternalDNS to decide on DNS name. ExternalDNS will pick one in order as listed below:</p> <ol> <li> <p>For ingress objects ExternalDNS will create a DNS record based on the hosts specified for the ingress object, as well as the <code>external-dns.alpha.kubernetes.io/hostname</code> annotation.    - For services ExternalDNS will look for the annotation <code>external-dns.alpha.kubernetes.io/hostname</code> on the service and use the loadbalancer IP, it also will look for the annotation <code>external-dns.alpha.kubernetes.io/internal-hostname</code> on the service and use the service IP.    - For ingresses, you can optionally force ExternalDNS to create records based on either the hosts specified or the <code>external-dns.alpha.kubernetes.io/hostname</code> annotation. This behavior is controlled by       setting the <code>external-dns.alpha.kubernetes.io/ingress-hostname-source</code> annotation on that ingress to either <code>defined-hosts-only</code> or <code>annotation-only</code>.</p> </li> <li> <p>If compatibility mode is enabled (e.g. <code>--compatibility={mate,molecule}</code> flag), External DNS will parse annotations used by Zalando/Mate, wearemolecule/route53-kubernetes. Compatibility mode with Kops DNS Controller is planned to be added in the future.</p> </li> <li> <p>If <code>--fqdn-template</code> flag is specified, e.g. <code>--fqdn-template={{.Name}}.my-org.com</code>, ExternalDNS will use service/ingress specifications for the provided template to generate DNS name.</p> </li> </ol>"},{"location":"docs/faq/#can-i-specify-multiple-global-fqdn-templates","title":"Can I specify multiple global FQDN templates?","text":"<p>Yes, you can. Pass in a comma separated list to <code>--fqdn-template</code>. Beaware this will double (triple, etc) the amount of DNS entries based on how many services, ingresses and so on you have and will get you faster towards the API request limit of your DNS provider.</p>"},{"location":"docs/faq/#which-service-and-ingress-controllers-are-supported","title":"Which Service and Ingress controllers are supported?","text":"<p>Regarding Services, we\u2019ll support the OSI Layer 4 load balancers that Kubernetes creates on AWS and Google Kubernetes Engine, and possibly other clusters running on Google Compute Engine.</p> <p>Regarding Ingress, we\u2019ll support:</p> <ul> <li>Google\u2019s Ingress Controller on GKE that integrates with their Layer 7 load balancers (GLBC)</li> <li>nginx-ingress-controller v0.9.x with a fronting Service</li> <li>Zalando\u2019s AWS Ingress controller, based on AWS ALBs and Skipper</li> <li>Traefik</li> <li>version 1.7, when <code>kubernetes.ingressEndpoint</code> is configured (<code>kubernetes.ingressEndpoint.useDefaultPublishedService</code> in the Helm chart)</li> <li>versions &gt;=2.0, when <code>providers.kubernetesIngress.ingressEndpoint</code> is configured (<code>providers.kubernetesIngress.publishedService.enabled</code> is set to <code>true</code> in the new Helm chart)</li> </ul>"},{"location":"docs/faq/#are-other-ingress-controllers-supported","title":"Are other Ingress Controllers supported?","text":"<p>For Ingress objects, ExternalDNS will attempt to discover the target hostname of the relevant Ingress Controller automatically. If you are using an Ingress Controller that is not listed above you may have issues with ExternalDNS not discovering Endpoints and consequently not creating any DNS records. As a workaround, it is possible to force create an Endpoint by manually specifying a target host/IP for the records to be created by setting the annotation <code>external-dns.alpha.kubernetes.io/target</code> in the Ingress object.</p> <p>Another reason you may want to override the ingress hostname or IP address is if you have an external mechanism for handling failover across ingress endpoints. Possible scenarios for this would include using keepalived-vip to manage failover faster than DNS TTLs might expire.</p> <p>Note that if you set the target to a hostname, then a CNAME record will be created. In this case, the hostname specified in the Ingress object\u2019s annotation must already exist. (i.e. you have a Service resource for your Ingress Controller with the <code>external-dns.alpha.kubernetes.io/hostname</code> annotation set to the same value)</p>"},{"location":"docs/faq/#what-about-other-projects-similar-to-externaldns","title":"What about other projects similar to ExternalDNS?","text":"<p>ExternalDNS is a joint effort to unify different projects accomplishing the same goals, namely:</p> <ul> <li>Kops\u2019 DNS Controller</li> <li>Zalando\u2019s Mate</li> <li>Molecule Software\u2019s route53-kubernetes</li> </ul> <p>We strive to make the migration from these implementations a smooth experience. This means that, for some time, we\u2019ll support their annotation semantics in ExternalDNS and allow both implementations to run side-by-side. This enables you to migrate incrementally and slowly phase out the other implementation.</p>"},{"location":"docs/faq/#how-does-it-work-with-other-implementations-and-legacy-records","title":"How does it work with other implementations and legacy records?","text":"<p>ExternalDNS will allow you to opt into any Services and Ingresses that you want it to consider, by an annotation. This way, it can co-exist with other implementations running in the same cluster if they also support this pattern. However, we\u2019ll most likely declare ExternalDNS to be the default implementation. This means that ExternalDNS will consider Services and Ingresses that don\u2019t specifically declare which controller they want to be processed by; this is similar to the <code>ingress.class</code> annotation on GKE.</p>"},{"location":"docs/faq/#im-afraid-you-will-mess-up-my-dns-records","title":"I\u2019m afraid you will mess up my DNS records","text":"<p>Since v0.3, ExternalDNS can be configured to use an ownership registry. When this option is enabled, ExternalDNS will keep track of which records it has control over, and will never modify any records over which it doesn\u2019t have control. This is a fundamental requirement to operate ExternalDNS safely when there might be other actors creating DNS records in the same target space.</p> <p>For now ExternalDNS uses TXT records to label owned records, and there might be other alternatives coming in the future releases.</p>"},{"location":"docs/faq/#does-anyone-use-externaldns-in-production","title":"Does anyone use ExternalDNS in production?","text":"<p>Yes, multiple companies are using ExternalDNS in production. Zalando, as an example, has been using it in production since its v0.3 release, mostly using the AWS provider.</p>"},{"location":"docs/faq/#how-can-we-start-using-externaldns","title":"How can we start using ExternalDNS?","text":"<p>Check out the following descriptive tutorials on how to run ExternalDNS in GKE and AWS or any other supported provider.</p>"},{"location":"docs/faq/#why-is-externaldns-only-adding-a-single-ip-address-in-route-53-on-aws-when-using-the-nginx-ingress-controller","title":"Why is ExternalDNS only adding a single IP address in Route 53 on AWS when using the <code>nginx-ingress-controller</code> ?","text":"<p>By default the <code>nginx-ingress-controller</code> assigns a single IP address to an Ingress resource when it\u2019s created. ExternalDNS uses what\u2019s assigned to the Ingress resource, so it too will use this single IP address when adding the record in Route 53.</p>"},{"location":"docs/faq/#how-do-i-get-it-to-use-the-fqdn-of-the-elb-assigned-to-my-nginx-ingress-controller-service-instead","title":"How do I get it to use the FQDN of the ELB assigned to my <code>nginx-ingress-controller</code> Service instead?","text":"<p>In most AWS deployments, you\u2019ll instead want the Route 53 entry to be the FQDN of the ELB that is assigned to the <code>nginx-ingress-controller</code> Service. To accomplish this, when you create the <code>nginx-ingress-controller</code> Deployment, you need to provide the <code>--publish-service</code> option to the <code>/nginx-ingress-controller</code> executable under <code>args</code>. Once this is deployed new Ingress resources will get the ELB\u2019s FQDN and ExternalDNS will use the same when creating records in Route 53.</p> <p>According to the <code>nginx-ingress-controller</code> docs the value you need to provide <code>--publish-service</code> is:</p> <p>Service fronting the ingress controllers. Takes the form namespace/name. The controller will set the endpoint records on the ingress objects to reflect those on the service.</p> <p>For example if your <code>nginx-ingress-controller</code> Service\u2019s name is <code>nginx-ingress-controller-svc</code> and it\u2019s in the <code>default</code> namespace the start of your resource YAML might look like the following. Note the second to last line.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n    spec:\n      hostNetwork: false\n      containers:\n        - name: nginx-ingress-controller\n          image: \"gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.11\"\n          imagePullPolicy: \"IfNotPresent\"\n          args:\n            - /nginx-ingress-controller\n            - --default-backend-service={your-backend-service}\n            - --publish-service=default/nginx-ingress-controller-svc\n            - --configmap={your-configmap}\n</code></pre>"},{"location":"docs/faq/#i-have-a-serviceingress-but-its-ignored-by-externaldns-why","title":"I have a Service/Ingress but it\u2019s ignored by ExternalDNS. Why?","text":"<p>ExternalDNS can be configured to only use Services or Ingresses as source. In case Services or Ingresses seem to be ignored in your setup, consider checking how the flag <code>--source</code> was configured when deployed. For reference, see the issue https://github.com/kubernetes-sigs/external-dns/issues/267.</p>"},{"location":"docs/faq/#im-using-an-elb-with-txt-registry-but-the-cname-record-clashes-with-the-txt-record-how-to-avoid-this","title":"I\u2019m using an ELB with TXT registry but the CNAME record clashes with the TXT record. How to avoid this?","text":"<p>CNAMEs cannot co-exist with other records, therefore you can use the <code>--txt-prefix</code> flag which makes sure to create a TXT record with a name following the pattern <code>prefix.&lt;CNAME record&gt;</code>. For reference, see the issue https://github.com/kubernetes-sigs/external-dns/issues/262.</p>"},{"location":"docs/faq/#can-i-force-externaldns-to-create-cname-records-for-elbalb","title":"Can I force ExternalDNS to create CNAME records for ELB/ALB?","text":"<p>The default logic is: when a target looks like an ELB/ALB, ExternalDNS will create ALIAS records for it. Under certain circumstances you want to force ExternalDNS to create CNAME records instead. If you want to do that, start ExternalDNS with the <code>--aws-prefer-cname</code> flag.</p> <p>Why should I want to force ExternalDNS to create CNAME records for ELB/ALB? Some motivations of users were:</p> <p>\u201cOur hosted zones records are synchronized with our enterprise DNS. The record type ALIAS is an AWS proprietary record type and AWS allows you to set a DNS record directly on AWS resources. Since this is not a DNS RfC standard and therefore can not be transferred and created in our enterprise DNS. So we need to force CNAME creation instead.\u201d</p> <p>or</p> <p>\u201cIn case of ALIAS if we do nslookup with domain name, it will return only IPs of ELB. So it is always difficult for us to locate ELB in AWS console to which domain is pointing. If we configure it with CNAME it will return exact ELB CNAME, which is more helpful.!\u201d</p>"},{"location":"docs/faq/#which-permissions-do-i-need-when-running-externaldns-on-a-gce-or-gke-node","title":"Which permissions do I need when running ExternalDNS on a GCE or GKE node","text":"<p>You need to add either https://www.googleapis.com/auth/ndev.clouddns.readwrite or https://www.googleapis.com/auth/cloud-platform on your instance group\u2019s scope.</p>"},{"location":"docs/faq/#how-can-i-run-externaldns-under-a-specific-gcp-service-account-eg-to-access-dns-records-in-other-projects","title":"How can I run ExternalDNS under a specific GCP Service Account, e.g. to access DNS records in other projects?","text":"<p>Have a look at https://github.com/linki/mate/blob/v0.6.2/examples/google/README.md#permissions</p>"},{"location":"docs/faq/#how-do-i-configure-multiple-sources-via-environment-variables-also-applies-to-domain-filters","title":"How do I configure multiple Sources via environment variables? (also applies to domain filters)","text":"<p>Separate the individual values via a line break. The equivalent of <code>--source=service --source=ingress</code> would be <code>service\\ningress</code>. However, it can be tricky do define that depending on your environment. The following examples work (zsh):</p> <p>Via docker:</p> <pre><code>$ docker run \\\n  -e EXTERNAL_DNS_SOURCE=$'service\\ningress' \\\n  -e EXTERNAL_DNS_PROVIDER=google \\\n  -e EXTERNAL_DNS_DOMAIN_FILTER=$'foo.com\\nbar.com' \\\n  registry.k8s.io/external-dns/external-dns:v0.15.1\ntime=\"2017-08-08T14:10:26Z\" level=info msg=\"config: &amp;{APIServerURL: KubeConfig: Sources:[service ingress] Namespace: ...\n</code></pre> <p>Locally:</p> <pre><code>$ export EXTERNAL_DNS_SOURCE=$'service\\ningress'\n$ external-dns --provider=google\nINFO[0000] config: &amp;{APIServerURL: KubeConfig: Sources:[service ingress] Namespace: ...\n</code></pre> <pre><code>$ EXTERNAL_DNS_SOURCE=$'service\\ningress' external-dns --provider=google\nINFO[0000] config: &amp;{APIServerURL: KubeConfig: Sources:[service ingress] Namespace: ...\n</code></pre> <p>In a Kubernetes manifest:</p> <pre><code>spec:\n  containers:\n  - name: external-dns\n    args:\n    - --provider=google\n    env:\n    - name: EXTERNAL_DNS_SOURCE\n      value: \"service\\ningress\"\n</code></pre> <p>Or preferably:</p> <pre><code>spec:\n  containers:\n  - name: external-dns\n    args:\n    - --provider=google\n    env:\n    - name: EXTERNAL_DNS_SOURCE\n      value: |-\n        service\n        ingress\n</code></pre>"},{"location":"docs/faq/#running-an-internal-and-external-dns-service","title":"Running an internal and external dns service","text":"<p>Sometimes you need to run an internal and an external dns service. The internal one should provision hostnames used on the internal network (perhaps inside a VPC), and the external one to expose DNS to the internet.</p> <p>To do this with ExternalDNS you can use the <code>--ingress-class</code> flag to specifically tie an instance of ExternalDNS to an instance of a ingress controller. Let\u2019s assume you have two ingress controllers, <code>internal</code> and <code>external</code>. You can then start two ExternalDNS providers, one with <code>--ingress-class=internal</code> and one with <code>--ingress-class=external</code>.</p> <p>If you need to search for multiple ingress classes, you can specify the flag multiple times, like so: <code>--ingress-class=internal --ingress-class=external</code>.</p> <p>The <code>--ingress-class</code> flag will check both the <code>spec.ingressClassName</code> field and the deprecated <code>kubernetes.io/ingress.class</code> annotation. The <code>spec.ingressClassName</code> tasks precedence over the annotation if both are supplied.</p> <p>Backward compatibility</p> <p>The previous <code>--annotation-filter</code> flag can still be used to restrict which objects ExternalDNS considers; for example, <code>--annotation-filter=kubernetes.io/ingress.class in (public,dmz)</code>.</p> <p>However, beware when using annotation filters with multiple sources, e.g. <code>--source=service --source=ingress</code>, since <code>--annotation-filter</code> will filter every given source object. If you need to use annotation filters against a specific source you have to run a separated external dns service containing only the wanted <code>--source</code>  and <code>--annotation-filter</code>.</p> <p>Note: the <code>--ingress-class</code> flag cannot be used at the same time as the <code>--annotation-filter=kubernetes.io/ingress.class in (...)</code> flag; if you do this an error will be raised.</p> <p>Performance considerations</p> <p>Filtering based on ingress class name or annotations means that the external-dns controller will receive all resources of that kind and then filter on the client-side. In larger clusters with many resources which change frequently this can cause performance issues. If only some resources need to be managed by an instance of external-dns then label filtering can be used instead of ingress class filtering (or legacy annotation filtering). This means that only those resources which match the selector specified in <code>--label-filter</code> will be passed to the controller.</p>"},{"location":"docs/faq/#how-do-i-specify-that-i-want-the-dns-record-to-point-to-either-the-nodes-public-or-private-ip-when-it-has-both","title":"How do I specify that I want the DNS record to point to either the Node\u2019s public or private IP when it has both?","text":"<p>If your Nodes have both public and private IP addresses, you might want to write DNS records with one or the other. For example, you may want to write a DNS record in a private zone that resolves to your Nodes\u2019 private IPs so that traffic never leaves your private network.</p> <p>To accomplish this, set this annotation on your service: <code>external-dns.alpha.kubernetes.io/access=private</code> Conversely, to force the public IP: <code>external-dns.alpha.kubernetes.io/access=public</code></p> <p>If this annotation is not set, and the node has both public and private IP addresses, then the public IP will be used by default.</p> <p>Some loadbalancer implementations assign multiple IP addresses as external addresses. You can filter the generated targets by their networks using <code>--target-net-filter=10.0.0.0/8</code> or <code>--exclude-target-net=10.0.0.0/8</code>.</p>"},{"location":"docs/faq/#can-external-dns-manageaddremove-records-in-a-hosted-zone-which-is-setup-in-different-aws-account","title":"Can external-dns manage(add/remove) records in a hosted zone which is setup in different AWS account?","text":"<p>Yes, give it the correct cross-account/assume-role permissions and use the <code>--aws-assume-role</code> flag https://github.com/kubernetes-sigs/external-dns/pull/524#issue-181256561</p>"},{"location":"docs/faq/#how-do-i-provide-multiple-values-to-the-annotation-external-dnsalphakubernetesiohostname","title":"How do I provide multiple values to the annotation <code>external-dns.alpha.kubernetes.io/hostname</code>?","text":"<p>Separate them by <code>,</code>.</p>"},{"location":"docs/faq/#are-there-official-docker-images-provided","title":"Are there official Docker images provided?","text":"<p>When we tag a new release, we push a container image to the Kubernetes projects official container registry with the following name:</p> <pre><code>registry.k8s.io/external-dns/external-dns\n</code></pre> <p>As tags, you use the external-dns release of choice(i.e. <code>v0.7.6</code>). A <code>latest</code> tag is not provided in the container registry.</p> <p>If you wish to build your own image, you can use the provided .ko.yaml as a starting point.</p>"},{"location":"docs/faq/#which-architectures-are-supported","title":"Which architectures are supported?","text":"<p>From <code>v0.7.5</code> on we support <code>amd64</code>, <code>arm32v7</code> and <code>arm64v8</code>. This means that you can run ExternalDNS on a Kubernetes cluster backed by Rasperry Pis or on ARM instances in the cloud as well as more traditional machines backed by <code>amd64</code> compatible CPUs.</p>"},{"location":"docs/faq/#which-operating-systems-are-supported","title":"Which operating systems are supported?","text":"<p>At the time of writing we only support GNU/linux and we have no plans of supporting Windows or other operating systems.</p>"},{"location":"docs/faq/#why-am-i-seeing-time-out-errors-even-though-i-have-connectivity-to-my-cluster","title":"Why am I seeing time out errors even though I have connectivity to my cluster?","text":"<p>If you\u2019re seeing an error such as this:</p> <pre><code>FATA[0060] failed to sync cache: timed out waiting for the condition\n</code></pre> <p>You may not have the correct permissions required to query all the necessary resources in your kubernetes cluster. Specifically, you may be running in a <code>namespace</code> that you don\u2019t have these permissions in. By default, commands are run against the <code>default</code> namespace. Try changing this to your particular namespace to see if that fixes the issue.</p>"},{"location":"docs/flags/","title":"Flags","text":"Flag Description <code>--[no-]version</code> Show application version. <code>--server=\"\"</code> The Kubernetes API server to connect to (default: auto-detect) <code>--kubeconfig=\"\"</code> Retrieve target cluster configuration from a Kubernetes configuration file (default: auto-detect) <code>--request-timeout=30s</code> Request timeout when calling Kubernetes APIs. 0s means no timeout <code>--[no-]resolve-service-load-balancer-hostname</code> Resolve the hostname of LoadBalancer-type Service object to IP addresses in order to create DNS A/AAAA records instead of CNAMEs <code>--[no-]listen-endpoint-events</code> Trigger a reconcile on changes to Endpoints, for Service source (default: false) <code>--cf-api-endpoint=\"\"</code> The fully-qualified domain name of the cloud foundry instance you are targeting <code>--cf-username=\"\"</code> The username to log into the cloud foundry API <code>--cf-password=\"\"</code> The password to log into the cloud foundry API <code>--gloo-namespace=gloo-system</code> The Gloo Proxy namespace; specify multiple times for multiple namespaces. (default: gloo-system) <code>--skipper-routegroup-groupversion=\"zalando.org/v1\"</code> The resource version for skipper routegroup <code>--source=source</code> The resource types that are queried for endpoints; specify multiple times for multiple sources (required, options: service, ingress, node, pod, fake, connector, gateway-httproute, gateway-grpcroute, gateway-tlsroute, gateway-tcproute, gateway-udproute, istio-gateway, istio-virtualservice, cloudfoundry, contour-httpproxy, gloo-proxy, crd, empty, skipper-routegroup, openshift-route, ambassador-host, kong-tcpingress, f5-virtualserver, f5-transportserver, traefik-proxy) <code>--openshift-router-name=OPENSHIFT-ROUTER-NAME</code> if source is openshift-route then you can pass the ingress controller name. Based on this name external-dns will select the respective router from the route status and map that routerCanonicalHostname to the route host while creating a CNAME record. <code>--namespace=\"\"</code> Limit resources queried for endpoints to a specific namespace (default: all namespaces) <code>--annotation-filter=\"\"</code> Filter resources queried for endpoints by annotation, using label selector semantics <code>--label-filter=\"\"</code> Filter resources queried for endpoints by label selector; currently supported by source types crd, gateway-httproute, gateway-grpcroute, gateway-tlsroute, gateway-tcproute, gateway-udproute, ingress, node, openshift-route, service and ambassador-host <code>--ingress-class=INGRESS-CLASS</code> Require an Ingress to have this class name (defaults to any class; specify multiple times to allow more than one class) <code>--fqdn-template=\"\"</code> A templated string that\u2019s used to generate DNS names from sources that don\u2019t define a hostname themselves, or to add a hostname suffix when paired with the fake source (optional). Accepts comma separated list for multiple global FQDN. <code>--[no-]combine-fqdn-annotation</code> Combine FQDN template and Annotations instead of overwriting <code>--[no-]ignore-hostname-annotation</code> Ignore hostname annotation when generating DNS names, valid only when \u2013fqdn-template is set (default: false) <code>--[no-]ignore-non-host-network-pods</code> Ignore pods not running on host network when using pod source (default: true) <code>--[no-]ignore-ingress-tls-spec</code> Ignore the spec.tls section in Ingress resources (default: false) <code>--gateway-name=GATEWAY-NAME</code> Limit Gateways of Route endpoints to a specific name (default: all names) <code>--gateway-namespace=GATEWAY-NAMESPACE</code> Limit Gateways of Route endpoints to a specific namespace (default: all namespaces) <code>--gateway-label-filter=GATEWAY-LABEL-FILTER</code> Filter Gateways of Route endpoints via label selector (default: all gateways) <code>--compatibility=</code> Process annotation semantics from legacy implementations (optional, options: mate, molecule, kops-dns-controller) <code>--[no-]ignore-ingress-rules-spec</code> Ignore the spec.rules section in Ingress resources (default: false) <code>--pod-source-domain=\"\"</code> Domain to use for pods records (optional) <code>--[no-]publish-internal-services</code> Allow external-dns to publish DNS records for ClusterIP services (optional) <code>--[no-]publish-host-ip</code> Allow external-dns to publish host-ip for headless services (optional) <code>--[no-]always-publish-not-ready-addresses</code> Always publish also not ready addresses for headless services (optional) <code>--connector-source-server=\"localhost:8080\"</code> The server to connect for connector source, valid only when using connector source <code>--crd-source-apiversion=\"externaldns.k8s.io/v1alpha1\"</code> API version of the CRD for crd source, e.g. <code>externaldns.k8s.io/v1alpha1</code>, valid only when using crd source <code>--crd-source-kind=\"DNSEndpoint\"</code> Kind of the CRD for the crd source in API group and version specified by crd-source-apiversion <code>--service-type-filter=SERVICE-TYPE-FILTER</code> The service types to take care about (default: all, expected: ClusterIP, NodePort, LoadBalancer or ExternalName) <code>--managed-record-types=A...</code> Record types to manage; specify multiple times to include many; (default: A, AAAA, CNAME) (supported records: A, AAAA, CNAME, NS, SRV, TXT) <code>--exclude-record-types=EXCLUDE-RECORD-TYPES</code> Record types to exclude from management; specify multiple times to exclude many; (optional) <code>--default-targets=DEFAULT-TARGETS</code> Set globally default host/IP that will apply as a target instead of source addresses. Specify multiple times for multiple targets (optional) <code>--target-net-filter=TARGET-NET-FILTER</code> Limit possible targets by a net filter; specify multiple times for multiple possible nets (optional) <code>--exclude-target-net=EXCLUDE-TARGET-NET</code> Exclude target nets (optional) <code>--[no-]traefik-disable-legacy</code> Disable listeners on Resources under the traefik.containo.us API Group <code>--[no-]traefik-disable-new</code> Disable listeners on Resources under the traefik.io API Group <code>--nat64-networks=NAT64-NETWORKS</code> Adding an A record for each AAAA record in NAT64-enabled networks; specify multiple times for multiple possible nets (optional) <code>--provider=provider</code> The DNS provider where the DNS records will be created (required, options: akamai, alibabacloud, aws, aws-sd, azure, azure-dns, azure-private-dns, civo, cloudflare, coredns, designate, digitalocean, dnsimple, exoscale, gandi, godaddy, google, ibmcloud, inmemory, linode, ns1, oci, ovh, pdns, pihole, plural, rfc2136, scaleway, skydns, tencentcloud, transip, ultradns, webhook) <code>--provider-cache-time=0s</code> The time to cache the DNS provider record list requests. <code>--domain-filter=</code> Limit possible target zones by a domain suffix; specify multiple times for multiple domains (optional) <code>--exclude-domains=</code> Exclude subdomains (optional) <code>--regex-domain-filter=</code> Limit possible domains and target zones by a Regex filter; Overrides domain-filter (optional) <code>--regex-domain-exclusion=</code> Regex filter that excludes domains and target zones matched by regex-domain-filter (optional); Require \u2018regex-domain-filter\u2019 <code>--zone-name-filter=</code> Filter target zones by zone domain (For now, only AzureDNS provider is using this flag); specify multiple times for multiple zones (optional) <code>--zone-id-filter=</code> Filter target zones by hosted zone id; specify multiple times for multiple zones (optional) <code>--google-project=\"\"</code> When using the Google provider, current project is auto-detected, when running on GCP. Specify other project with this. Must be specified when running outside GCP. <code>--google-batch-change-size=1000</code> When using the Google provider, set the maximum number of changes that will be applied in each batch. <code>--google-batch-change-interval=1s</code> When using the Google provider, set the interval between batch changes. <code>--google-zone-visibility=</code> When using the Google provider, filter for zones with this visibility (optional, options: public, private) <code>--alibaba-cloud-config-file=\"/etc/kubernetes/alibaba-cloud.json\"</code> When using the Alibaba Cloud provider, specify the Alibaba Cloud configuration file (required when \u2013provider=alibabacloud) <code>--alibaba-cloud-zone-type=</code> When using the Alibaba Cloud provider, filter for zones of this type (optional, options: public, private) <code>--aws-zone-type=</code> When using the AWS provider, filter for zones of this type (optional, options: public, private) <code>--aws-zone-tags=</code> When using the AWS provider, filter for zones with these tags <code>--aws-profile=</code> When using the AWS provider, name of the profile to use <code>--aws-assume-role=\"\"</code> When using the AWS API, assume this IAM role. Useful for hosted zones in another AWS account. Specify the full ARN, e.g. <code>arn:aws:iam::123455567:role/external-dns</code> (optional) <code>--aws-assume-role-external-id=\"\"</code> When using the AWS API and assuming a role then specify this external ID` (optional) <code>--aws-batch-change-size=1000</code> When using the AWS provider, set the maximum number of changes that will be applied in each batch. <code>--aws-batch-change-size-bytes=32000</code> When using the AWS provider, set the maximum byte size that will be applied in each batch. <code>--aws-batch-change-size-values=1000</code> When using the AWS provider, set the maximum total record values that will be applied in each batch. <code>--aws-batch-change-interval=1s</code> When using the AWS provider, set the interval between batch changes. <code>--[no-]aws-evaluate-target-health</code> When using the AWS provider, set whether to evaluate the health of a DNS target (default: enabled, disable with \u2013no-aws-evaluate-target-health) <code>--aws-api-retries=3</code> When using the AWS API, set the maximum number of retries before giving up. <code>--[no-]aws-prefer-cname</code> When using the AWS provider, prefer using CNAME instead of ALIAS (default: disabled) <code>--aws-zones-cache-duration=0s</code> When using the AWS provider, set the zones list cache TTL (0s to disable). <code>--[no-]aws-zone-match-parent</code> Expand limit possible target by sub-domains (default: disabled) <code>--[no-]aws-sd-service-cleanup</code> When using the AWS CloudMap provider, delete empty Services without endpoints (default: disabled) <code>--aws-sd-create-tag=AWS-SD-CREATE-TAG</code> When using the AWS CloudMap provider, add tag to created services. The flag can be used multiple times <code>--azure-config-file=\"/etc/kubernetes/azure.json\"</code> When using the Azure provider, specify the Azure configuration file (required when \u2013provider=azure) <code>--azure-resource-group=\"\"</code> When using the Azure provider, override the Azure resource group to use (optional) <code>--azure-subscription-id=\"\"</code> When using the Azure provider, override the Azure subscription to use (optional) <code>--azure-user-assigned-identity-client-id=\"\"</code> When using the Azure provider, override the client id of user assigned identity in config file (optional) <code>--azure-zones-cache-duration=0s</code> When using the Azure provider, set the zones list cache TTL (0s to disable). <code>--tencent-cloud-config-file=\"/etc/kubernetes/tencent-cloud.json\"</code> When using the Tencent Cloud provider, specify the Tencent Cloud configuration file (required when \u2013provider=tencentcloud) <code>--tencent-cloud-zone-type=</code> When using the Tencent Cloud provider, filter for zones with visibility (optional, options: public, private) <code>--[no-]cloudflare-proxied</code> When using the Cloudflare provider, specify if the proxy mode must be enabled (default: disabled) <code>--[no-]cloudflare-custom-hostnames</code> When using the Cloudflare provider, specify if the Custom Hostnames feature will be used. Requires \u201cCloudflare for SaaS\u201d enabled. (default: disabled) <code>--cloudflare-custom-hostnames-min-tls-version=1.0</code> When using the Cloudflare provider with the Custom Hostnames, specify which Minimum TLS Version will be used by default. (default: 1.0, options: 1.0, 1.1, 1.2, 1.3) <code>--cloudflare-custom-hostnames-certificate-authority=google</code> When using the Cloudflare provider with the Custom Hostnames, specify which Cerrtificate Authority will be used by default. (default: google, options: google, ssl_com, lets_encrypt) <code>--cloudflare-dns-records-per-page=100</code> When using the Cloudflare provider, specify how many DNS records listed per page, max possible 5,000 (default: 100) <code>--cloudflare-region-key=CLOUDFLARE-REGION-KEY</code> When using the Cloudflare provider, specify the region (default: earth) <code>--coredns-prefix=\"/skydns/\"</code> When using the CoreDNS provider, specify the prefix name <code>--akamai-serviceconsumerdomain=\"\"</code> When using the Akamai provider, specify the base URL (required when \u2013provider=akamai and edgerc-path not specified) <code>--akamai-client-token=\"\"</code> When using the Akamai provider, specify the client token (required when \u2013provider=akamai and edgerc-path not specified) <code>--akamai-client-secret=\"\"</code> When using the Akamai provider, specify the client secret (required when \u2013provider=akamai and edgerc-path not specified) <code>--akamai-access-token=\"\"</code> When using the Akamai provider, specify the access token (required when \u2013provider=akamai and edgerc-path not specified) <code>--akamai-edgerc-path=\"\"</code> When using the Akamai provider, specify the .edgerc file path. Path must be reachable form invocation environment. (required when \u2013provider=akamai and *-token, secret serviceconsumerdomain not specified) <code>--akamai-edgerc-section=\"\"</code> When using the Akamai provider, specify the .edgerc file path (Optional when edgerc-path is specified) <code>--oci-config-file=\"/etc/kubernetes/oci.yaml\"</code> When using the OCI provider, specify the OCI configuration file (required when \u2013provider=oci <code>--oci-compartment-ocid=OCI-COMPARTMENT-OCID</code> When using the OCI provider, specify the OCID of the OCI compartment containing all managed zones and records.  Required when using OCI IAM instance principal authentication. <code>--oci-zone-scope=GLOBAL</code> When using OCI provider, filter for zones with this scope (optional, options: GLOBAL, PRIVATE). Defaults to GLOBAL, setting to empty value will target both. <code>--[no-]oci-auth-instance-principal</code> When using the OCI provider, specify whether OCI IAM instance principal authentication should be used (instead of key-based auth via the OCI config file). <code>--oci-zones-cache-duration=0s</code> When using the OCI provider, set the zones list cache TTL (0s to disable). <code>--inmemory-zone=</code> Provide a list of pre-configured zones for the inmemory provider; specify multiple times for multiple zones (optional) <code>--ovh-endpoint=\"ovh-eu\"</code> When using the OVH provider, specify the endpoint (default: ovh-eu) <code>--ovh-api-rate-limit=20</code> When using the OVH provider, specify the API request rate limit, X operations by seconds (default: 20) <code>--pdns-server=\"http://localhost:8081\"</code> When using the PowerDNS/PDNS provider, specify the URL to the pdns server (required when \u2013provider=pdns) <code>--pdns-server-id=\"localhost\"</code> When using the PowerDNS/PDNS provider, specify the id of the server to retrieve. Should be <code>localhost</code> except when the server is behind a proxy (optional when \u2013provider=pdns) (default: localhost) <code>--pdns-api-key=\"\"</code> When using the PowerDNS/PDNS provider, specify the API key to use to authorize requests (required when \u2013provider=pdns) <code>--[no-]pdns-skip-tls-verify</code> When using the PowerDNS/PDNS provider, disable verification of any TLS certificates (optional when \u2013provider=pdns) (default: false) <code>--ns1-endpoint=\"\"</code> When using the NS1 provider, specify the URL of the API endpoint to target (default: https://api.nsone.net/v1/) <code>--[no-]ns1-ignoressl</code> When using the NS1 provider, specify whether to verify the SSL certificate (default: false) <code>--ns1-min-ttl=NS1-MIN-TTL</code> Minimal TTL (in seconds) for records. This value will be used if the provided TTL for a service/ingress is lower than this. <code>--digitalocean-api-page-size=50</code> Configure the page size used when querying the DigitalOcean API. <code>--ibmcloud-config-file=\"/etc/kubernetes/ibmcloud.json\"</code> When using the IBM Cloud provider, specify the IBM Cloud configuration file (required when \u2013provider=ibmcloud <code>--[no-]ibmcloud-proxied</code> When using the IBM provider, specify if the proxy mode must be enabled (default: disabled) <code>--godaddy-api-key=\"\"</code> When using the GoDaddy provider, specify the API Key (required when \u2013provider=godaddy) <code>--godaddy-api-secret=\"\"</code> When using the GoDaddy provider, specify the API secret (required when \u2013provider=godaddy) <code>--godaddy-api-ttl=GODADDY-API-TTL</code> TTL (in seconds) for records. This value will be used if the provided TTL for a service/ingress is not provided. <code>--[no-]godaddy-api-ote</code> When using the GoDaddy provider, use OTE api (optional, default: false, when \u2013provider=godaddy) <code>--tls-ca=\"\"</code> When using TLS communication, the path to the certificate authority to verify server communications (optionally specify \u2013tls-client-cert for two-way TLS) <code>--tls-client-cert=\"\"</code> When using TLS communication, the path to the certificate to present as a client (not required for TLS) <code>--tls-client-cert-key=\"\"</code> When using TLS communication, the path to the certificate key to use with the client certificate (not required for TLS) <code>--exoscale-apienv=\"api\"</code> When using Exoscale provider, specify the API environment (optional) <code>--exoscale-apizone=\"ch-gva-2\"</code> When using Exoscale provider, specify the API Zone (optional) <code>--exoscale-apikey=\"\"</code> Provide your API Key for the Exoscale provider <code>--exoscale-apisecret=\"\"</code> Provide your API Secret for the Exoscale provider <code>--rfc2136-host=</code> When using the RFC2136 provider, specify the host of the DNS server (optionally specify multiple times when when using \u2013rfc2136-load-balancing-strategy) <code>--rfc2136-port=0</code> When using the RFC2136 provider, specify the port of the DNS server <code>--rfc2136-zone=RFC2136-ZONE</code> When using the RFC2136 provider, specify zone entries of the DNS server to use <code>--[no-]rfc2136-create-ptr</code> When using the RFC2136 provider, enable PTR management <code>--[no-]rfc2136-insecure</code> When using the RFC2136 provider, specify whether to attach TSIG or not (default: false, requires \u2013rfc2136-tsig-keyname and rfc2136-tsig-secret) <code>--rfc2136-tsig-keyname=\"\"</code> When using the RFC2136 provider, specify the TSIG key to attached to DNS messages (required when \u2013rfc2136-insecure=false) <code>--rfc2136-tsig-secret=\"\"</code> When using the RFC2136 provider, specify the TSIG (base64) value to attached to DNS messages (required when \u2013rfc2136-insecure=false) <code>--rfc2136-tsig-secret-alg=\"\"</code> When using the RFC2136 provider, specify the TSIG (base64) value to attached to DNS messages (required when \u2013rfc2136-insecure=false) <code>--[no-]rfc2136-tsig-axfr</code> When using the RFC2136 provider, specify the TSIG (base64) value to attached to DNS messages (required when \u2013rfc2136-insecure=false) <code>--rfc2136-min-ttl=0s</code> When using the RFC2136 provider, specify minimal TTL (in duration format) for records. This value will be used if the provided TTL for a service/ingress is lower than this <code>--[no-]rfc2136-gss-tsig</code> When using the RFC2136 provider, specify whether to use secure updates with GSS-TSIG using Kerberos (default: false, requires \u2013rfc2136-kerberos-realm, \u2013rfc2136-kerberos-username, and rfc2136-kerberos-password) <code>--rfc2136-kerberos-username=\"\"</code> When using the RFC2136 provider with GSS-TSIG, specify the username of the user with permissions to update DNS records (required when \u2013rfc2136-gss-tsig=true) <code>--rfc2136-kerberos-password=\"\"</code> When using the RFC2136 provider with GSS-TSIG, specify the password of the user with permissions to update DNS records (required when \u2013rfc2136-gss-tsig=true) <code>--rfc2136-kerberos-realm=\"\"</code> When using the RFC2136 provider with GSS-TSIG, specify the realm of the user with permissions to update DNS records (required when \u2013rfc2136-gss-tsig=true) <code>--rfc2136-batch-change-size=50</code> When using the RFC2136 provider, set the maximum number of changes that will be applied in each batch. <code>--[no-]rfc2136-use-tls</code> When using the RFC2136 provider, communicate with name server over tls <code>--[no-]rfc2136-skip-tls-verify</code> When using TLS with the RFC2136 provider, disable verification of any TLS certificates <code>--rfc2136-load-balancing-strategy=disabled</code> When using the RFC2136 provider, specify the load balancing strategy (default: disabled, options: random, round-robin, disabled) <code>--transip-account=\"\"</code> When using the TransIP provider, specify the account name (required when \u2013provider=transip) <code>--transip-keyfile=\"\"</code> When using the TransIP provider, specify the path to the private key file (required when \u2013provider=transip) <code>--pihole-server=\"\"</code> When using the Pihole provider, the base URL of the Pihole web server (required when \u2013provider=pihole) <code>--pihole-password=\"\"</code> When using the Pihole provider, the password to the server if it is protected <code>--[no-]pihole-tls-skip-verify</code> When using the Pihole provider, disable verification of any TLS certificates <code>--plural-cluster=\"\"</code> When using the plural provider, specify the cluster name you\u2019re running with <code>--plural-provider=\"\"</code> When using the plural provider, specify the provider name you\u2019re running with <code>--policy=sync</code> Modify how DNS records are synchronized between sources and providers (default: sync, options: sync, upsert-only, create-only) <code>--registry=txt</code> The registry implementation to use to keep track of DNS record ownership (default: txt, options: txt, noop, dynamodb, aws-sd) <code>--txt-owner-id=\"default\"</code> When using the TXT or DynamoDB registry, a name that identifies this instance of ExternalDNS (default: default) <code>--txt-prefix=\"\"</code> When using the TXT registry, a custom string that\u2019s prefixed to each ownership DNS record (optional). Could contain record type template like \u2018%{record_type}-prefix-\u2018. Mutual exclusive with txt-suffix! <code>--txt-suffix=\"\"</code> When using the TXT registry, a custom string that\u2019s suffixed to the host portion of each ownership DNS record (optional). Could contain record type template like \u2018-%{record_type}-suffix\u2019. Mutual exclusive with txt-prefix! <code>--txt-wildcard-replacement=\"\"</code> When using the TXT registry, a custom string that\u2019s used instead of an asterisk for TXT records corresponding to wildcard DNS records (optional) <code>--[no-]txt-encrypt-enabled</code> When using the TXT registry, set if TXT records should be encrypted before stored (default: disabled) <code>--txt-encrypt-aes-key=\"\"</code> When using the TXT registry, set TXT record decryption and encryption 32 byte aes key (required when \u2013txt-encrypt=true) <code>--[no-]txt-new-format-only</code> When using the TXT registry, only use new format records which include record type information (e.g., prefix: \u2018a-\u2018). Reduces number of TXT records (default: disabled) <code>--dynamodb-region=\"\"</code> When using the DynamoDB registry, the AWS region of the DynamoDB table (optional) <code>--dynamodb-table=\"external-dns\"</code> When using the DynamoDB registry, the name of the DynamoDB table (default: \u201cexternal-dns\u201d) <code>--txt-cache-interval=0s</code> The interval between cache synchronizations in duration format (default: disabled) <code>--interval=1m0s</code> The interval between two consecutive synchronizations in duration format (default: 1m) <code>--min-event-sync-interval=5s</code> The minimum interval between two consecutive synchronizations triggered from kubernetes events in duration format (default: 5s) <code>--[no-]once</code> When enabled, exits the synchronization loop after the first iteration (default: disabled) <code>--[no-]dry-run</code> When enabled, prints DNS record changes rather than actually performing them (default: disabled) <code>--[no-]events</code> When enabled, in addition to running every interval, the reconciliation loop will get triggered when supported sources change (default: disabled) <code>--log-format=text</code> The format in which log messages are printed (default: text, options: text, json) <code>--metrics-address=\":7979\"</code> Specify where to serve the metrics and health check endpoint (default: :7979) <code>--log-level=info</code> Set the level of logging. (default: info, options: panic, debug, info, warning, error, fatal) <code>--webhook-provider-url=\"http://localhost:8888\"</code> The URL of the remote endpoint to call for the webhook provider (default: http://localhost:8888) <code>--webhook-provider-read-timeout=5s</code> The read timeout for the webhook provider in duration format (default: 5s) <code>--webhook-provider-write-timeout=10s</code> The write timeout for the webhook provider in duration format (default: 10s) <code>--[no-]webhook-server</code> When enabled, runs as a webhook server instead of a controller. (default: false)."},{"location":"docs/initial-design/","title":"Proposal: Design of External DNS","text":""},{"location":"docs/initial-design/#background","title":"Background","text":"<p>Project proposal</p> <p>Initial discussion</p> <p>This document describes the initial design proposal.</p> <p>External DNS is purposed to fill the existing gap of creating DNS records for Kubernetes resources. While there exist alternative solutions, this project is meant to be a standard way of managing DNS records for Kubernetes. The current project is a fusion of the following projects and driven by its maintainers:</p> <ol> <li>Kops DNS Controller</li> <li>Mate</li> <li>wearemolecule/route53-kubernetes</li> </ol>"},{"location":"docs/initial-design/#example-use-case","title":"Example use case","text":"<p>User runs <code>kubectl create -f ingress.yaml</code>, this will create an ingress as normal. Typically the user would then have to manually create a DNS record pointing the ingress endpoint If the external-dns controller is running on the cluster, it could automatically configure the DNS records instead, by observing the host attribute in the ingress object.</p>"},{"location":"docs/initial-design/#goals","title":"Goals","text":"<ol> <li>Support AWS Route53 and Google Cloud DNS providers</li> <li>DNS for Kubernetes services(type=Loadbalancer) and Ingress</li> <li>Create/update/remove records as according to Kubernetes resources state</li> <li>It should address main requirements and support main features of the projects mentioned above</li> </ol>"},{"location":"docs/initial-design/#design","title":"Design","text":""},{"location":"docs/initial-design/#extensibility","title":"Extensibility","text":"<p>New cloud providers should be easily pluggable. Initially only AWS/Google platforms are supported. However, in the future we are planning to incorporate CoreDNS and Azure DNS as possible DNS providers</p>"},{"location":"docs/initial-design/#configuration","title":"Configuration","text":"<p>DNS records will be automatically created in multiple situations:</p> <ol> <li>Setting <code>spec.rules.host</code> on an ingress object.</li> <li>Setting <code>spec.tls.hosts</code> on an ingress object.</li> <li>Adding the annotation <code>external-dns.alpha.kubernetes.io/hostname</code> on an ingress object.</li> <li>Adding the annotation <code>external-dns.alpha.kubernetes.io/hostname</code> on a <code>type=LoadBalancer</code> service object.</li> </ol>"},{"location":"docs/initial-design/#annotations","title":"Annotations","text":"<p>Record configuration should occur via resource annotations. Supported annotations:</p> Annotations Tag external-dns.alpha.kubernetes.io/controller Description Tells a DNS controller to process this service. This is useful when running different DNS controllers at the same time (or different versions of the same controller). Details The v1 implementation of dns-controller would look for service annotations <code>dns-controller</code> and <code>dns-controller/v1</code> but not for <code>mate/v1</code> or <code>dns-controller/v2</code> Default dns-controller Example dns-controller/v1 Required false \u2014 \u2014 Tag external-dns.alpha.kubernetes.io/hostname Description Fully qualified name of the desired record Default none Example foo.example.org Required Only for services. Ingress hostname is retrieved from <code>spec.rules.host</code> meta data on ingress"},{"location":"docs/initial-design/#compatibility","title":"Compatibility","text":"<p>External DNS should be compatible with annotations used by three above mentioned projects. The idea is that resources created and tagged with annotations for other projects should continue to be valid and now managed by External DNS.</p> <p>Mate</p> <p>Mate does not require services/ingress to be tagged. Therefore, it is not safe to run both Mate and External-DNS simultaneously. The idea is that initial release (?) of External DNS will support Mate annotations, which indicates the hostname to be created. Therefore the switch should be simple.</p> Annotations Tag zalando.org/dnsname Description Hostname to be registered Default Empty(falls back to template based approach) Example foo.example.org Required false <p>route53-kubernetes</p> <p>It should be safe to run both <code>route53-kubernetes</code> and <code>external-dns</code> simultaneously. Since <code>route53-kubernetes</code> only looks at services with the label <code>dns=route53</code> and does not support ingress there should be no collisions between annotations. If users desire to switch to <code>external-dns</code> they can run both controllers and migrate services over as they are able.</p>"},{"location":"docs/initial-design/#ownership","title":"Ownership","text":"<p>External DNS should be responsible for the created records. Which means that the records should be tagged and only tagged records are viable for future deletion/update. It should not mess with pre-existing records created via other means.</p>"},{"location":"docs/initial-design/#ownership-via-txt-records","title":"Ownership via TXT records","text":"<p>Each record managed by External DNS is accompanied with a TXT record with a specific value to indicate that corresponding DNS record is managed by External DNS and it can be updated/deleted respectively. TXT records are limited to lifetimes of service/ingress objects and are created/deleted once k8s resources are created/deleted.</p>"},{"location":"docs/nat64/","title":"Configure NAT64 DNS Records","text":"<p>Some NAT64 configurations are entirely handled outside the Kubernetes cluster, therefore Kubernetes does not know anything about the associated IPv4 addresses. ExternalDNS should also be able to create A records for those cases. Therefore, we can configure <code>nat64-networks</code>, which must be a /96 network. You can also specify multiple <code>nat64-networks</code> for more complex setups. This creates an additional A record with a NAT64-translated IPv4 address for each AAAA record pointing to an IPv6 address within the given <code>nat64-networks</code>.</p> <p>This can be configured with the following flag passed to the operator binary. You can also pass multiple <code>nat64-networks</code> by using a comma as seperator.</p> <pre><code>--nat64-networks=\"2001:db8:96::/96\"\n</code></pre>"},{"location":"docs/nat64/#setup-example","title":"Setup Example","text":"<p>We use an external NAT64 resolver and SIIT (Stateless IP/ICMP Translation). Therefore, our nodes only have IPv6 IP adresses but can reach IPv4 addresses and can be reached via IPv4. Outgoing connections are a classic NAT64 setup, where all IPv6 addresses gets translated to a small pool of IPv4 addresses. Incoming connnections are mapped on a different IPv4 pool, e.g. <code>198.51.100.0/24</code>, which can get translated one-to-one to IPv6 addresses. We dedicate a <code>/96</code> network for this, for example <code>2001:db8:96::/96</code>, so <code>198.51.100.0/24</code> can translated to <code>2001:db8:96::c633:6400/120</code>. Note: <code>/120</code> IPv6 network has exactly as many IP addresses as <code>/24</code> IPv4 network.</p> <p>Therefore, the <code>/96</code> network can be configured as <code>nat64-networks</code>. This means, that <code>2001:0DB8:96::198.51.100.10</code> or <code>2001:db8:96::c633:640a</code> can be translated to <code>198.51.100.10</code>. Any source can point a record to an IPv6 address within the given <code>nat64-networks</code>, for example <code>2001:db8:96::c633:640a</code>. This creates by default an AAAA record and - if <code>nat64-networks</code> is configured - also an A record with <code>198.51.100.10</code> as target.</p>"},{"location":"docs/rate-limits/","title":"DNS provider API rate limits considerations","text":""},{"location":"docs/rate-limits/#introduction","title":"Introduction","text":"<p>By design, external-dns refreshes all the records of a zone using API calls. This refresh may happen peridically and upon any changed object if the flag <code>--events</code> is enabled.</p> <p>Depending on the size of the zone and the infrastructure deployment, this may lead to external-dns hitting the DNS provider\u2019s rate-limits more easily.</p> <p>In particular, it has been found that with 200k records in an AWS Route53 zone, each refresh triggers around 70 API calls to retrieve all the records, making it more likely to hit the AWS Route53 API rate limits.</p> <p>To prevent this problem from happening, external-dns has implemented a cache to reduce the pressure on the DNS provider APIs.</p> <p>This cache is optional and systematically invalidated when DNS records have been changed in the cluster (new or deleted domains or changed target).</p>"},{"location":"docs/rate-limits/#trade-offs","title":"Trade-offs","text":"<p>The major trade-off of this setting relies in the ability to recover from a deleted record on the DNS provider side. As the DNS records are cached in memory, external-dns will not be made aware of the missing records and will hence take a longer time to restore the deleted or modified record on the provider side.</p> <p>This option is enabled using the <code>--provider-cache-time=15m</code> command line argument, and turned off when <code>--provider-cache-time=0m</code></p>"},{"location":"docs/rate-limits/#monitoring","title":"Monitoring","text":"<p>You can evaluate the behaviour of the cache thanks to the built-in metrics</p> <ul> <li><code>external_dns_provider_cache_records_calls</code></li> <li>The number of calls to the provider cache Records list.</li> <li>The label <code>from_cache=true</code> indicates that the records were retrieved from memory and the DNS provider was not reached</li> <li>The label <code>from_cache=false</code> indicates that the cache was not used and the records were retrieved from the provider</li> <li><code>external_dns_provider_cache_apply_changes_calls</code></li> <li>The number of calls to the provider cache ApplyChanges.</li> <li>Each ApplyChange systematically invalidates the cache and makes subsequent Records list to be retrieved from the provider without cache.</li> </ul>"},{"location":"docs/rate-limits/#related-options","title":"Related options","text":"<p>This global option is available for all providers and can be used in pair with other global or provider-specific options to fine-tune the behaviour of external-dns to match the specific needs of your deployments, with the goal to reduce the number of API calls to your DNS provider.</p> <ul> <li>Google</li> <li><code>--google-batch-change-interval=1s</code> When using the Google provider, set the interval between batch changes. ($EXTERNAL_DNS_GOOGLE_BATCH_CHANGE_INTERVAL)</li> <li><code>--google-batch-change-size=1000</code> When using the Google provider, set the maximum number of changes that will be applied in each batch.</li> <li>AWS</li> <li><code>--aws-batch-change-interval=1s</code> When using the AWS provider, set the interval between batch changes.</li> <li><code>--aws-batch-change-size=1000</code> When using the AWS provider, set the maximum number of changes that will be applied in each batch.</li> <li><code>--aws-batch-change-size-bytes=32000</code> When using the AWS provider, set the maximum byte size that will be applied in each batch.</li> <li><code>--aws-batch-change-size-values=1000</code> When using the AWS provider, set the maximum total record values that will be applied in each batch.</li> <li><code>--aws-zones-cache-duration=0s</code> When using the AWS provider, set the zones list cache TTL (0s to disable).</li> <li><code>--[no-]aws-zone-match-parent</code> Expand limit possible target by sub-domains</li> <li>Cloudflare</li> <li><code>--cloudflare-dns-records-per-page=100</code> When using the Cloudflare provider, specify how many DNS records listed per page, max possible 5,000 (default: 100)</li> <li>OVH</li> <li> <p><code>--ovh-api-rate-limit=20</code> When using the OVH provider, specify the API request rate limit, X operations by seconds (default: 20)</p> </li> <li> <p>Global</p> </li> <li><code>--registry=txt</code> The registry implementation to use to keep track of DNS record ownership.<ul> <li>Other registry options such as dynamodb can help mitigate rate limits by storing the registry outside of the DNS hosted zone (default: txt, options: txt, noop, dynamodb, aws-sd)</li> </ul> </li> <li><code>--txt-cache-interval=0s</code> The interval between cache synchronizations in duration format (default: disabled)</li> <li><code>--interval=1m0s</code> The interval between two consecutive synchronizations in duration format (default: 1m)</li> <li><code>--min-event-sync-interval=5s</code> The minimum interval between two consecutive synchronizations triggered from kubernetes events in duration format (default: 5s)</li> <li><code>--[no-]events</code> When enabled, in addition to running every interval, the reconciliation loop will get triggered when supported sources change (default: disabled)</li> </ul> <p>A general recommendation is to enable <code>--events</code> and keep <code>--min-event-sync-interval</code> relatively low to have a better responsiveness when records are created or updated inside the cluster. This should represent an acceptable propagation time between the creation of your k8s resources and the time they become registered in your DNS server.</p> <p>On a general manner, the higher the <code>--provider-cache-time</code>, the lower the impact on the rate limits, but also, the slower the recovery in case of a deletion. The <code>--provider-cache-time</code> value should hence be set to an acceptable time to automatically recover restore deleted records.</p> <p>\u270d\ufe0f Note that caching is done within the external-dns controller memory. You can invalidate the cache at any point in time by restarting it (for example doing a rolling update).</p>"},{"location":"docs/release/","title":"Release","text":""},{"location":"docs/release/#release-cycle","title":"Release cycle","text":"<p>Currently we don\u2019t release regularly. Whenever we think it makes sense to release a new version we do it. You might want to ask in our Slack channel external-dns when the next release will come out.</p>"},{"location":"docs/release/#staging-release-cycle","title":"Staging Release cycle","text":"<p>A new staging image is released weekly and can be found at gcr.io/k8s-staging-external-dns/external-dns.</p> <p>There is a time lag between merging changes into the master branch and the subsequent creation of the staging image.</p> <p>Example command to fetch <code>10</code> most recent staging images:</p> <pre><code>export EXT_DNS_VERSION=\"v0.15.1\"\ncurl -sLk https://gcr.io/v2/k8s-staging-external-dns/external-dns/tags/list | jq | grep \"$EXT_DNS_VERSION\" | tail -n 10\n</code></pre>"},{"location":"docs/release/#versioning-convention","title":"Versioning convention","text":"<p>These are the conventions that we will be using for releases following <code>0.7.6</code>:</p> <ul> <li> <p>Patch version should be updated if we need to merge bugfixes, e.g. provider a does need a fix in order make updates working again. I would see updating or improving documentation here.</p> </li> <li> <p>Minor version should be updated if new features are implemented in existing providers or new provider get introduced.</p> </li> <li> <p>Major version should be upgraded if we introduce breaking changes.</p> </li> </ul>"},{"location":"docs/release/#how-to-release-a-new-image","title":"How to release a new image","text":""},{"location":"docs/release/#prerequisite","title":"Prerequisite","text":"<p>We use https://github.com/cli/cli to automate the release process. Please install it according to the official documentation.</p> <p>You must be an official maintainer of the project to be able to do a release.</p>"},{"location":"docs/release/#steps","title":"Steps","text":"<ul> <li>Run <code>scripts/releaser.sh</code> to create a new GitHub release. Alternatively you can create a release in the GitHub UI making sure to click on the autogenerate release node feature.</li> <li>The step above will trigger the Kubernetes based CI/CD system Prow. Verify that a new image was built and uploaded to <code>gcr.io/k8s-staging-external-dns/external-dns</code>.</li> <li>Create a PR in the k8s.io repo by taking the current staging image using the sha256 digest. Once the PR is merged, the image will be live with the corresponding tag specified in the PR.</li> <li>See https://github.com/kubernetes/k8s.io/pull/540 for reference</li> <li>Verify that the image is pullable with the given tag (i.e. <code>v0.7.5</code>).</li> <li>Branch out from the default branch and run <code>scripts/kustomize-version-updater.sh</code> to update the image tag used in the kustomization.yaml.</li> <li>Create an issue to release the corresponding Helm chart via the chart release process (below) assigned to a chart maintainer</li> <li>Create a PR with the kustomize change.</li> <li>Create a PR to replace all versions for docker images in the tutorials. A possible script to use is <code>sd registry.k8s.io/external-dns/external-dns:v0.15.1</code></li> <li>Once the PR is merged, all is done :-)</li> </ul>"},{"location":"docs/release/#how-to-release-a-new-chart-version","title":"How to release a new chart version","text":"<p>The chart needs to be released in response to an ExternalDNS image release or on an as-needed basis; this should be triggered by an issue to release the chart.</p>"},{"location":"docs/release/#steps_1","title":"Steps","text":"<ul> <li>Create a PR to update Chart.yaml with the ExternalDNS version in <code>appVersion</code>, agreed on chart release version in <code>version</code> and <code>annotations</code> showing the changes</li> <li>Validate that the chart linting is successful</li> <li>Merge the PR to trigger a GitHub action to release the chart</li> </ul>"},{"location":"docs/ttl/","title":"Configure DNS record TTL (Time-To-Live)","text":"<p>An optional annotation <code>external-dns.alpha.kubernetes.io/ttl</code> is available to customize the TTL value of a DNS record. TTL is specified as an integer encoded as string representing seconds.</p> <p>To configure it, simply annotate a service/ingress, e.g.:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: nginx.external-dns-test.my-org.com.\n    external-dns.alpha.kubernetes.io/ttl: \"60\"\n  ...\n</code></pre> <p>TTL can also be specified as a duration value parsable by Golang time.ParseDuration:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: nginx.external-dns-test.my-org.com.\n    external-dns.alpha.kubernetes.io/ttl: \"1m\"\n  ...\n</code></pre> <p>Both examples result in the same value of 60 seconds TTL.</p> <p>TTL must be a positive value.</p>"},{"location":"docs/ttl/#providers","title":"Providers","text":"<ul> <li> AWS (Route53)</li> <li> Azure</li> <li> Cloudflare</li> <li> DigitalOcean</li> <li> DNSimple</li> <li> Google</li> <li> InMemory</li> <li> Linode</li> <li> TransIP</li> <li> RFC2136</li> <li> UltraDNS</li> </ul> <p>PRs welcome!</p>"},{"location":"docs/ttl/#notes","title":"Notes","text":"<p>When the <code>external-dns.alpha.kubernetes.io/ttl</code> annotation is not provided, the TTL will default to 0 seconds and <code>endpoint.TTL.isConfigured()</code> will be false.</p>"},{"location":"docs/ttl/#aws-provider","title":"AWS Provider","text":"<p>The AWS Provider overrides the value to 300s when the TTL is 0. This value is a constant in the provider code.</p>"},{"location":"docs/ttl/#azure","title":"Azure","text":"<p>TTL value should be between 1 and 2,147,483,647 seconds. By default it will be 300s.</p>"},{"location":"docs/ttl/#cloudflare-provider","title":"CloudFlare Provider","text":"<p>CloudFlare overrides the value to \u201cauto\u201d when the TTL is 0.</p>"},{"location":"docs/ttl/#digitalocean-provider","title":"DigitalOcean Provider","text":"<p>The DigitalOcean Provider overrides the value to 300s when the TTL is 0. This value is a constant in the provider code.</p>"},{"location":"docs/ttl/#dnsimple-provider","title":"DNSimple Provider","text":"<p>The DNSimple Provider default TTL is used when the TTL is 0. The default TTL is 3600s.</p>"},{"location":"docs/ttl/#google-provider","title":"Google Provider","text":"<p>Previously with the Google Provider, TTL\u2019s were hard-coded to 300s. For safety, the Google Provider overrides the value to 300s when the TTL is 0. This value is a constant in the provider code.</p> <p>For the moment, it is impossible to use a TTL value of 0 with the AWS, DigitalOcean, or Google Providers. This behavior may change in the future.</p>"},{"location":"docs/ttl/#linode-provider","title":"Linode Provider","text":"<p>The Linode Provider default TTL is used when the TTL is 0. The default is 24 hours</p>"},{"location":"docs/ttl/#transip-provider","title":"TransIP Provider","text":"<p>The TransIP Provider minimal TTL is used when the TTL is 0. The minimal TTL is 60s.</p>"},{"location":"docs/ttl/#ultradns","title":"UltraDNS","text":"<p>The UltraDNS provider minimal TTL is used when the TTL is not provided. The default TTL is account level default TTL, if defined, otherwise 24 hours.</p>"},{"location":"docs/annotations/annotations/","title":"Annotations","text":"<p>ExternalDNS sources support a number of annotations on the Kubernetes resources that they examine.</p> <p>The following table documents which sources support which annotations:</p> Source controller hostname internal-hostname target ttl (provider-specific) Ambassador Yes Yes Yes Connector Contour Yes Yes<sup>1</sup> Yes Yes Yes CloudFoundry CRD F5 Yes Yes Gateway Yes Yes<sup>1</sup> Yes<sup>4</sup> Yes Yes Gloo Yes Yes<sup>5</sup> Yes<sup>5</sup> Ingress Yes Yes<sup>1</sup> Yes Yes Yes Istio Yes Yes<sup>1</sup> Yes Yes Yes Kong Yes<sup>1</sup> Yes Yes Yes Node Yes Yes Yes OpenShift Yes Yes<sup>1</sup> Yes Yes Yes Pod Yes Yes Yes Service Yes Yes<sup>1</sup> Yes<sup>1</sup><sup>2</sup> Yes<sup>3</sup> Yes Yes Skipper Yes Yes<sup>1</sup> Yes Yes Yes Traefik Yes<sup>1</sup> Yes Yes Yes"},{"location":"docs/annotations/annotations/#external-dnsalphakubernetesioaccess","title":"external-dns.alpha.kubernetes.io/access","text":"<p>Specifies which set of node IP addresses to use for a <code>Service</code> of type <code>NodePort</code>.</p> <p>If the value is <code>public</code>, use the Nodes\u2019 addresses of type <code>ExternalIP</code>, plus IPv6 addresses of type <code>InternalIP</code>.</p> <p>If the value is <code>private</code>, use the Nodes\u2019 addresses of type <code>InternalIP</code>.</p> <p>If the annotation is not present and there is at least one address of type <code>ExternalIP</code>, behave as if the value were <code>public</code>, otherwise behave as if the value were <code>private</code>.</p>"},{"location":"docs/annotations/annotations/#external-dnsalphakubernetesiocontroller","title":"external-dns.alpha.kubernetes.io/controller","text":"<p>If this annotation exists and has a value other than <code>dns-controller</code> then the source ignores the resource.</p>"},{"location":"docs/annotations/annotations/#external-dnsalphakubernetesioendpoints-type","title":"external-dns.alpha.kubernetes.io/endpoints-type","text":"<p>Specifies which set of addresses to use for a headless <code>Service</code>.</p> <p>If the value is <code>NodeExternalIP</code>, use each relevant <code>Pod</code>\u2019s <code>Node</code>\u2019s address of type <code>ExternalIP</code> plus each IPv6 address of type <code>InternalIP</code>.</p> <p>Otherwise, if the value is <code>HostIP</code> or the <code>--publish-host-ip</code> flag is specified, use each relevant <code>Pod</code>\u2019s <code>Status.HostIP</code>.</p> <p>Otherwise, use the <code>IP</code> of each of the <code>Service</code>\u2019s <code>Endpoints</code>\u2019s <code>Addresses</code>.</p>"},{"location":"docs/annotations/annotations/#external-dnsalphakubernetesiohostname","title":"external-dns.alpha.kubernetes.io/hostname","text":"<p>Specifies the domain for the resource\u2019s DNS records.</p> <p>Multiple hostnames can be specified through a comma-separated list, e.g. <code>svc.mydomain1.com,svc.mydomain2.com</code>.</p> <p>For <code>Pods</code>, uses the <code>Pod</code>\u2019s <code>Status.PodIP</code>, unless they are <code>hostNetwork: true</code> in which case the NodeExternalIP is used for IPv4 and NodeInternalIP for IPv6.</p>"},{"location":"docs/annotations/annotations/#external-dnsalphakubernetesioingress-hostname-source","title":"external-dns.alpha.kubernetes.io/ingress-hostname-source","text":"<p>Specifies where to get the domain for an <code>Ingress</code> resource.</p> <p>If the value is <code>defined-hosts-only</code>, use only the domains from the <code>Ingress</code> spec.</p> <p>If the value is <code>annotation-only</code>, use only the domains from the <code>Ingress</code> annotations.</p> <p>If the annotation is not present, use the domains from both the spec and annotations.</p>"},{"location":"docs/annotations/annotations/#external-dnsalphakubernetesiointernal-hostname","title":"external-dns.alpha.kubernetes.io/internal-hostname","text":"<p>Specifies the domain for the resource\u2019s DNS records that are for use from internal networks.</p> <p>For <code>Services</code> of type <code>LoadBalancer</code>, uses the <code>Service</code>\u2019s <code>ClusterIP</code>.</p> <p>For <code>Pods</code>, uses the <code>Pod</code>\u2019s <code>Status.PodIP</code>, unless they are <code>hostNetwork: true</code> in which case the NodeExternalIP is used for IPv4 and NodeInternalIP for IPv6.</p>"},{"location":"docs/annotations/annotations/#external-dnsalphakubernetesiotarget","title":"external-dns.alpha.kubernetes.io/target","text":"<p>Specifies a comma-separated list of values to override the resource\u2019s DNS record targets (RDATA).</p> <p>Targets that parse as IPv4 addresses are published as A records and targets that parse as IPv6 addresses are published as AAAA records. All other targets are published as CNAME records.</p>"},{"location":"docs/annotations/annotations/#external-dnsalphakubernetesiottl","title":"external-dns.alpha.kubernetes.io/ttl","text":"<p>Specifies the TTL (time to live) for the resource\u2019s DNS records.</p> <p>The value may be specified as either a duration or an integer number of seconds. It must be between 1 and 2,147,483,647 seconds.</p>"},{"location":"docs/annotations/annotations/#provider-specific-annotations","title":"Provider-specific annotations","text":"<p>Some providers define their own annotations. Cloud-specific annotations have keys prefixed as follows:</p> Cloud Annotation prefix AWS <code>external-dns.alpha.kubernetes.io/aws-</code> CloudFlare <code>external-dns.alpha.kubernetes.io/cloudflare-</code> IBM Cloud <code>external-dns.alpha.kubernetes.io/ibmcloud-</code> Scaleway <code>external-dns.alpha.kubernetes.io/scw-</code> <p>Additional annotations that are currently implemented only by AWS are:</p>"},{"location":"docs/annotations/annotations/#external-dnsalphakubernetesioalias","title":"external-dns.alpha.kubernetes.io/alias","text":"<p>If the value of this annotation is <code>true</code>, specifies that CNAME records generated by the resource should instead be alias records.</p> <p>This annotation is only relevant if the <code>--aws-prefer-cname</code> flag is specified.</p>"},{"location":"docs/annotations/annotations/#external-dnsalphakubernetesioset-identifier","title":"external-dns.alpha.kubernetes.io/set-identifier","text":"<p>Specifies the set identifier for DNS records generated by the resource.</p> <p>A set identifier differentiates among multiple DNS record sets that have the same combination of domain and type. Which record set or sets are returned to queries is then determined by the configured routing policy.</p> <ol> <li> <p>Unless the <code>--ignore-hostname-annotation</code> flag is specified.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Only behaves differently than <code>hostname</code> for <code>Service</code>s of type <code>ClusterIP</code> or <code>LoadBalancer</code>.\u00a0\u21a9</p> </li> <li> <p>Also supported on <code>Pods</code> referenced from a headless <code>Service</code>\u2019s <code>Endpoints</code>.\u00a0\u21a9</p> </li> <li> <p>The annotation must be on the <code>Gateway</code>.\u00a0\u21a9</p> </li> <li> <p>The annotation must be on the listener\u2019s <code>VirtualService</code>.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"docs/contributing/","title":"Developer Documentations (Advanced Topics)","text":"<p>This folder contains developer documentation.</p> <p>When you are ready to contribute, you can select issue at Good First Issues.</p> <p>To get started see: dev-guide.md.</p> <p>Note; when new feature/fix is ready, consider also to provide a way to test this manually with manifests and kubectl commands</p>"},{"location":"docs/contributing/#submit-an-issue","title":"Submit an Issue","text":"<p>In addition to contributions, we welcome bug reports and feature requests.</p>"},{"location":"docs/contributing/chart/","title":"Helm Chart","text":""},{"location":"docs/contributing/chart/#chart-changes","title":"Chart Changes","text":"<p>When contributing chart changes please follow the same process as when contributing other content but also please DON\u2019T modify Chart.yaml in the PR as this would result in a chart release when merged and will mean that your PR will need modifying before it can be accepted.</p> <p>The chart version will be updated as part of the PR to release the chart.</p> <p>Please DO add your changes to the CHANGELOG.md file in the chart directory under the <code>## [UNRELEASED]</code> section, if there isn\u2019t an uncommented <code>## [UNRELEASED]</code> section please copy the commented out template and use that.</p>"},{"location":"docs/contributing/design/","title":"Design","text":"<p>ExternalDNS\u2019s sources of DNS records live in package source. They implement the <code>Source</code> interface that has a single method <code>Endpoints</code> which returns the represented source\u2019s objects converted to <code>Endpoints</code>. Endpoints are just a tuple of DNS name and target where target can be an IP or another hostname.</p> <p>For example, the <code>ServiceSource</code> returns all Services converted to <code>Endpoints</code> where the hostname is the value of the <code>external-dns.alpha.kubernetes.io/hostname</code> annotation and the target is the IP of the load balancer or the target is the IP of the service ClusterIP.</p> <p>This list of endpoints is passed to the Plan which determines the difference between the current DNS records and the desired list of <code>Endpoints</code>.</p> <p>Once the difference has been figured out the list of intended changes is passed to a <code>Registry</code> which live in the registry package. The registry is a wrapper and access point to DNS provider. Registry implements the ownership concept by marking owned records and filtering out records not owned by ExternalDNS before passing them to DNS provider.</p> <p>The provider is the adapter to the DNS provider, e.g. Google Cloud DNS. It implements two methods: <code>ApplyChanges</code> to apply a set of changes filtered by <code>Registry</code> and <code>Records</code> to retrieve the current list of records from the DNS provider.</p> <p>The orchestration between the different components is controlled by the controller.</p> <p>You can pick which <code>Source</code> and <code>Provider</code> to use at runtime via the <code>--source</code> and <code>--provider</code> flags, respectively.</p>"},{"location":"docs/contributing/design/#adding-a-dns-provider","title":"Adding a DNS Provider","text":"<p>A typical way to start on, e.g. a CoreDNS provider, would be to add a <code>coredns.go</code> to the providers package and implement the interface methods. Then you would have to register your provider under a name in <code>main.go</code>, e.g. <code>coredns</code>, and would be able to trigger it\u2019s functions via setting <code>--provider=coredns</code>.</p> <p>Note, how your provider doesn\u2019t need to know anything about where the DNS records come from, nor does it have to figure out the difference between the current and the desired state, it merely executes the actions calculated by the plan.</p>"},{"location":"docs/contributing/design/#running-github-actions-locally","title":"Running GitHub Actions locally","text":"<p>You can also extend the CI workflow which is currently implemented as GitHub Action within the workflow folder. In order to test your changes before committing you can leverage act to run the GitHub Action locally.</p> <p>Follow the installation instructions in the nektos/act README.md. Afterwards just run <code>act</code> within the root folder of the project.</p> <p>For further usage of <code>act</code> refer to its documentation.</p>"},{"location":"docs/contributing/dev-guide/","title":"Developer Reference","text":"<p>The <code>external-dns</code> is the work of thousands of contributors, and is maintained by a small team within kubernetes-sigs. This document covers basic needs to work with <code>external-dns</code> codebase. It contains instructions to build, run, and test <code>external-dns</code>.</p>"},{"location":"docs/contributing/dev-guide/#tools","title":"Tools","text":"<p>Building and/or testing <code>external-dns</code> requires additional tooling.</p> <ul> <li>Git</li> <li>Go 1.24+</li> <li>Go modules</li> <li>golangci-lint</li> <li>ko</li> <li>kubectl</li> <li>helm</li> <li>spectral</li> <li>python</li> </ul>"},{"location":"docs/contributing/dev-guide/#first-steps","title":"First Steps","text":"<p>***Configure Development Environment**</p> <p>You must have a working Go environment, compile the build, and set up testing.</p> <pre><code>git clone https://github.com/kubernetes-sigs/external-dns.git &amp;&amp; cd external-dns\n</code></pre>"},{"location":"docs/contributing/dev-guide/#building-testing","title":"Building &amp; Testing","text":"<p>The project uses the make build system. It\u2019ll run code generators, tests and static code analysis.</p> <p>Build, run tests and lint the code:</p> <pre><code>make go-lint\nmake test\nmake cover-html\n</code></pre> <p>If added any flags or metrics, re-generate documentation</p> <pre><code>make generate-flags-documentation\nmake generate-metrics-documentation\n</code></pre> <p>We require all changes to be covered by acceptance tests and/or unit tests, depending on the situation. In the context of the <code>external-dns</code>, acceptance tests are tests of interactions with providers, such as creating, reading information about, and destroying DNS resources. In contrast, unit tests test functionality wholly within the codebase itself, such as function tests.</p>"},{"location":"docs/contributing/dev-guide/#continuous-integration","title":"Continuous Integration","text":"<p>When submitting a pull request, you\u2019ll notice that we run several automated processes on your proposed change. Some of these processes are tests to ensure your contribution aligns with our standards. While we strive for accuracy, some users may find these tests confusing.</p>"},{"location":"docs/contributing/dev-guide/#execute-code-without-building-binary","title":"Execute code without building binary","text":"<p>The <code>external-dns</code> does not require <code>make build</code>. You could compile and run Go program with the command</p> <pre><code>go run main.go \\\n    --provider=aws \\\n    --registry=txt \\\n    --source=fake \\\n    --log-level=info\n</code></pre> <p>For this command to run successfully, it will require AWS credentials and access to local or remote access.</p> <p>To run local cluster please refer to running local cluster</p>"},{"location":"docs/contributing/dev-guide/#deploying-a-local-build","title":"Deploying a local build","text":"<p>After building local images, it is often useful to deploy those images in a local cluster</p> <p>We use Minikube but it could be Kind or any other solution.</p> <ul> <li>Create local cluster</li> <li>Build and load local images</li> <li>Deploy with Helm</li> <li>Deploy with kubernetes manifests</li> </ul>"},{"location":"docs/contributing/dev-guide/#create-a-local-cluster","title":"Create a local cluster","text":"<p>For simplicity, minikube can be used to create a single node cluster.</p> <p>You can set a specific Kubernetes version by setting the node\u2019s container image. See basic controls within the documentation about configuration for more details on this.</p> <p>Once you have a configuration in place, create the cluster with that configuration:</p> <pre><code>minikube start \\\n  --profile=external-dns \\\n  --memory=2000 \\\n  --cpus=2 \\\n  --disk-size=5g \\\n  --kubernetes-version=v1.31 \\\n  --driver=docker\n\nminikube profile external-dns\n</code></pre> <p>After the new Kubernetes cluster is ready, identify the cluster is running as the single node cluster:</p> <pre><code>\u276f\u276f kubectl get nodes\nNAME           STATUS   ROLES           AGE   VERSION\nexternal-dns   Ready    control-plane   16s   v1.31.4\n</code></pre>"},{"location":"docs/contributing/dev-guide/#building-local-images","title":"Building local images","text":"<p>When building local images with ko you can\u2019t specify the registry used to create the image names. It will always be ko.local.</p> <ul> <li>minikube handbooks</li> </ul> <p>Note: You could skip this step if you build and push image to your private registry or using an official external-dns image</p> <pre><code>\u276f\u276f export KO_DOCKER_REPO=ko.local\n\u276f\u276f export VERSION=v1\n\u276f\u276f docker context use rancher-desktop ## (optional) this command is only required when using rancher-desktop\n\u276f\u276f ls -al /var/run/docker.sock ## (optional) validate tha docker runtime is configured correctly and symlink exist\n\n\u276f\u276f ko build --tags ${VERSION}\n\u276f\u276f docker images\n$$ ko.local/external-dns-9036f6870f30cbdefa42a10f30bada63   local-v1\n</code></pre> <p>Push image to minikube</p> <p>Refer to load image</p> <pre><code>\u276f\u276f minikube image load ko.local/external-dns-9036f6870f30cbdefa42a10f30bada63:local-v1\n\u276f\u276f minikube image ls\n$$ registry.k8s.io/pause:3.10\n$$ ...\n$$ ko.local/external-dns-9036f6870f30cbdefa42a10f30bada63:local-v1\n$$ ...\n\u276f\u276f kubectl run external-dns --image=ko.local/external-dns-9036f6870f30cbdefa42a10f30bada63:local-v1 --image-pull-policy=Never\n</code></pre> <p>Build and push directly in minikube</p> <p>Any <code>docker</code> command you run in this current terminal will run against the docker inside minikube cluster.</p> <p>Refer to push directly</p> <pre><code>\u276f\u276f eval $(minikube -p external-dns docker-env)\n\u276f\u276f echo $MINIKUBE_ACTIVE_DOCKERD\n$$ external-dns\n\u276f\u276f export VERSION=v1\n\u276f\u276f ko build --local --tags ${VERSION}\n\u276f\u276f docker images\n$$ REPOSITORY                                               TAG\n$$ registry.k8s.io/kube-apiserver                           v1.31.4\n$$ ....\n$$ ko.local/external-dns-9036f6870f30cbdefa42a10f30bada63   minikube-v1\n$$ ...\n\u276f\u276f eval $(minikube docker-env -u) ## unset minikube\n</code></pre> <p>Pushing to an in-cluster using Registry addon</p> <p>Refer to pushing images for a full configuration</p> <pre><code>\u276f\u276f export KO_DOCKER_REPO=$(minikube ip):5000\n\u276f\u276f export VERSION=registry-v1\n\u276f\u276f minikube addons enable registry\n\u276f\u276f ko build --tags ${VERSION}\n</code></pre>"},{"location":"docs/contributing/dev-guide/#building-image-and-push-to-a-registry","title":"Building image and push to a registry","text":"<p>Build container image and push to a specific registry</p> <pre><code>make build.push IMAGE=your-registry/external-dns\n</code></pre>"},{"location":"docs/contributing/dev-guide/#deploy-with-helm","title":"Deploy with Helm","text":"<p>Build local images if required, load them on a local cluster, and deploy helm charts, run:</p> <p>Render chart templates locally and display the output</p> <pre><code>\u276f\u276f helm lint --debug charts/external-dns\n\u276f\u276f helm template external-dns charts/external-dns --output-dir _scratch\n</code></pre> <p>Deploy manifests to a cluster with required values</p> <pre><code>\u276f\u276f kubectl apply -f _scratch --recursive=true\n</code></pre> <p>Modify chart or values and validate the diff</p> <pre><code>\u276f\u276f helm template external-dns charts/external-dns --output-dir _scratch\n\u276f\u276f kubectl diff -f _scratch/external-dns --recursive=true --show-managed-fields=false\n</code></pre>"},{"location":"docs/contributing/dev-guide/#helm-values","title":"Helm Values","text":"<p>This helm chart comes with a JSON schema generated from values with helm schema plugin.</p> <ol> <li>Install required plugin(s)</li> </ol> <pre><code>\u276f\u276f scripts/helm-tools.sh --install\n</code></pre> <ol> <li>Ensure that the schema is always up-to-date</li> </ol> <pre><code>\u276f\u276f scripts/helm-tools.sh --diff\n</code></pre> <ol> <li>When not up-to-date, update JSON schema</li> </ol> <pre><code>\u276f\u276f scripts/helm-tools.sh --schema\n</code></pre> <ol> <li>Runs a series of tests to verify that the chart is well-formed, linted and JSON schema is valid</li> </ol> <pre><code>\u276f\u276f scripts/helm-tools.sh --lint\n</code></pre> <ol> <li>Auto-generate documentation for helm charts into markdown files.</li> </ol> <pre><code>\u276f\u276f scripts/helm-tools.sh --docs\n</code></pre> <ol> <li>Run helm unittets.</li> </ol> <pre><code>\u276f\u276f make helm-test\n</code></pre> <ol> <li>Add an entry to the chart CHANGELOG.md under <code>## UNRELEASED</code> section and <code>open</code> pull request</li> </ol>"},{"location":"docs/contributing/dev-guide/#deploy-with-kubernetes-manifests","title":"Deploy with kubernetes manifests","text":"<p>Note; kubernetes manifest are not up to date. Consider to create an <code>examples</code> folder</p> <pre><code>kubectl apply -f kustomize --recursive=true --dry-run=client\n</code></pre>"},{"location":"docs/contributing/dev-guide/#contribute-to-documentation","title":"Contribute to documentation","text":"<p>All documentation is in <code>docs</code> folder. If new page is added or removed, make sure <code>mkdocs.yml</code> is also updated.</p> <p>Install required dependencies. In order to not to break system packages, we are going to use virtual environments with pipenv.</p> <pre><code>\u276f\u276f pipenv shell\n\u276f\u276f pip install -r docs/scripts/requirements.txt\n\u276f\u276f mkdocs serve\n$$ ...\n$$ Serving on http://127.0.0.1:8000/\n</code></pre>"},{"location":"docs/contributing/sources-and-providers/","title":"Sources and Providers","text":"<p>ExternalDNS supports swapping out endpoint sources and DNS providers and both sides are pluggable. There currently exist multiple sources for different provider implementations.</p>"},{"location":"docs/contributing/sources-and-providers/#sources","title":"Sources","text":"<p>Sources are an abstraction over any kind of source of desired Endpoints, e.g.:</p> <ul> <li>a list of Service objects from Kubernetes</li> <li>a random list for testing purposes</li> <li>an aggregated list of multiple nested sources</li> </ul> <p>The <code>Source</code> interface has a single method called <code>Endpoints</code> that should return all desired Endpoint objects as a flat list.</p> <pre><code>type Source interface {\n Endpoints() ([]*endpoint.Endpoint, error)\n}\n</code></pre> <p>All sources live in package <code>source</code>.</p> <ul> <li><code>ServiceSource</code>: collects all Services that have an external IP and returns them as Endpoint objects. The desired DNS name corresponds to an annotation set on the Service or is compiled from the Service attributes via the FQDN Go template string.</li> <li><code>IngressSource</code>: collects all Ingresses that have an external IP and returns them as Endpoint objects. The desired DNS name corresponds to the host rules defined in the Ingress object.</li> <li><code>IstioGatewaySource</code>: collects all Istio Gateways and returns them as Endpoint objects. The desired DNS name corresponds to the hosts listed within the servers spec of each Gateway object.</li> <li><code>ContourIngressRouteSource</code>: collects all Contour IngressRoutes and returns them as Endpoint objects. The desired DNS name corresponds to the <code>virtualhost.fqdn</code> listed within the spec of each IngressRoute object.</li> <li><code>FakeSource</code>: returns a random list of Endpoints for the purpose of testing providers without having access to a Kubernetes cluster.</li> <li><code>ConnectorSource</code>: returns a list of Endpoint objects which are served by a tcp server configured through <code>connector-source-server</code> flag.</li> <li><code>CRDSource</code>: returns a list of Endpoint objects sourced from the spec of CRD objects. For more details refer to CRD source documentation.</li> <li><code>EmptySource</code>: returns an empty list of Endpoint objects for the purpose of testing and cleaning out entries.</li> </ul>"},{"location":"docs/contributing/sources-and-providers/#providers","title":"Providers","text":"<p>Providers are an abstraction over any kind of sink for desired Endpoints, e.g.:</p> <ul> <li>storing them in Google Cloud DNS</li> <li>printing them to stdout for testing purposes</li> <li>fanning out to multiple nested providers</li> </ul> <p>The <code>Provider</code> interface has two methods: <code>Records</code> and <code>ApplyChanges</code>. <code>Records</code> should return all currently existing DNS records converted to Endpoint objects as a flat list. Upon receiving a change set (via an object of <code>plan.Changes</code>), <code>ApplyChanges</code> should translate these to the provider specific actions in order to persist them in the provider\u2019s storage.</p> <pre><code>type Provider interface {\n Records() ([]*endpoint.Endpoint, error)\n ApplyChanges(changes *plan.Changes) error\n}\n</code></pre> <p>The interface tries to be generic and assumes a flat list of records for both functions. However, many providers scope records into zones. Therefore, the provider implementation has to do some extra work to return that flat list. For instance, the AWS provider fetches the list of all hosted zones before it can return or apply the list of records. If the provider has no concept of zones or if it makes sense to cache the list of hosted zones it is happily allowed to do so. Furthermore, the provider should respect the <code>--domain-filter</code> flag to limit the affected records by a domain suffix. For instance, the AWS provider filters out all hosted zones that doesn\u2019t match that domain filter.</p> <p>All providers live in package <code>provider</code>.</p> <ul> <li><code>GoogleProvider</code>: returns and creates DNS records in Google Cloud DNS</li> <li><code>AWSProvider</code>: returns and creates DNS records in AWS Route 53</li> <li><code>AzureProvider</code>: returns and creates DNS records in Azure DNS</li> <li><code>InMemoryProvider</code>: Keeps a list of records in local memory</li> </ul>"},{"location":"docs/contributing/sources-and-providers/#usage","title":"Usage","text":"<p>You can choose any combination of sources and providers on the command line. Given a cluster on AWS you would most likely want to use the Service and Ingress Source in combination with the AWS provider. <code>Service</code> + <code>InMemory</code> is useful for testing your service collecting functionality, whereas <code>Fake</code> + <code>Google</code> is useful for testing that the Google provider behaves correctly, etc.</p>"},{"location":"docs/monitoring/","title":"Monitoring &amp; Observability","text":"<p>Monitoring is a crucial aspect of maintaining the health and performance of your applications. It involves collecting, analyzing, and using information to ensure that your system is running smoothly and efficiently. Effective monitoring helps in identifying issues early, understanding system behavior, and making informed decisions to improve performance and reliability.</p> <p>For <code>external-dns</code>, all metrics available for scraping are exposed on the <code>/metrics</code> endpoint. The metrics are in the Prometheus exposition format, which is widely used for monitoring and alerting.</p> <p>To access the metrics:</p> <pre><code>curl https://localhost:7979/metrics\n</code></pre> <p>In the metrics output, you\u2019ll see the help text, type information, and current value of the <code>external_dns_registry_endpoints_total</code> counter:</p> <pre><code># HELP external_dns_registry_endpoints_total Number of Endpoints in the registry\n# TYPE external_dns_registry_endpoints_total gauge\nexternal_dns_registry_endpoints_total 11\n</code></pre> <p>You can configure a locally running Prometheus instance to scrape metrics from the application. Here\u2019s an example prometheus.yml configuration:</p> <pre><code>scrape_configs:\n- job_name: external-dns\n  scrape_interval: 10s\n  static_configs:\n  - targets:\n    - localhost:7979\n</code></pre> <p>For more detailed information on how to instrument application with Prometheus, you can refer to the Prometheus Go client library documentation.</p>"},{"location":"docs/monitoring/#what-metrics-can-i-get-from-externaldns-and-what-do-they-mean","title":"What metrics can I get from ExternalDNS and what do they mean?","text":"<ul> <li>The project maintain a metrics page with a list of supported custom metrics.</li> <li>Go runtime metrics also available for scraping.</li> </ul> <p>ExternalDNS exposes 3 types of metrics: Sources, Registry errors and Cache hits.</p> <p><code>Source</code>s are mostly Kubernetes API objects. Examples of <code>source</code> errors may be connection errors to the Kubernetes API server itself or missing RBAC permissions. It can also stem from incompatible configuration in the objects itself like invalid characters, processing a broken fqdnTemplate, etc.</p> <p><code>Registry</code> errors are mostly Provider errors, unless there\u2019s some coding flaw in the registry package. Provider errors often arise due to accessing their APIs due to network or missing cloud-provider permissions when reading records. When applying a changeset, errors will arise if the changeset applied is incompatible with the current state.</p> <p>In case of an increased error count, you could correlate them with the <code>http_request_duration_seconds{handler=\"instrumented_http\"}</code> metric which should show increased numbers for status codes 4xx (permissions, configuration, invalid changeset) or 5xx (apiserver down).</p> <p>You can use the host label in the metric to figure out if the request was against the Kubernetes API server (Source errors) or the DNS provider API (Registry/Provider errors).</p>"},{"location":"docs/monitoring/metrics/","title":"Available Metrics","text":"<p>All metrics available for scraping are exposed on the <code>/metrics</code> endpoint. The metrics are in the Prometheus exposition format.</p> <p>To access the metrics:</p> <pre><code>curl https://localhost:7979/metrics\n</code></pre>"},{"location":"docs/monitoring/metrics/#supported-metrics","title":"Supported Metrics","text":"<p>Full metric name is constructed as follows: <code>external_dns_&lt;subsystem&gt;_&lt;name&gt;</code></p> Name Metric Type Subsystem Help last_reconcile_timestamp_seconds Gauge controller Timestamp of last attempted sync with the DNS provider last_sync_timestamp_seconds Gauge controller Timestamp of last successful sync with the DNS provider no_op_runs_total Counter controller Number of reconcile loops ending up with no changes on the DNS provider side. verified_a_records Gauge controller Number of DNS A-records that exists both in source and registry. verified_aaaa_records Gauge controller Number of DNS AAAA-records that exists both in source and registry. cache_apply_changes_calls Counter provider Number of calls to the provider cache ApplyChanges. cache_records_calls Counter provider Number of calls to the provider cache Records list. a_records Gauge registry Number of Registry A records. aaaa_records Gauge registry Number of Registry AAAA records. endpoints_total Gauge registry Number of Endpoints in the registry errors_total Counter registry Number of Registry errors. a_records Gauge source Number of Source A records. aaaa_records Gauge source Number of Source AAAA records. endpoints_total Gauge source Number of Endpoints in all sources errors_total Counter source Number of Source errors. adjustendpoints_errors_total Gauge webhook_provider Errors with AdjustEndpoints method adjustendpoints_requests_total Gauge webhook_provider Requests with AdjustEndpoints method applychanges_errors_total Gauge webhook_provider Errors with ApplyChanges method applychanges_requests_total Gauge webhook_provider Requests with ApplyChanges method records_errors_total Gauge webhook_provider Errors with Records method records_requests_total Gauge webhook_provider Requests with Records method"},{"location":"docs/monitoring/metrics/#available-go-runtime-metrics","title":"Available Go Runtime Metrics","text":"<p>The following Go runtime metrics are available for scraping. Please note that they may change over time and they are OS dependent.</p> Name go_gc_duration_seconds go_gc_gogc_percent go_gc_gomemlimit_bytes go_goroutines go_info go_memstats_alloc_bytes go_memstats_alloc_bytes_total go_memstats_buck_hash_sys_bytes go_memstats_frees_total go_memstats_gc_sys_bytes go_memstats_heap_alloc_bytes go_memstats_heap_idle_bytes go_memstats_heap_inuse_bytes go_memstats_heap_objects go_memstats_heap_released_bytes go_memstats_heap_sys_bytes go_memstats_last_gc_time_seconds go_memstats_mallocs_total go_memstats_mcache_inuse_bytes go_memstats_mcache_sys_bytes go_memstats_mspan_inuse_bytes go_memstats_mspan_sys_bytes go_memstats_next_gc_bytes go_memstats_other_sys_bytes go_memstats_stack_inuse_bytes go_memstats_stack_sys_bytes go_memstats_sys_bytes go_sched_gomaxprocs_threads go_threads http_request_duration_seconds process_cpu_seconds_total process_max_fds process_open_fds process_resident_memory_bytes process_start_time_seconds process_virtual_memory_bytes process_virtual_memory_max_bytes process_network_receive_bytes_total process_network_transmit_bytes_total"},{"location":"docs/proposal/001-leader-election/","title":"Leader Election","text":"<pre><code>---\ntitle: leader election proposal\nversion: 0.15.1\nauthors: @ivankatliarchuk\ncreation-date: 2025-01-30\nstatus: not-planned\n---\n</code></pre>"},{"location":"docs/proposal/001-leader-election/#leader-election","title":"Leader Election","text":"<p>In Kubernetes, leader election is a mechanism used by applications, controllers, or distributed systems to designate one instance or node as the \u201cleader\u201d that is responsible for managing specific tasks, while others operate as followers or standbys. This ensures coordinated and fault-tolerant operations in highly available systems.</p> <ul> <li>Kubernetes Coordinated Leader Election</li> <li>Kubernetes Concepts: Leases</li> </ul>"},{"location":"docs/proposal/001-leader-election/#leader-election-in-kubernetes","title":"Leader Election in Kubernetes","text":"<p>The leader election mechanism implemented in Go code relies on Kubernetes coordination features, specifically Lease object in the <code>coordination.k8s.io</code> API Group. Lease locks provide a way to acquire a lease on a shared resource, which can be used to determine the leader among a group of nodes.</p> <p>Leader Election Sequence Diagram</p> <pre><code>sequenceDiagram\n    participant R1 as Replica 1 (Leader)\n    participant LR as Lock Resource\n    participant R2 as Replica 2 (Standby)\n    participant R3 as Replica 3 (Standby)\n\n    R1-&gt;&gt;LR: Update Lock Resource\n    Note over LR: currentLeader: R1&lt;br&gt;timeStamp: 12:21&lt;br&gt;leaseDuration: 10s\n\n    loop Every polling period\n        R2-&gt;&gt;LR: Poll leader status\n        LR--&gt;&gt;R2: Return lock info\n        R3-&gt;&gt;LR: Poll leader status\n        LR--&gt;&gt;R3: Return lock info\n    end\n\n    Note over R2,R3: Replicas remain on standby&lt;br&gt;as long as leader is active</code></pre> <p>Leader Election Flow</p> <pre><code>graph TD\nsubgraph Active Replica\nA[Replica 1]\nend\nsubgraph Kubernetes Resource Lock\nA[\"fa:fa-server  Replica 1\"] --&gt; |Hold The Lock| C@{ label: \"Lock\" }\nend\nsubgraph Standby Replicas\n  D[\"fa:fa-server  Replica 2\"] --&gt;|Poll| C\n  E[\"fa:fa-server  Replica 3\"] --&gt;|Poll| C[\"fa:fa-lock Lock\"]\nend\n    style C color:#8C52FF,fill:#A6A6A6\n    style A color:#8C52FF,fill:#00BF63\n    style D color:#000000,fill:#FFDE59\n    style E color:#000000,fill:#FFDE59</code></pre> <p>How Leader Is Elected</p> <pre><code>flowchart TD\n    A[Start Leader Election] --&gt;|Replica 1 Becomes Leader| B(Update Lock Resource)\n    B --&gt; C{Is Leader Active?}\n    C --&gt;|Yes| D[Replicas 2 &amp; 3 Poll Leader Status]\n    C --&gt;|No| E[Trigger New Election]\n    E --&gt;|New Leader Found| F[Replica X Becomes Leader]\n    E --&gt;|No Leader| G[Retry Election]\n    F --&gt; B\n    G --&gt; C\n    D --&gt; C</code></pre>"},{"location":"docs/proposal/001-leader-election/#enable-leader-election","title":"Enable Leader Election","text":"<p>Minimum supported Kubernetes version is <code>v1.26</code>.</p> <p>Currently, this feature is \u201copt-in\u201d. The <code>--enable-leader-election</code> flag must be explicitly provided to activate it in the service.</p> Flag Description <code>--enable-leader-election</code> This flag is required to enable leader election logic <pre><code>args:\n   --registry=txt \\\n   --source=fake \\\n   --enable-leader-election\n</code></pre>"},{"location":"docs/proposal/001-leader-election/#how-leader-election-works-in-kubernetes","title":"How Leader Election Works in Kubernetes","text":"<ol> <li> <p>Lease API:    - Kubernetes provides a built-in <code>Lease</code> object in the <code>coordination.k8s.io/v1</code> API group, specifically designed for leader election.    - The leader writes a lease object with metadata (such as its identity and timestamp) to signal that it is the current leader.</p> </li> <li> <p>Election Process:    - All participating pods (or nodes) periodically check for the lease.    - The lease contains details of the current leader\u2019s identity (e.g., a pod name).    - If the lease expires or is not renewed, other contenders can try to acquire leadership by writing their identity into the lease object.</p> </li> <li> <p>Heartbeat (Lease Renewal):    - The current leader must periodically update the lease to retain leadership.    - If the leader fails to renew the lease within the configured timeout, leadership is relinquished, and another instance can take over.</p> </li> </ol>"},{"location":"docs/proposal/001-leader-election/#key-concepts","title":"Key Concepts","text":"<ul> <li>Lease Duration: Defines how long the leader is considered valid after the last lease renewal. Short lease durations result in faster failovers but higher contention and potential performance impact.</li> <li>Leader Identity: Usually the name or ID of the pod that holds the leadership role.</li> <li>Backoff and Contention: Followers typically wait and retry with a backoff period to avoid overwhelming the system when a leader is lost.</li> </ul>"},{"location":"docs/proposal/001-leader-election/#why-leader-election-is-important","title":"Why Leader Election is Important","text":"<p>Leader election ensures that:</p> <ul> <li>High Availability: Fail-over to a new leader ensures availability even if the current leader goes down.</li> <li>Data Consistency: Only one leader acts on critical tasks, preventing duplicate work or conflicting updates.</li> <li>Workload Distribution: Secondary replicas can be on standby, reducing resource contention.</li> </ul>"},{"location":"docs/proposal/001-leader-election/#use-cases","title":"Use Cases","text":"<p>Leader election functionality is critical for building reliable, fault-tolerant, and scalable applications on Kubernetes.</p> <ul> <li>Cluster Upgrades: Leader election ensures smooth cluster upgrades by designating one instance as responsible for orchestrating upgrades or managing specific components during the process. By preventing multiple instances from making changes concurrently, it avoids conflicts and reduces downtime, ensuring consistency across the cluster.</li> <li>Workload Running on Spot Instances: For workloads running on cost-effective but ephemeral spot instances, leader election is crucial for resiliency. When a spot instance running the leader is preempted, the failover process enables a standby instance to seamlessly take over leadership, ensuring continued execution of critical tasks.</li> <li>Requirement for Disaster Recovery: In disaster recovery scenarios, leader election provides fault tolerance by allowing another instance to take over when the primary leader becomes unavailable. This guarantees operational continuity even in the face of unexpected failures, supporting robust disaster recovery strategies.</li> <li>High Availability (HA) Scenarios: In highly available systems, leader election ensures that a single active leader manages essential processes or state, while backups remain ready to step in instantly in case of failure. This minimizes recovery time objectives (RTO) and eliminates single points of failure.</li> <li>Enhanced Reliability in Distributed Systems: Incorporating leader election into your distributed system enhances its overall reliability. It avoids the pitfalls of uncoordinated task execution, providing deterministic behavior and ensuring only one instance manages critical tasks at any given time.</li> <li>Conflict Prevention: Leader election serves as a guard against conflicts arising from multiple instances attempting to execute the same tasks. By ensuring that only the elected leader acts on shared resources or processes, it prevents data corruption, inconsistencies, and wasted computational effort.</li> </ul>"},{"location":"docs/proposal/002-internal-ipv6-handling-rollback/","title":"002 internal ipv6 handling rollback","text":"<pre><code>---\ntitle: \"Proposal: Rollback IPv6 internal Node IP exposure\"\nversion: if applicable\nauthors: @ivankatliarchuk, @szuecs, @mloiseleur\ncreation-date: 2025-01-01\nstatus: approved\n---\n</code></pre>"},{"location":"docs/proposal/002-internal-ipv6-handling-rollback/#introduce-feature-flag-for-ipv6-internal-node-ip-handling-in-external-dns-and-change-the-behavior","title":"Introduce Feature Flag for IPv6 Internal Node IP Handling in \u2018\u2019external-dns\u2019\u2019 and Change the behavior","text":""},{"location":"docs/proposal/002-internal-ipv6-handling-rollback/#summary","title":"Summary","text":"<p>This proposal aims to introduce a feature flag in \u2018external-dns\u2019 to control the handling of IPv6 internal node IPs. In the current version, the feature flag will default to the existing behavior. In the next <code>minor</code> or <code>minor+N</code> version, the default behavior will be reversed, encouraging users to adopt the new behavior while providing a transition period.</p>"},{"location":"docs/proposal/002-internal-ipv6-handling-rollback/#motivation","title":"Motivation","text":"<p>The discussion in issue#4566 and the subsequent pr#4574 and pr#4808 highlighted concerns regarding the treatment of IPv6 internal node IPs. To address these concerns without causing immediate disruption, a feature flag will allow users to opt-out the current behavior, providing flexibility during the transition.</p>"},{"location":"docs/proposal/002-internal-ipv6-handling-rollback/#goals","title":"Goals","text":"<ul> <li>Introduce feature to toggle the handling of IPv6 internal node IPs</li> </ul>"},{"location":"docs/proposal/002-internal-ipv6-handling-rollback/#non-goals","title":"Non-Goals","text":"<ul> <li>Propose/Add an annotation for this specific use case</li> <li>Provide support for <code>external-dns.alpha.kubernetes.io/expose-internal-ipv6</code> in follow-up releases.</li> <li>Managing dual annotation and flag may introduce complexity.</li> </ul>"},{"location":"docs/proposal/002-internal-ipv6-handling-rollback/#proposal","title":"Proposal","text":"<ul> <li>Introduce Feature Flag</li> <li>Add a feature flag, e.g., <code>--expose-internal-ipv6=true</code>, to control the handling of IPv6 internal node IPs.</li> <li> <p>In the current version, this flag will default to <code>true</code>, maintaining the existing behavior.</p> </li> <li> <p>Flip Default Behavior in Next Minor Version</p> </li> <li>In the subsequent minor release, change the default value of <code>--expose-internal-ipv6</code> to <code>false</code>, adopting the new behavior by default.</li> <li>Users can still override this behavior by explicitly setting the flag as needed.</li> </ul> <p>Proposed Changes in <code>source/node.go</code> file.</p> <pre><code>// IPv6 addresses are labeled as NodeInternalIP despite being usable externally as well.\nif addr.Type == v1.NodeInternalIP &amp;&amp; ns.exposeInternalIP &amp;&amp; ... {\n    pv6Addresses = append(ipv6Addresses, addr.Address)\n}\n</code></pre>"},{"location":"docs/proposal/002-internal-ipv6-handling-rollback/#user-stories","title":"User Stories","text":"<ul> <li> <p>As a cluster Operator or Administrator, I want to control the handling of IPv6 internal node IPs to align with defined network topology and configuration.</p> </li> <li> <p>As a SecDevOps, I want to ensure that <code>external-dns</code> does not expose internal IPv6 node addresses via public DNS records, so that I can prevent unintended data leaks and reduce the attack surface of my Kubernetes cluster.</p> </li> <li> <p>As a SecDevOps, I want to use a feature flag to selectively enable or disable the new IPv6 behavior in <code>external-dns</code>, so that I can evaluate its security impact before it becomes the default setting in future releases.</p> </li> <li> <p>As a SecDevOps, I want to use a feature flag to selectively enable or disable the new IPv6 behavior in <code>external-dns</code>, so that I can detect misconfigurations, act on potential security incidents, and ensure compliance with security policies.</p> </li> </ul>"},{"location":"docs/proposal/002-internal-ipv6-handling-rollback/#implementation-steps","title":"Implementation Steps","text":"<ul> <li>Code Changes:</li> <li> <p>Implement the feature flag in the \u2018external-dns\u2019 codebase to toggle the handling of IPv6 internal node IPs.</p> </li> <li> <p>Documentation:</p> </li> <li>Update the \u2018external-dns\u2019 documentation to include information about the feature flag, its purpose, and usage examples.</li> </ul>"},{"location":"docs/proposal/002-internal-ipv6-handling-rollback/#drawbacks","title":"Drawbacks","text":"<ul> <li>Introducing a feature flag adds complexity to the configuration and codebase.</li> <li>Changing default behavior in a future release may still cause issues for users who are unaware of the change.</li> </ul>"},{"location":"docs/proposal/002-internal-ipv6-handling-rollback/#alternatives","title":"Alternatives","text":"<ul> <li>Immediate Behavior Change</li> <li>Directly change the behavior without a feature flag, which could lead to unexpected issues for users.</li> <li>No Change</li> <li>Maintain the current behavior, potentially leaving the concerns unaddressed.</li> <li>Users may not be able to update an <code>external-dns</code> version due to security, compliance or any other concerns.</li> </ul>"},{"location":"docs/proposal/design-template/","title":"Design template","text":"<pre><code>---\ntitle: New Feature or Deprecation/Removal Proposal\nversion: if applicable\nauthors: you, me\ncreation-date: 2025-01-25 # format ISO 8601: YYYY-MM-DD\nstatus: draft|approved|rejected|not-planned|partially-implemented|implemented\n---\n</code></pre>"},{"location":"docs/proposal/design-template/#new-feature-or-deprecationremoval-proposal","title":"New Feature or Deprecation/Removal Proposal","text":""},{"location":"docs/proposal/design-template/#table-of-contents","title":"Table of Contents","text":"<p>// add it here</p>"},{"location":"docs/proposal/design-template/#summary","title":"Summary","text":"<p>Please provide a summary of this proposal.</p>"},{"location":"docs/proposal/design-template/#motivation","title":"Motivation","text":"<p>What is the motivation of this proposal? Why is it useful and relevant?</p>"},{"location":"docs/proposal/design-template/#goals","title":"Goals","text":"<p>What are the goals of this proposal, what\u2019s the problem we want to solve?</p>"},{"location":"docs/proposal/design-template/#non-goals","title":"Non-Goals","text":"<p>What are explicit non-goals of this proposal?</p>"},{"location":"docs/proposal/design-template/#proposal","title":"Proposal","text":"<p>How does the proposal look like?</p>"},{"location":"docs/proposal/design-template/#user-stories","title":"User Stories","text":"<p>How would users use this feature, what are their needs?</p>"},{"location":"docs/proposal/design-template/#api","title":"API","text":"<p>Please describe the API (CRD or other) and show some examples.</p>"},{"location":"docs/proposal/design-template/#behavior","title":"Behavior","text":"<p>How should the new CRD or feature behave? Are there edge cases?</p>"},{"location":"docs/proposal/design-template/#drawbacks","title":"Drawbacks","text":"<p>If we implement this feature, what are drawbacks and disadvantages of this approach?</p>"},{"location":"docs/proposal/design-template/#alternatives","title":"Alternatives","text":"<p>What alternatives do we have and what are their pros and cons?</p>"},{"location":"docs/proposal/multi-target/","title":"Multiple Targets per hostname","text":"<p>(November 2017)</p>"},{"location":"docs/proposal/multi-target/#purpose","title":"Purpose","text":"<p>One should be able to define multiple targets (IPs/Hostnames) in the same Kubernetes resource object and expect ExternalDNS create DNS record(s) with a specified hostname and all targets. So far the connection between k8s resources (ingress/services) and DNS records were not streamlined. This proposal aims to make the connection explicit, making k8s resources acquire or release certain DNS names. As long as the resource ingress/service owns the record it can have multiple targets enable iff they are specified in the same resource.</p>"},{"location":"docs/proposal/multi-target/#use-cases","title":"Use cases","text":"<p>See https://github.com/kubernetes-sigs/external-dns/issues/239</p>"},{"location":"docs/proposal/multi-target/#current-behaviour","title":"Current behaviour","text":"<p>(as of the moment of writing)</p> <p>Central piece of enabling multi-target is having consistent and correct behaviour in <code>plan</code> component in regards to how endpoints generated from kubernetes resources are mapped to dns records. Current implementation of the <code>plan</code> has inconsistent behaviour in the following scenarios, all of which must be resolved before multi-target support can be enabled in the provider implementations:</p> <ol> <li> <p>No records registered so far. Two different ingresses request same hostname but different targets, e.g. Ingress A: example.com -&gt; 1.1.1.1 and Ingress B: example.com -&gt; 2.2.2.2</p> <ul> <li>Current Behaviour: both are added to the \u201cCreate\u201d (records to be created) list and passed to Provider</li> <li>Expected Behaviour: only one (random/ or according to predefined strategy) should be chosen and passed to Provider</li> </ul> <p>NOTE: while this seems to go against multi-target support, this is done so no other resource can \u201chijack\u201d already created DNS record. Multi targets are supported only on per single resource basis</p> </li> <li> <p>Now let\u2019s say Ingress A was chosen and successfully created, but both ingress A and B are still there. So on next iteration ExternalDNS would see both again in the Desired list.</p> <ul> <li>Current Behaviour: DNS record target will change to that of Ingress B.</li> <li>Expected Behaviour: Ingress A should stay unchanged. Ingress B record is not created</li> </ul> </li> <li> <p>DNS record for Ingress A was created but its target has changed. Ingress B is still there</p> <ul> <li>Current Behaviour: Undetermined behaviour based on which ingress will be parsed last.</li> <li>Expected Behaviour: DNS record should point to the new target specified in A. Ingress B should still be ignored.</li> </ul> <p>NOTE: both 2. and 3. can be resolved if External DNS is aware which resource has already acquired DNS record</p> </li> <li> <p>Ingress C has multiple targets: 1.1.1.1 and 2.2.2.2</p> <ul> <li>Current Behaviour: Both targets are split into different endpoints and we end up in one of the cases above</li> <li>Expected Behaviour: Endpoint should contain list of targets and treated as one ingress object.</li> </ul> </li> </ol>"},{"location":"docs/proposal/multi-target/#requirements-and-assumptions","title":"Requirements and assumptions","text":"<p>For this feature to work we have to make sure that:</p> <ol> <li>DNS records are now owned by certain ingress/service resources. For External DNS it would mean that TXT records now should store back-reference for the resource this record was created for, i.e. <code>\"heritage=external-dns,external-dns/resource=ingress/default/my-ingress-object-name\"</code></li> <li> <p>DNS records are updated only:</p> <ul> <li> <p>If owning resource target list has changed</p> </li> <li> <p>If owning resource record is not found in the desired list (meaning it was deleted), therefore it will now be owned by another record. So its target list will be updated</p> </li> <li> <p>Changes related to other record properties (e.g. TTL)</p> </li> </ul> </li> <li> <p>All of the issues described in <code>Current Behaviour</code> sections are resolved</p> </li> </ol> <p>Once Create/Update/Delete lists are calculated correctly (this is where conflicts based on requested DNS names are resolved) they are passed to <code>provider</code>, where <code>provider</code> specific implementation will decide how to convert the structures into required formats. If DNS provider does not (or partially) support multi targets then it is up to the provider to make sure that the change list of records passed to the DNS provider API is valid. TODO: explain best strategy.</p> <p>Additionally see https://github.com/kubernetes-sigs/external-dns/issues/258</p>"},{"location":"docs/proposal/multi-target/#implementation-plan","title":"Implementation plan","text":"<p>Brief summary of open PRs and what they are trying to address:</p>"},{"location":"docs/proposal/multi-target/#prs","title":"PRs","text":"<ol> <li> <p>https://github.com/kubernetes-sigs/external-dns/pull/243 - first attempt to add support for multiple targets. It is lagging far behind from tip</p> <p>what it does: unfinished attempt to extend <code>Endpoint</code> struct, for it to allow multiple targets (essentially <code>target string -&gt; targets []string</code>)</p> <p>action: evaluate if rebasing makes sense, or we can just close it.</p> </li> <li> <p>https://github.com/kubernetes-sigs/external-dns/pull/261 - attempt to rework <code>plan</code> to make it work correctly with multiple targets.</p> <p>what it does : attempts to fix issues with <code>plan</code> described in <code>Current Behaviour</code> section above. Included tests reveal the current problem with <code>plan</code></p> <p>action: rebase on default branch and make necessary changes to satisfy requirements listed in this document including back-reference to owning record</p> </li> <li> <p>https://github.com/kubernetes-sigs/external-dns/pull/326 - attempt to add multiple target support.</p> <p>what it does: for each pair <code>DNS Name</code> + <code>Record Type</code> it aggregates all targets from the cluster and passes them to Provider. It adds basic support</p> <p>action: the <code>plan</code> logic will probably needs to be reworked, however the rest concerning support in Providers and extending <code>Endpoint</code> struct can be reused. Rebase on default branch and add missing pieces. Depends on <code>2</code>.</p> </li> </ol> <p>Related PRs: https://github.com/kubernetes-sigs/external-dns/pull/331/files,  https://github.com/kubernetes-sigs/external-dns/pull/347/files - aiming at AWS Route53 weighted records. These PRs should be considered after common agreement about the way to address multi-target support is achieved. Related discussion:  https://github.com/kubernetes-sigs/external-dns/issues/196</p>"},{"location":"docs/proposal/multi-target/#how-to-proceed-from-here","title":"How to proceed from here","text":"<p>The following steps are needed:</p> <ol> <li>Make sure consensus regarding the approach is achieved via collaboration on the current document</li> <li>Notify all PR (see above) authors about the agreed approach</li> <li> <p>Implementation:</p> <p>a. <code>Plan</code> is working as expected - either based on #261 above or from scratch. <code>Plan</code> should be working correctly regardless of multi-target support</p> <p>b. Extensive testing making sure new <code>plan</code> does not introduce any breaking changes</p> <p>c. Change Endpoint struct to support multiple targets - based on #326 - integrate it with new <code>plan</code> @sethpollack</p> <p>d. Make sure new endpoint format can still be used in providers which have only partial support for multi targets TODO: how ? . This is to be done by simply using first target in the targets list.</p> <p>e. Add support for multi target which are already addressed in #326. It goes alongside c. and can be based on the same PR @sethpollack. New providers added since then should maintain same functionality.</p> </li> <li> <p>Extensive testing on all providers before making new release</p> </li> <li>Update all related documentation and explain how multi targets are supported on per provider basis</li> <li>Think of introducing weighted records (see PRs section above) and making them configurable.</li> </ol>"},{"location":"docs/proposal/multi-target/#open-questions","title":"Open questions","text":"<ul> <li>Handling cases when ingress/service targets include both hostnames and IPs - postpone this until use cases occurs</li> <li>\u201cWeighted records scope\u201d: https://github.com/kubernetes-sigs/external-dns/issues/196 - this should be considered once multi-target support is implemented</li> </ul>"},{"location":"docs/registry/dynamodb/","title":"The DynamoDB registry","text":"<p>As opposed to the default TXT registry, the DynamoDB registry stores DNS record metadata in an AWS DynamoDB table instead of in TXT records in a hosted zone. This following tutorial extends Setting up ExternalDNS for Services on AWS to use the DynamoDB registry instead.</p>"},{"location":"docs/registry/dynamodb/#iam-permissions","title":"IAM permissions","text":"<p>The ExternalDNS IAM Policy must additionally be granted the following permissions:</p> <pre><code>    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"DynamoDB:DescribeTable\",\n        \"DynamoDB:PartiQLDelete\",\n        \"DynamoDB:PartiQLInsert\",\n        \"DynamoDB:PartiQLUpdate\",\n        \"DynamoDB:Scan\"\n      ],\n      \"Resource\": [\n        \"arn:aws:dynamodb:*:*:table/external-dns\"\n      ]\n    }\n</code></pre> <p>The region and account ID may be specified explicitly specified instead of using wildcards.</p>"},{"location":"docs/registry/dynamodb/#create-a-dynamodb-table","title":"Create a DynamoDB Table","text":"<p>By default, the DynamoDB registry stores data in the table named <code>external-dns</code> and it needs to exist before configuring ExternalDNS to use the DynamoDB registry. If the DynamoDB table has a different name, it may be specified using the <code>--dynamodb-table</code> flag. If the DynamoDB table is in a different region, it may be specified using the <code>--dynamodb-region</code> flag.</p> <p>The following command creates a DynamoDB table with the name: <code>external-dns</code>:</p> <p>The table must have a partition (HASH) key named <code>k</code> of type string (<code>S</code>) and the table must NOT have a sort (RANGE) key.</p> <pre><code>aws dynamodb create-table \\\n  --table-name external-dns \\\n  --attribute-definitions \\\n    AttributeName=k,AttributeType=S \\\n  --key-schema \\\n    AttributeName=k,KeyType=HASH \\\n  --provisioned-throughput \\\n    ReadCapacityUnits=5,WriteCapacityUnits=5 \\\n  --table-class STANDARD\n</code></pre>"},{"location":"docs/registry/dynamodb/#set-up-a-hosted-zone","title":"Set up a hosted zone","text":"<p>Follow Set up a hosted zone</p>"},{"location":"docs/registry/dynamodb/#modify-externaldns-deployment","title":"Modify ExternalDNS deployment","text":"<p>The ExternalDNS deployment from Deploy ExternalDNS needs the following modifications:</p> <ul> <li><code>--registry=txt</code> should be changed to <code>--registry=dynamodb</code></li> <li>Add <code>--dynamodb-table=external-dns</code> to specify the name of the DynamoDB table, its value defaults to <code>external-dns</code></li> <li>Add <code>--dynamodb-region=us-east-1</code> to specify the region of the DynamoDB table</li> </ul> <p>For example:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\n  labels:\n    app.kubernetes.io/name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-dns\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: external-dns\n    spec:\n      containers:\n        - name: external-dns\n          image: registry.k8s.io/external-dns/external-dns:v0.15.1\n          args:\n            - --source=service\n            - --source=ingress\n            - --domain-filter=example.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n            - --provider=aws\n            - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization\n            - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both)\n            - --registry=dynamodb # previously, --registry=txt\n            - --dynamodb-table=external-dns # defaults to external-dns\n            - --dynamodb-region=us-east-1 # set to the region the DynamoDB table in\n            - --txt-owner-id=my-hostedzone-identifier\n          env:\n            - name: AWS_DEFAULT_REGION\n              value: us-east-1 # change to region where EKS is installed\n      # # Uncomment below if using static credentials\n      #       - name: AWS_SHARED_CREDENTIALS_FILE\n      #        value: /.aws/credentials\n      #     volumeMounts:\n      #       - name: aws-credentials\n      #         mountPath: /.aws\n      #         readOnly: true\n      # volumes:\n      #   - name: aws-credentials\n      #     secret:\n      #       secretName: external-dns\n</code></pre>"},{"location":"docs/registry/dynamodb/#validate-externaldns-works","title":"Validate ExternalDNS works","text":"<p>Create either a Service or an Ingress and</p> <p>After roughly two minutes, check that the corresponding entry was created in the DynamoDB table:</p> <pre><code>aws dynamodb scan --table-name external-dns\n</code></pre> <p>This will show something like:</p> <pre><code>{\n    \"Items\": [\n        {\n            \"k\": {\n                \"S\": \"nginx.example.com#A#\"\n            },\n            \"o\": {\n                \"S\": \"my-identifier\"\n            },\n            \"l\": {\n                \"M\": {\n                    \"resource\": {\n                        \"S\": \"service/default/nginx\"\n                    }\n                }\n            }\n        }\n    ],\n    \"Count\": 1,\n    \"ScannedCount\": 1,\n    \"ConsumedCapacity\": null\n}\n</code></pre>"},{"location":"docs/registry/dynamodb/#clean-up","title":"Clean up","text":"<p>In addition to the clean up steps in Setting up ExternalDNS for Services on AWS, delete the DynamoDB table that was used as a registry.</p> <pre><code>aws dynamodb delete-table \\\n  --table-name external-dns\n</code></pre>"},{"location":"docs/registry/dynamodb/#caching","title":"Caching","text":"<p>The DynamoDB registry can optionally cache DNS records read from the provider. This can mitigate rate limits imposed by the provider.</p> <p>Caching is enabled by specifying a cache duration with the <code>--txt-cache-interval</code> flag.</p>"},{"location":"docs/registry/dynamodb/#migration-from-txt-registry","title":"Migration from TXT registry","text":"<p>If any ownership TXT records exist for the configured owner, the DynamoDB registry will migrate the metadata therein to the DynamoDB table. If any such TXT records exist, any previous values for <code>--txt-prefix</code>, <code>--txt-suffix</code>, <code>--txt-wildcard-replacement</code>, and <code>--txt-encrypt-aes-key</code> must be supplied.</p> <p>If TXT records are in the set of managed record types specified by <code>--managed-record-types</code>, it will then delete the ownership TXT records on a subsequent reconciliation.</p>"},{"location":"docs/registry/registry/","title":"Registries","text":"<p>A registry persists metadata pertaining to DNS records.</p> <p>The most important metadata is the owning external-dns deployment. This is specified using the <code>--txt-owner-id</code> flag, specifying a value unique to the deployment of external-dns and which doesn\u2019t change for the lifetime of the deployment. Deployments in different clusters but sharing a DNS zone need to use different owner IDs.</p> <p>The registry implementation is specified using the <code>--registry</code> flag.</p>"},{"location":"docs/registry/registry/#supported-registries","title":"Supported registries","text":"<ul> <li>txt (default) - Stores metadata in TXT records in the same provider.</li> <li>dynamodb - Stores metadata in an AWS DynamoDB table.</li> <li>noop - Passes metadata directly to the provider. For most providers, this means the metadata is not persisted.</li> <li>aws-sd - Stores metadata in AWS Service Discovery. Only usable with the <code>aws-sd</code> provider.</li> </ul>"},{"location":"docs/registry/txt/","title":"The TXT registry","text":"<p>The TXT registry is the default registry. It stores DNS record metadata in TXT records, using the same provider.</p>"},{"location":"docs/registry/txt/#record-format-options","title":"Record Format Options","text":"<p>The TXT registry supports two formats for storing DNS record metadata:</p> <ul> <li>Legacy format: Creates a TXT record without record type information</li> <li>New format: Creates a TXT record with record type information (e.g., \u2018a-\u2019 prefix for A records)</li> </ul> <p>By default, the TXT registry creates records in both formats for backwards compatibility. You can configure it to use only the new format by using the <code>--txt-new-format-only</code> flag. This reduces the number of TXT records created, which can be helpful when working with provider-specific record limits.</p> <p>Note: The following record types always use only the new format regardless of this setting:</p> <ul> <li>AAAA records</li> <li>Encrypted TXT records (when using <code>--txt-encrypt-enabled</code>)</li> </ul> <p>Example:</p> <pre><code># Default behavior - creates both formats\nexternal-dns --provider=aws --source=ingress --managed-record-types=A --managed-record-types=TXT\n\n# Only create new format records (alongside other required flags)\nexternal-dns --provider=aws --source=ingress --managed-record-types=A --managed-record-types=TXT --txt-new-format-only\n</code></pre> <p>The <code>--txt-new-format-only</code> flag should be used in addition to your existing external-dns configuration flags. It does not implicitly configure TXT record handling - you still need to specify <code>--managed-record-types=TXT</code> if you want external-dns to manage TXT records.</p>"},{"location":"docs/registry/txt/#migration-to-new-format-only","title":"Migration to New Format Only","text":"<p>When transitioning from dual-format to new-format-only records:</p> <ul> <li>Ensure all your <code>external-dns</code> instances support the new format</li> <li>Enable the <code>--txt-new-format-only</code> flag on your external-dns instances Manually clean up any existing legacy format TXT records from your DNS provider</li> </ul> <p>Note: <code>external-dns</code> will not automatically remove legacy format records when switching to new-format-only mode. You\u2019ll need to clean up the old records manually if desired.</p>"},{"location":"docs/registry/txt/#prefixes-and-suffixes","title":"Prefixes and Suffixes","text":"<p>In order to avoid having the registry TXT records collide with TXT or CNAME records created from sources, you can configure a fixed prefix or suffix to be added to the first component of the domain of all registry TXT records.</p> <p>The prefix or suffix may not be changed after initial deployment, lest the registry records be orphaned and the metadata be lost.</p> <p>The prefix or suffix may contain the substring <code>%{record_type}</code>, which is replaced with the record type of the DNS record for which it is storing metadata.</p> <p>The prefix is specified using the <code>--txt-prefix</code> flag and the suffix is specified using the <code>--txt-suffix</code> flag. The two flags are mutually exclusive.</p>"},{"location":"docs/registry/txt/#wildcard-replacement","title":"Wildcard Replacement","text":"<p>The <code>--txt-wildcard-replacement</code> flag specifies a string to use to replace the \u201c*\u201d in registry TXT records for wildcard domains. Without using this, registry TXT records for wildcard domains will have invalid domain syntax and be rejected by most providers.</p>"},{"location":"docs/registry/txt/#encryption","title":"Encryption","text":"<p>Registry TXT records may contain information, such as the internal ingress name or namespace, considered sensitive, , which attackers could exploit to gather information about your infrastructure. By encrypting TXT records, you can protect this information from unauthorized access.</p> <p>Encryption is enabled by setting the <code>--txt-encrypt-enabled</code>. The 32-byte AES-256-GCM encryption key must be specified in URL-safe base64 form (recommended) or be a plain text, using the <code>--txt-encrypt-aes-key=&lt;key&gt;</code> flag.</p> <p>Note that the key used for encryption should be a secure key and properly managed to ensure the security of your TXT records.</p>"},{"location":"docs/registry/txt/#generating-the-txt-encryption-key","title":"Generating the TXT Encryption Key","text":"<p>Python</p> <pre><code>python -c 'import os,base64; print(base64.urlsafe_b64encode(os.urandom(32)).decode())'\n</code></pre> <p>Bash</p> <pre><code>dd if=/dev/urandom bs=32 count=1 2&gt;/dev/null | base64 | tr -d -- '\\n' | tr -- '+/' '-_'; echo\n</code></pre> <p>OpenSSL</p> <pre><code>openssl rand -base64 32 | tr -- '+/' '-_'\n</code></pre> <p>PowerShell</p> <pre><code># Add System.Web assembly to session, just in case\nAdd-Type -AssemblyName System.Web\n[Convert]::ToBase64String([System.Text.Encoding]::UTF8.GetBytes([System.Web.Security.Membership]::GeneratePassword(32,4))).Replace(\"+\",\"-\").Replace(\"/\",\"_\")\n</code></pre> <p>Terraform</p> <pre><code>resource \"random_password\" \"txt_key\" {\n  length           = 32\n  override_special = \"-_\"\n}\n</code></pre>"},{"location":"docs/registry/txt/#manually-encryptingdecrypting-txt-records","title":"Manually Encrypting/Decrypting TXT Records","text":"<p>In some cases you might need to edit registry TXT records. The following example Go code encrypts and decrypts such records.</p> <pre><code>package main\n\nimport (\n \"fmt\"\n \"sigs.k8s.io/external-dns/endpoint\"\n)\n\nfunc main() {\n keys := []string{\n  \"ZPitL0NGVQBZbTD6DwXJzD8RiStSazzYXQsdUowLURY=\", // safe base64 url encoded 44 bytes and 32 when decoded\n  \"01234567890123456789012345678901\",             // plain txt 32 bytes\n  \"passphrasewhichneedstobe32bytes!\",             // plain txt 32 bytes\n }\n\n for _, k := range keys {\n  key := []byte(k)\n  if len(key) != 32 {\n   // if key is not a plain txt let's decode\n   var err error\n   if key, err = b64.StdEncoding.DecodeString(string(key)); err != nil || len(key) != 32 {\n    fmt.Errorf(\"the AES Encryption key must have a length of 32 byte\")\n   }\n  }\n  encrypted, _ := endpoint.EncryptText(\n   \"heritage=external-dns,external-dns/owner=example,external-dns/resource=ingress/default/example\",\n   key,\n   nil,\n  )\n  decrypted, _, err := endpoint.DecryptText(encrypted, key)\n  if err != nil {\n   fmt.Println(\"Error decrypting:\", err, \"for key:\", k)\n  }\n  fmt.Println(decrypted)\n }\n}\n</code></pre>"},{"location":"docs/registry/txt/#caching","title":"Caching","text":"<p>The TXT registry can optionally cache DNS records read from the provider. This can mitigate rate limits imposed by the provider.</p> <p>Caching is enabled by specifying a cache duration with the <code>--txt-cache-interval</code> flag.</p>"},{"location":"docs/sources/about/","title":"About","text":"Source Resources annotation-filter label-filter ambassador-host Host.getambassador.io Yes Yes connector contour-httpproxy HttpProxy.projectcontour.io Yes cloudfoundry crd DNSEndpoint.externaldns.k8s.io Yes Yes f5-virtualserver VirtualServer.cis.f5.com Yes gateway-grpcroute GRPCRoute.gateway.networking.k8s.io Yes Yes gateway-httproute HTTPRoute.gateway.networking.k8s.io Yes Yes gateway-tcproute TCPRoute.gateway.networking.k8s.io Yes Yes gateway-tlsroute TLSRoute.gateway.networking.k8s.io Yes Yes gateway-udproute UDPRoute.gateway.networking.k8s.io Yes Yes gloo-proxy Proxy.gloo.solo.io ingress Ingress.networking.k8s.io Yes Yes istio-gateway Gateway.networking.istio.io Yes istio-virtualservice VirtualService.networking.istio.io Yes kong-tcpingress TCPIngress.configuration.konghq.com Yes node Node Yes Yes openshift-route Route.route.openshift.io Yes Yes pod Pod service Service Yes Yes skipper-routegroup RouteGroup.zalando.org Yes traefik-proxy IngressRoute.traefik.io IngressRouteTCP.traefik.io IngressRouteUDP.traefik.io Yes"},{"location":"docs/sources/crd/","title":"CRD Source","text":"<p>CRD source provides a generic mechanism to manage DNS records in your favourite DNS provider supported by external-dns.</p>"},{"location":"docs/sources/crd/#details","title":"Details","text":"<p>CRD source watches for a user specified CRD to extract Endpoints from its <code>Spec</code>. So users need to create such a CRD and register it to the kubernetes cluster and then create new object(s) of the CRD specifying the Endpoints.</p>"},{"location":"docs/sources/crd/#registering-crd","title":"Registering CRD","text":"<p>Here is typical example of CRD API type which provides Endpoints to <code>CRD source</code>:</p> <pre><code>type TTL int64\ntype Targets []string\ntype ProviderSpecificProperty struct {\n    Name  string `json:\"name,omitempty\"`\n    Value string `json:\"value,omitempty\"`\n}\ntype ProviderSpecific []ProviderSpecificProperty\ntype Labels map[string]string\n\ntype Endpoint struct {\n    // The hostname of the DNS record\n    DNSName string `json:\"dnsName,omitempty\"`\n    // The targets the DNS record points to\n    Targets Targets `json:\"targets,omitempty\"`\n    // RecordType type of record, e.g. CNAME, A, SRV, TXT etc\n    RecordType string `json:\"recordType,omitempty\"`\n    // TTL for the record\n    RecordTTL TTL `json:\"recordTTL,omitempty\"`\n    // Labels stores labels defined for the Endpoint\n    // +optional\n    Labels Labels `json:\"labels,omitempty\"`\n    // ProviderSpecific stores provider specific config\n    // +optional\n    ProviderSpecific ProviderSpecific `json:\"providerSpecific,omitempty\"`\n}\n\ntype DNSEndpointSpec struct {\n    Endpoints []*Endpoint `json:\"endpoints,omitempty\"`\n}\n\ntype DNSEndpointStatus struct {\n    // The generation observed by the external-dns controller.\n    // +optional\n    ObservedGeneration int64 `json:\"observedGeneration,omitempty\"`\n}\n\n// +genclient\n// +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object\n\n// DNSEndpoint is the CRD wrapper for Endpoint\n// +k8s:openapi-gen=true\n// +kubebuilder:resource:path=dnsendpoints\n// +kubebuilder:subresource:status\ntype DNSEndpoint struct {\n    metav1.TypeMeta   `json:\",inline\"`\n    metav1.ObjectMeta `json:\"metadata,omitempty\"`\n\n    Spec   DNSEndpointSpec   `json:\"spec,omitempty\"`\n    Status DNSEndpointStatus `json:\"status,omitempty\"`\n}\n</code></pre> <p>Refer to kubebuilder to create and register the CRD.</p>"},{"location":"docs/sources/crd/#usage","title":"Usage","text":"<p>One can use CRD source by specifying <code>--source</code> flag with <code>crd</code> and specifying the ApiVersion and Kind of the CRD with <code>--crd-source-apiversion</code> and <code>crd-source-kind</code> respectively. for e.g:</p> <pre><code>build/external-dns --source crd --crd-source-apiversion externaldns.k8s.io/v1alpha1  --crd-source-kind DNSEndpoint --provider inmemory --once --dry-run\n</code></pre>"},{"location":"docs/sources/crd/#creating-dns-records","title":"Creating DNS Records","text":"<p>Create the objects of CRD type by filling in the fields of CRD and DNS record would be created accordingly.</p>"},{"location":"docs/sources/crd/#example","title":"Example","text":"<p>Here is an example CRD manifest generated by kubebuilder. Apply this to register the CRD</p> <pre><code>$ kubectl apply --validate=false -f docs/sources/crd/crd-manifest.yaml\ncustomresourcedefinition.apiextensions.k8s.io \"dnsendpoints.externaldns.k8s.io\" created\n</code></pre> <p>Then you can create the dns-endpoint yaml similar to dnsendpoint-example</p> <pre><code>$ kubectl apply -f docs/sources/crd/dnsendpoint-example.yaml\ndnsendpoint.externaldns.k8s.io \"examplednsrecord\" created\n</code></pre> <p>Run external-dns in dry-mode to see whether external-dns picks up the DNS record from CRD.</p> <pre><code>$ build/external-dns --source crd --crd-source-apiversion externaldns.k8s.io/v1alpha1  --crd-source-kind DNSEndpoint --provider inmemory --once --dry-run\nINFO[0000] running in dry-run mode. No changes to DNS records will be made.\nINFO[0000] Connected to cluster at https://192.168.99.100:8443\nINFO[0000] CREATE: foo.bar.com 180 IN A 192.168.99.216\nINFO[0000] CREATE: foo.bar.com 0 IN TXT \"heritage=external-dns,external-dns/owner=default\"\n</code></pre>"},{"location":"docs/sources/crd/#using-crd-source-to-manage-dns-records-in-different-dns-providers","title":"Using CRD source to manage DNS records in different DNS providers","text":"<p>CRD source provides a generic mechanism and declarative way to manage DNS records in different DNS providers using external-dns.</p> <p>Not all the record types are enabled by default so the required record types must be enabled by using <code>--managed-record-types</code>.</p> <pre><code>external-dns --source=crd \\\n  --domain-filter=example.com \\\n  --managed-record-types=A \\\n  --managed-record-types=CNAME \\\n  --managed-record-types=NS\n</code></pre> <ul> <li>Example for record type <code>A</code></li> </ul> <pre><code>apiVersion: externaldns.k8s.io/v1alpha1\nkind: DNSEndpoint\nmetadata:\n  name: examplearecord\nspec:\n  endpoints:\n  - dnsName: example.com\n    recordTTL: 60\n    recordType: A\n    targets:\n    - 10.0.0.1\n</code></pre> <ul> <li>Example for record type <code>CNAME</code></li> </ul> <pre><code>apiVersion: externaldns.k8s.io/v1alpha1\nkind: DNSEndpoint\nmetadata:\n  name: examplecnamerecord\nspec:\n  endpoints:\n  - dnsName: test-a.example.com\n    recordTTL: 300\n    recordType: CNAME\n    targets:\n    - example.com\n</code></pre> <ul> <li>Example for record type <code>NS</code></li> </ul> <pre><code>apiVersion: externaldns.k8s.io/v1alpha1\nkind: DNSEndpoint\nmetadata:\n  name: ns-record\nspec:\n  endpoints:\n  - dnsName: zone.example.com\n    recordTTL: 300\n    recordType: NS\n    targets:\n    - ns1.example.com\n    - ns2.example.com\n</code></pre>"},{"location":"docs/sources/crd/#rbac-configuration","title":"RBAC configuration","text":"<p>If you use RBAC, extend the <code>external-dns</code> ClusterRole with:</p> <pre><code>- apiGroups: [\"externaldns.k8s.io\"]\n  resources: [\"dnsendpoints\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"externaldns.k8s.io\"]\n  resources: [\"dnsendpoints/status\"]\n  verbs: [\"*\"]\n</code></pre>"},{"location":"docs/sources/f5-transportserver/","title":"F5 Networks TransportServer Source","text":"<p>This tutorial describes how to configure ExternalDNS to use the F5 Networks TransportServer Source. It is meant to supplement the other provider-specific setup tutorials.</p> <p>The F5 Networks TransportServer CRD is part of this project. See more in-depth info regarding the TransportServer CRD here.</p>"},{"location":"docs/sources/f5-transportserver/#start-with-externaldns-with-the-f5-networks-transportserver-source","title":"Start with ExternalDNS with the F5 Networks TransportServer source","text":"<ol> <li> <p>Make sure that you have the <code>k8s-bigip-ctlr</code> installed in your cluster. The needed CRDs are bundled within the controller.</p> </li> <li> <p>In your Helm <code>values.yaml</code> add:</p> </li> </ol> <pre><code>sources:\n  - ...\n  - f5-transportserver\n  - ...\n</code></pre> <p>or add it in your <code>Deployment</code> if you aren\u2019t installing <code>external-dns</code> via Helm:</p> <pre><code>args:\n- --source=f5-transportserver\n</code></pre> <p>Note that, in case you\u2019re not installing via Helm, you\u2019ll need the following in the <code>ClusterRole</code> bound to the service account of <code>external-dns</code>:</p> <pre><code>- apiGroups:\n  - cis.f5.com\n  resources:\n  - transportservers\n  verbs:\n  - get\n  - list\n  - watch\n</code></pre>"},{"location":"docs/sources/f5-transportserver/#example-transportserver-cr-w-host-in-spec","title":"Example TransportServer CR w/ host in spec","text":"<pre><code>apiVersion: cis.f5.com/v1\nkind: TransportServer\nmetadata:\n  labels:\n    f5cr: 'true'\n  name: test-ts\n  namespace: test-ns\nspec:\n  bigipRouteDomain: 0\n  host: test.example.com\n  ipamLabel: vips\n  mode: standard\n  pool:\n    service: test-service\n    servicePort: 4222\n  virtualServerPort: 4222\n</code></pre>"},{"location":"docs/sources/f5-transportserver/#example-transportserver-cr-w-target-annotation-set","title":"Example TransportServer CR w/ target annotation set","text":"<p>If the <code>external-dns.alpha.kubernetes.io/target</code> annotation is set, the record created will reflect that and everything else will be ignored.</p> <pre><code>apiVersion: cis.f5.com/v1\nkind: TransportServer\nmetadata:\n  annotations:\n    external-dns.alpha.kubernetes.io/target: 10.172.1.12\n  labels:\n    f5cr: 'true'\n  name: test-ts\n  namespace: test-ns\nspec:\n  bigipRouteDomain: 0\n  host: test.example.com\n  ipamLabel: vips\n  mode: standard\n  pool:\n    service: test-service\n    servicePort: 4222\n  virtualServerPort: 4222\n</code></pre>"},{"location":"docs/sources/f5-transportserver/#example-transportserver-cr-w-virtualserveraddress-set","title":"Example TransportServer CR w/ VirtualServerAddress set","text":"<p>If <code>virtualServerAddress</code> is set, the record created will reflect that. <code>external-dns.alpha.kubernetes.io/target</code> will take precedence though.</p> <pre><code>apiVersion: cis.f5.com/v1\nkind: TransportServer\nmetadata:\n  labels:\n    f5cr: 'true'\n  name: test-ts\n  namespace: test-ns\nspec:\n  bigipRouteDomain: 0\n  host: test.example.com\n  ipamLabel: vips\n  mode: standard\n  pool:\n    service: test-service\n    servicePort: 4222\n  virtualServerPort: 4222\n  virtualServerAddress: 10.172.1.123\n</code></pre> <p>If there is no target annotation or <code>virtualServerAddress</code> field set, then it\u2019ll use the <code>VSAddress</code> field from the created TransportServer status to create the record.</p>"},{"location":"docs/sources/f5-virtualserver/","title":"F5 Networks VirtualServer Source","text":"<p>This tutorial describes how to configure ExternalDNS to use the F5 Networks VirtualServer Source. It is meant to supplement the other provider-specific setup tutorials.</p> <p>The F5 Networks VirtualServer CRD is part of this project. See more in-depth info regarding the VirtualServer CRD here.</p>"},{"location":"docs/sources/f5-virtualserver/#start-with-externaldns-with-the-f5-networks-virtualserver-source","title":"Start with ExternalDNS with the F5 Networks VirtualServer source","text":"<ol> <li> <p>Make sure that you have the <code>k8s-bigip-ctlr</code> installed in your cluster. The needed CRDs are bundled within the controller.</p> </li> <li> <p>In your Helm <code>values.yaml</code> add:</p> </li> </ol> <pre><code>sources:\n  - ...\n  - f5-virtualserver\n  - ...\n</code></pre> <p>or add it in your <code>Deployment</code> if you aren\u2019t installing <code>external-dns</code> via Helm:</p> <pre><code>args:\n- --source=f5-virtualserver\n</code></pre> <p>Note that, in case you\u2019re not installing via Helm, you\u2019ll need the following in the <code>ClusterRole</code> bound to the service account of <code>external-dns</code>:</p> <pre><code>- apiGroups:\n  - cis.f5.com\n  resources:\n  - virtualservers\n  verbs:\n  - get\n  - list\n  - watch\n</code></pre>"},{"location":"docs/sources/gateway-api/","title":"Gateway API Route Sources","text":"<p>This describes how to configure ExternalDNS to use Gateway API Route sources. It is meant to supplement the other provider-specific setup tutorials.</p>"},{"location":"docs/sources/gateway-api/#supported-api-versions","title":"Supported API Versions","text":"<p>ExternalDNS currently supports a mixture of v1alpha2, v1beta1, v1 APIs.</p> <p>Gateway API has two release channels: Standard and Experimental. The Experimental channel includes v1alpha2, v1beta2, and v1 APIs. The Standard channel only includes v1beta2 and v1 APIs, not v1alpha2.</p> <p>TCPRoutes, TLSRoutes, and UDPRoutes only exist in v1alpha2 and continued support for these versions is NOT guaranteed. At some time in the future, Gateway API will graduate these Routes to v1. ExternalDNS will likely follow that upgrade and move to the v1 API, where they will be available in the Standard release channel. This will be a breaking change if your Experimental CRDs are not updated to include the new v1 API.</p> <p>Gateways and HTTPRoutes are available in v1alpha2, v1beta1, and v1 APIs. However, some notable environments are behind in upgrading their CRDs to include the v1 API. For compatibility reasons Gateways and HTTPRoutes use the v1beta1 API.</p> <p>GRPCRoutes are available in v1alpha2 and v1 APIs, not v1beta2. Therefore, GRPCRoutes use the v1 API which is available in both release channels. Unfortunately, this means they will not be available in environments with old CRDs.</p>"},{"location":"docs/sources/gateway-api/#hostnames","title":"Hostnames","text":"<p>HTTPRoute and TLSRoute specs, along with their associated Gateway Listeners, contain hostnames that will be used by ExternalDNS. However, no such hostnames may be specified in TCPRoute or UDPRoute specs. For TCPRoutes and UDPRoutes, the <code>external-dns.alpha.kubernetes.io/hostname</code> annotation is the recommended way to provide their hostnames to ExternalDNS. This annotation is also supported for HTTPRoutes and TLSRoutes by ExternalDNS, but it\u2019s strongly recommended that they use their specs to provide all intended hostnames, since the Gateway that ultimately routes their requests/connections won\u2019t recognize additional hostnames from the annotation.</p>"},{"location":"docs/sources/gateway-api/#manifest-with-rbac","title":"Manifest with RBAC","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n  namespace: default\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"namespaces\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"gateway.networking.k8s.io\"]\n  resources: [\"gateways\",\"httproutes\",\"grpcroutes\",\"tlsroutes\",\"tcproutes\",\"udproutes\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        # Add desired Gateway API Route sources.\n        - --source=gateway-httproute\n        - --source=gateway-grpcroute\n        - --source=gateway-tlsroute\n        - --source=gateway-tcproute\n        - --source=gateway-udproute\n        # Optionally, limit Routes to those in the given namespace.\n        - --namespace=my-route-namespace\n        # Optionally, limit Routes to those matching the given label selector.\n        - --label-filter=my-route-label==my-route-value\n        # Optionally, limit Route endpoints to those Gateways with the given name.\n        - --gateway-name=my-gateway-name\n        # Optionally, limit Route endpoints to those Gateways in the given namespace.\n        - --gateway-namespace=my-gateway-namespace\n        # Optionally, limit Route endpoints to those Gateways matching the given label selector.\n        - --gateway-label-filter=my-gateway-label==my-gateway-value\n        # Add provider-specific flags...\n        - --domain-filter=external-dns-test.my-org.com\n        - --provider=google\n        - --registry=txt\n        - --txt-owner-id=my-identifier\n</code></pre>"},{"location":"docs/sources/gateway/","title":"Gateway sources","text":"<p>The gateway-grcproute, gateway-httproute, gateway-tcproute, gateway-tlsroute, and gateway-udproute sources create DNS entries based on their respective <code>gateway.networking.k8s.io</code> resources.</p>"},{"location":"docs/sources/gateway/#filtering-the-routes-considered","title":"Filtering the Routes considered","text":"<p>These sources support the <code>--label-filter</code> flag, which filters *Route resources by a set of labels.</p>"},{"location":"docs/sources/gateway/#domain-names","title":"Domain names","text":"<p>To calculate the Domain names created from a *Route, this source first collects a set of domain names from the *Route.</p> <p>It then iterates over each of the <code>status.parents</code> with a matching Gateway and at least one matching listener. For each matching listener, if the listener has a <code>hostname</code>, it narrows the set of domain names from the *Route to the portion that overlaps the <code>hostname</code>. If a matching listener does not have a <code>hostname</code>, it uses the un-narrowed set of domain names.</p>"},{"location":"docs/sources/gateway/#domain-names-from-route","title":"Domain names from Route","text":"<p>The set of domain names from a *Route is sourced from the following places:</p> <ul> <li> <p>If the *Route is a GRPCRoute, HTTPRoute, or TLSRoute, adds each of the<code>spec.hostnames</code>.</p> </li> <li> <p>Adds the hostnames from any <code>external-dns.alpha.kubernetes.io/hostname</code> annotation on the *Route.   This behavior is suppressed if the <code>--ignore-hostname-annotation</code> flag was specified.</p> </li> <li> <p>If no endpoints were produced by the previous steps   or the <code>--combine-fqdn-annotation</code> flag was specified, then adds hostnames   generated from any<code>--fqdn-template</code> flag.</p> </li> <li> <p>If no endpoints were produced by the previous steps, each   attached Gateway listener will use its <code>hostname</code>, if present.</p> </li> </ul>"},{"location":"docs/sources/gateway/#matching-gateways","title":"Matching Gateways","text":"<p>Matching Gateways are discovered by iterating over the *Route\u2019s <code>status.parents</code>:</p> <ul> <li> <p>Ignores parents with a <code>parentRef.group</code> other than <code>gateway.networking.k8s.io</code> or a <code>parentRef.kind</code> other than <code>Gateway</code>.</p> </li> <li> <p>If the <code>--gateway-name</code> flag was specified, ignores parents with a <code>parentRef.name</code> other than the   specified value.</p> </li> </ul> <p>For example, given the following HTTPRoute:</p> <pre><code>```yaml\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: echo\nspec:\n  hostnames:\n    - echoserver.example.org\n  parentRefs:\n    - group: networking.k8s.io\n      kind: Gateway\n      name: internal\n---\napiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: echo2\nspec:\n  hostnames:\n    - echoserver2.example.org\n  parentRefs:\n    - group: networking.k8s.io\n      kind: Gateway\n      name: external\n```\n</code></pre> <p>And using the <code>--gateway-name=external</code> flag, only the <code>echo2</code> HTTPRoute will be considered for DNS entries.</p> <ul> <li> <p>If the <code>--gateway-namespace</code> flag was specified, ignores parents with a <code>parentRef.namespace</code> other   than the specified value.</p> </li> <li> <p>If the <code>--gateway-label-filter</code> flag was specified, ignores parents whose Gateway does not match the   specified label filter.</p> </li> <li> <p>Ignores parents whose Gateway either does not exist or has not accepted the route.</p> </li> </ul>"},{"location":"docs/sources/gateway/#matching-listeners","title":"Matching listeners","text":"<p>Iterates over all listeners for the parent\u2019s <code>parentRef.sectionName</code>:</p> <ul> <li>Ignores listeners whose <code>protocol</code> field does not match the kind of the *Route per the following table:</li> </ul> kind protocols GRPCRoute HTTP, HTTPS HTTPRoute HTTP, HTTPS TCPRoute TCP TLSRoute TLS UDPRoute UDP <ul> <li> <p>If the parent\u2019s <code>parentRef.port</code> port is specified, ignores listeners without a matching <code>port</code>.</p> </li> <li> <p>Ignores listeners which specify an <code>allowedRoutes</code> which does not allow the route.</p> </li> </ul>"},{"location":"docs/sources/gateway/#targets","title":"Targets","text":"<p>The targets of the DNS entries created from a *Route are sourced from the following places:</p> <ol> <li> <p>If a matching parent Gateway has an <code>external-dns.alpha.kubernetes.io/target</code> annotation, uses    the values from that.</p> </li> <li> <p>Otherwise, iterates over that parent Gateway\u2019s <code>status.addresses</code>,    adding each address\u2019s <code>value</code>.</p> </li> </ol> <p>The targets from each parent Gateway matching the *Route are then combined and de-duplicated.</p>"},{"location":"docs/sources/gateway/#dualstack-routes","title":"Dualstack Routes","text":"<p>Gateway resources may be served from an external-loadbalancer which may support both IPv4 and \u201cdualstack\u201d (both IPv4 and IPv6) interfaces. When using the AWS Route53 provider, External DNS Controller will always create both A and AAAA alias DNS records by default, regardless of whether the load balancer is dual stack or not.</p>"},{"location":"docs/sources/gateway/#example","title":"Example","text":"<pre><code>apiVersion: gateway.networking.k8s.io/v1\nkind: HTTPRoute\nmetadata:\n  name: echo\nspec:\n  hostnames:\n    - echoserver.example.org\n  rules:\n    - backendRefs:\n        - group: \"\"\n          kind: Service\n          name: echo\n          port: 1027\n          weight: 1\n      matches:\n        - path:\n            type: PathPrefix\n            value: /echo\n</code></pre>"},{"location":"docs/sources/gloo-proxy/","title":"Gloo Proxy Source","text":"<p>This tutorial describes how to configure ExternalDNS to use the Gloo Proxy source. It is meant to supplement the other provider-specific setup tutorials.</p>"},{"location":"docs/sources/gloo-proxy/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        # update this to the desired external-dns version\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=gloo-proxy\n        - --gloo-namespace=custom-gloo-system # gloo system namespace. Specify multiple times for multiple namespaces. Omit to use the default (gloo-system)\n        - --provider=aws\n        - --registry=txt\n        - --txt-owner-id=my-identifier\n</code></pre>"},{"location":"docs/sources/gloo-proxy/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<p>Could be change if you have mulitple sources</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\",\"watch\"]\n- apiGroups: [\"gloo.solo.io\"]\n  resources: [\"proxies\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"gateway.solo.io\"]\n  resources: [\"virtualservices\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        # update this to the desired external-dns version\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=gloo-proxy\n        - --gloo-namespace=custom-gloo-system # gloo system namespace. Specify multiple times for multiple namespaces. Omit to use the default (gloo-system)\n        - --provider=aws\n        - --registry=txt\n        - --txt-owner-id=my-identifier\n</code></pre>"},{"location":"docs/sources/ingress/","title":"Ingress source","text":"<p>The ingress source creates DNS entries based on <code>Ingress.networking.k8s.io</code> resources.</p>"},{"location":"docs/sources/ingress/#filtering-the-ingresses-considered","title":"Filtering the Ingresses considered","text":"<p>The <code>--ingress-class</code> flag filters Ingress resources by a set of ingress classes. The flag may be specified multiple times in order to allow multiple ingress classes.</p> <p>This source supports the <code>--label-filter</code> flag, which filters Ingress resources by a set of labels.</p>"},{"location":"docs/sources/ingress/#domain-names","title":"Domain names","text":"<p>The domain names of the DNS entries created from an Ingress are sourced from the following places:</p> <ol> <li>Iterates over the Ingress\u2019s <code>spec.rules</code>, adding any non-empty <code>host</code>.</li> </ol> <p>This behavior is suppressed if the <code>--ignore-ingress-rules-spec</code> flag was specified or the Ingress had an <code>external-dns.alpha.kubernetes.io/ingress-hostname-source: annotation-only</code> annotation.</p> <ol> <li>Iterates over the Ingress\u2019s <code>spec.tls</code>, adding each member of <code>hosts</code>.</li> </ol> <p>This behavior is suppressed if the <code>--ignore-ingress-tls-spec</code> flag was specified or the Ingress had an <code>external-dns.alpha.kubernetes.io/ingress-hostname-source: annotation-only</code> annotation,</p> <ol> <li>Adds the hostnames from any <code>external-dns.alpha.kubernetes.io/hostname</code> annotation.</li> </ol> <p>This behavior is suppressed if the <code>--ignore-hostname-annotation</code> flag was specified or the Ingress had an <code>external-dns.alpha.kubernetes.io/ingress-hostname-source: defined-hosts-only</code> annotation.</p> <ol> <li>If no DNS entries were produced for an Ingress by the previous steps or the <code>--combine-fqdn-annotation</code> flag was specified, then adds hostnames generated from any<code>--fqdn-template</code> flag.</li> </ol>"},{"location":"docs/sources/ingress/#targets","title":"Targets","text":"<p>The targets of the DNS entries created from an Ingress are sourced from the following places:</p> <ol> <li> <p>If the Ingress has an <code>external-dns.alpha.kubernetes.io/target</code> annotation, uses the values from that.</p> </li> <li> <p>Otherwise, iterates over the Ingress\u2019s <code>status.loadBalancer.ingress</code>, adding each non-empty <code>ip</code> and <code>hostname</code>.</p> </li> </ol>"},{"location":"docs/sources/istio/","title":"Istio Gateway / Virtual Service Source","text":"<p>This tutorial describes how to configure ExternalDNS to use the Istio Gateway source. It is meant to supplement the other provider-specific setup tutorials.</p> <p>Note: Using the Istio Gateway source requires Istio &gt;=1.0.0.</p> <ul> <li>Manifest (for clusters without RBAC enabled)</li> <li>Manifest (for clusters with RBAC enabled)</li> <li>Update existing ExternalDNS Deployment</li> </ul>"},{"location":"docs/sources/istio/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service\n        - --source=ingress\n        - --source=istio-gateway        # choose one\n        - --source=istio-virtualservice # or both\n        - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n        - --provider=aws\n        - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization\n        - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both)\n        - --registry=txt\n        - --txt-owner-id=my-identifier\n</code></pre>"},{"location":"docs/sources/istio/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\"]\n- apiGroups: [\"networking.istio.io\"]\n  resources: [\"gateways\", \"virtualservices\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service\n        - --source=ingress\n        - --source=istio-gateway\n        - --source=istio-virtualservice\n        - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n        - --provider=aws\n        - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization\n        - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both)\n        - --registry=txt\n        - --txt-owner-id=my-identifier\n</code></pre>"},{"location":"docs/sources/istio/#update-existing-externaldns-deployment","title":"Update existing ExternalDNS Deployment","text":"<ul> <li>For clusters with running <code>external-dns</code>, you can just update the deployment.</li> <li>With access to the <code>kube-system</code> namespace, update the existing <code>external-dns</code> deployment.</li> <li>Add a parameter to the arguments of the container to create dns entries with <code>--source=istio-gateway</code>.</li> </ul> <p>Execute the following command or update the argument.</p> <pre><code>kubectl patch deployment external-dns --type='json' \\\n  -p='[{\"op\": \"add\", \"path\": \"/spec/template/spec/containers/0/args/2\", \"value\": \"--source=istio-gateway\" }]'\n</code></pre> <p>In case the setup uses a <code>clusterrole</code>, just append a new value to the enable the istio group.</p> <pre><code>kubectl patch clusterrole external-dns --type='json' \\\n  -p='[{\"op\": \"add\", \"path\": \"/rules/4\", \"value\": { \"apiGroups\": [ \"networking.istio.io\"], \"resources\": [\"gateways\"],\"verbs\": [\"get\", \"watch\", \"list\" ]} }]'\n</code></pre>"},{"location":"docs/sources/istio/#verify-that-istio-gatewayvirtualservice-source-works","title":"Verify that Istio Gateway/VirtualService Source works","text":"<p>Follow the Istio ingress traffic tutorial to deploy a sample service that will be exposed outside of the service mesh. The following are relevant snippets from that tutorial.</p>"},{"location":"docs/sources/istio/#install-a-sample-service","title":"Install a sample service","text":"<p>With automatic sidecar injection:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/istio/istio/release-1.6/samples/httpbin/httpbin.yaml\n</code></pre> <p>Otherwise:</p> <pre><code>kubectl apply -f &lt;(istioctl kube-inject -f https://raw.githubusercontent.com/istio/istio/release-1.6/samples/httpbin/httpbin.yaml)\n</code></pre>"},{"location":"docs/sources/istio/#using-a-gateway-as-a-source","title":"Using a Gateway as a source","text":""},{"location":"docs/sources/istio/#create-an-istio-gateway","title":"Create an Istio Gateway","text":"<pre><code>$ cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: httpbin-gateway\n  namespace: istio-system\nspec:\n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"httpbin.example.com\" # this is used by external-dns to extract DNS names\nEOF\n</code></pre>"},{"location":"docs/sources/istio/#configure-routes-for-traffic-entering-via-the-gateway","title":"Configure routes for traffic entering via the Gateway","text":"<pre><code>$ cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: httpbin\nspec:\n  hosts:\n  - \"httpbin.example.com\"\n  gateways:\n  - istio-system/httpbin-gateway\n  http:\n  - match:\n    - uri:\n        prefix: /status\n    - uri:\n        prefix: /delay\n    route:\n    - destination:\n        port:\n          number: 8000\n        host: httpbin\nEOF\n</code></pre>"},{"location":"docs/sources/istio/#using-a-virtualservice-as-a-source","title":"Using a VirtualService as a source","text":""},{"location":"docs/sources/istio/#create-an-istio-gateway_1","title":"Create an Istio Gateway","text":"<pre><code>$ cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: httpbin-gateway\n  namespace: istio-system\nspec:\n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"*\"\nEOF\n</code></pre>"},{"location":"docs/sources/istio/#configure-routes-for-traffic-entering-via-the-gateway_1","title":"Configure routes for traffic entering via the Gateway","text":"<pre><code>$ cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: httpbin\nspec:\n  hosts:\n  - \"httpbin.example.com\" # this is used by external-dns to extract DNS names\n  gateways:\n  - istio-system/httpbin-gateway\n  http:\n  - match:\n    - uri:\n        prefix: /status\n    - uri:\n        prefix: /delay\n    route:\n    - destination:\n        port:\n          number: 8000\n        host: httpbin\nEOF\n</code></pre> <p>To get the targets to the extracted DNS names, external-dns is able to gather information from the kubernetes service of the Istio Ingress Gateway. Please take a look at the source service documentation for more information on this.</p> <p>It is also possible to set the targets manually by using the <code>external-dns.alpha.kubernetes.io/target</code> annotation on the Istio Ingress Gateway resource or the Istio VirtualService.</p>"},{"location":"docs/sources/istio/#access-the-sample-service-using-curl","title":"Access the sample service using <code>curl</code>","text":"<pre><code>$ curl -I http://httpbin.example.com/status/200\nHTTP/1.1 200 OK\nserver: envoy\ndate: Tue, 28 Aug 2018 15:26:47 GMT\ncontent-type: text/html; charset=utf-8\naccess-control-allow-origin: *\naccess-control-allow-credentials: true\ncontent-length: 0\nx-envoy-upstream-service-time: 5\n</code></pre> <p>Accessing any other URL that has not been explicitly exposed should return an HTTP 404 error:</p> <pre><code>$ curl -I http://httpbin.example.com/headers\nHTTP/1.1 404 Not Found\ndate: Tue, 28 Aug 2018 15:27:48 GMT\nserver: envoy\ntransfer-encoding: chunked\n</code></pre> <p>Note: The <code>-H</code> flag in the original Istio tutorial is no longer necessary in the <code>curl</code> commands.</p>"},{"location":"docs/sources/istio/#optional-gateway-annotation","title":"Optional Gateway Annotation","text":"<p>To support setups where an Ingress resource is used provision an external LB you can add the following annotation to your Gateway</p> <p>Note: The Ingress namespace can be omitted if its in the same namespace as the gateway</p> <pre><code>$ cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: httpbin-gateway\n  namespace: istio-system\n  annotations:\n    \"external-dns.alpha.kubernetes.io/ingress\": \"$ingressNamespace/$ingressName\"\nspec:\n  selector:\n    istio: ingressgateway # use Istio default gateway implementation\n  servers:\n  - port:\n      number: 80\n      name: http\n      protocol: HTTP\n    hosts:\n    - \"*\"\nEOF\n</code></pre>"},{"location":"docs/sources/istio/#debug-externaldns","title":"Debug ExternalDNS","text":"<ul> <li>Look for the deployment pod to see the status</li> </ul> <p>```console$ kubectl get pods | grep external-dns external-dns-6b84999479-4knv9     1/1     Running   0   3h29m <pre><code>* Watch for the logs as follows\n\n```console\nkubectl logs -f external-dns-6b84999479-4knv9\n</code></pre></p> <p>At this point, you can <code>create</code> or <code>update</code> any <code>Istio Gateway</code> object with <code>hosts</code> entries array.</p> <p>ATTENTION: Make sure to specify those whose account is related to the DNS record.</p> <ul> <li>Successful executions will print the following</li> </ul> <pre><code>time=\"2020-01-17T06:08:08Z\" level=info msg=\"Desired change: CREATE httpbin.example.com A\"\ntime=\"2020-01-17T06:08:08Z\" level=info msg=\"Desired change: CREATE httpbin.example.com TXT\"\ntime=\"2020-01-17T06:08:08Z\" level=info msg=\"2 record(s) in zone example.com. were successfully updated\"\ntime=\"2020-01-17T06:09:08Z\" level=info msg=\"All records are already up to date, there are no changes for the matching hosted zones\"\n</code></pre> <ul> <li>If there\u2019s any problem around <code>clusterrole</code>, you would see the errors showing wrong permissions:</li> </ul> <pre><code>source \\\"gateways\\\" in API group \\\"networking.istio.io\\\" at the cluster scope\"\ntime=\"2020-01-17T06:07:08Z\" level=error msg=\"gateways.networking.istio.io is forbidden: User \\\"system:serviceaccount:kube-system:external-dns\\\" cannot list resource \\\"gateways\\\" in API group \\\"networking.istio.io\\\" at the cluster scope\"\n</code></pre>"},{"location":"docs/sources/kong/","title":"Kong TCPIngress Source","text":"<p>This tutorial describes how to configure ExternalDNS to use the Kong TCPIngress source. It is meant to supplement the other provider-specific setup tutorials.</p>"},{"location":"docs/sources/kong/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        # update this to the desired external-dns version\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=kong-tcpingress\n        - --provider=aws\n        - --registry=txt\n        - --txt-owner-id=my-identifier\n</code></pre>"},{"location":"docs/sources/kong/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<p>Could be changed if you have mulitple sources</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\",\"watch\"]\n- apiGroups: [\"configuration.konghq.com\"]\n  resources: [\"tcpingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        # update this to the desired external-dns version\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=kong-tcpingress\n        - --provider=aws\n        - --registry=txt\n        - --txt-owner-id=my-identifier\n</code></pre>"},{"location":"docs/sources/mx-record/","title":"MX record with CRD source","text":"<p>You can create and manage MX records with the help of CRD source and <code>DNSEndpoint</code> CRD. Currently, this feature is only supported by <code>aws</code>, <code>azure</code>, <code>google</code> and <code>digitalocean</code> providers.</p> <p>In order to start managing MX records you need to set the <code>--managed-record-types=MX</code> flag.</p> <pre><code>external-dns --source crd --provider {aws|azure|google|digitalocean} --managed-record-types=A --managed-record-types=CNAME --managed-record-types=MX\n</code></pre> <p>Targets within the CRD need to be specified according to the RFC 1034 (section 3.6.1). Below is an example of <code>example.com</code> DNS MX record which specifies two separate targets with distinct priorities.</p> <pre><code>apiVersion: externaldns.k8s.io/v1alpha1\nkind: DNSEndpoint\nmetadata:\n  name: examplemxrecord\nspec:\n  endpoints:\n    - dnsName: example.com\n      recordTTL: 180\n      recordType: MX\n      targets:\n        - 10 mailhost1.example.com\n        - 20 mailhost2.example.com\n</code></pre>"},{"location":"docs/sources/nodes/","title":"Cluster Nodes as Source","text":"<p>This tutorial describes how to configure ExternalDNS to use the cluster nodes as source. Using nodes (<code>--source=node</code>) as source is possible to synchronize a DNS zone with the nodes of a cluster.</p> <p>The node source adds an <code>A</code> record per each node <code>externalIP</code> (if not found, any IPv4 <code>internalIP</code> is used instead). It also adds an <code>AAAA</code> record per each node IPv6 <code>internalIP</code>. The TTL of the records can be set with the <code>external-dns.alpha.kubernetes.io/ttl</code> node annotation.</p> <p>Nodes marked as Unschedulable as per core/v1/NodeSpec are excluded. This avoid exposing Unhealthy, NotReady or SchedulingDisabled (cordon) nodes.</p>"},{"location":"docs/sources/nodes/#manifest-for-cluster-without-rbac-enabled","title":"Manifest (for cluster without RBAC enabled)","text":"<pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=node # will use nodes as source\n        - --provider=aws\n        - --zone-name-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n        - --domain-filter=external-dns-test.my-org.com\n        - --aws-zone-type=public\n        - --registry=txt\n        - --fqdn-template={{.Name}}.external-dns-test.my-org.com\n        - --txt-owner-id=my-identifier\n        - --policy=sync\n        - --log-level=debug\n</code></pre>"},{"location":"docs/sources/nodes/#manifest-for-cluster-with-rbac-enabled","title":"Manifest (for cluster with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"route.openshift.io\"]\n  resources: [\"routes\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: external-dns\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=node # will use nodes as source\n        - --provider=aws\n        - --zone-name-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n        - --domain-filter=external-dns-test.my-org.com\n        - --aws-zone-type=public\n        - --registry=txt\n        - --fqdn-template={{.Name}}.external-dns-test.my-org.com\n        - --txt-owner-id=my-identifier\n        - --policy=sync\n        - --log-level=debug\n</code></pre>"},{"location":"docs/sources/ns-record/","title":"NS record with CRD source","text":"<p>You can create NS records with the help of CRD source and <code>DNSEndpoint</code> CRD.</p> <p>In order to start managing NS records you need to set the <code>--managed-record-types=NS</code> flag.</p> <pre><code>external-dns --source crd --managed-record-types=A --managed-record-types=CNAME --managed-record-types=NS\n</code></pre> <p>Consider the following example</p> <pre><code>apiVersion: externaldns.k8s.io/v1alpha1\nkind: DNSEndpoint\nmetadata:\n  name: ns-record\nspec:\n  endpoints:\n  - dnsName: zone.example.com\n    recordTTL: 300\n    recordType: NS\n    targets:\n    - ns1.example.com\n    - ns2.example.com\n</code></pre> <p>After instantiation of this Custom Resource external-dns will create NS record with the help of configured provider, e.g. <code>aws</code></p>"},{"location":"docs/sources/openshift/","title":"OpenShift Route Source","text":"<p>This tutorial describes how to configure ExternalDNS to use the OpenShift Route source. It is meant to supplement the other provider-specific setup tutorials.</p>"},{"location":"docs/sources/openshift/#for-ocp-4x","title":"For OCP 4.x","text":"<p>In OCP 4.x, if you have multiple OpenShift ingress controllers then you must specify an ingress controller name (also called router name), you can get it from the route\u2019s <code>status.ingress[*].routerName</code> field. If you don\u2019t specify a router name when you have multiple ingress controllers in your cluster then the first router from the route\u2019s <code>status.ingress</code> will be used. Note that the router must have admitted the route in order to be selected. Once the router is known, ExternalDNS will use this router\u2019s canonical hostname as the target for the CNAME record.</p> <p>Starting from OCP 4.10 you can use ExternalDNS Operator to manage ExternalDNS instances. Example of its custom resource for AWS provider:</p> <pre><code>  apiVersion: externaldns.olm.openshift.io/v1alpha1\n  kind: ExternalDNS\n  metadata:\n    name: sample\n  spec:\n    provider:\n      type: AWS\n    source:\n      openshiftRouteOptions:\n        routerName: default\n      type: OpenShiftRoute\n    zones:\n      - Z05387772BD5723IZFRX3\n</code></pre> <p>This will create an ExternalDNS POD with the following container args in <code>external-dns</code> namespace:</p> <pre><code>spec:\n  containers:\n  - args:\n    - --metrics-address=127.0.0.1:7979\n    - --txt-owner-id=external-dns-sample\n    - --provider=aws\n    - --source=openshift-route\n    - --policy=sync\n    - --registry=txt\n    - --log-level=debug\n    - --zone-id-filter=Z05387772BD5723IZFRX3\n    - --openshift-router-name=default\n    - --txt-prefix=external-dns-\n</code></pre>"},{"location":"docs/sources/openshift/#for-ocp-311-environment","title":"For OCP 3.11 environment","text":""},{"location":"docs/sources/openshift/#prepare-router_canonical_hostname-in-defaultrouter-deployment","title":"Prepare ROUTER_CANONICAL_HOSTNAME in default/router deployment","text":"<p>Read and go through Finding the Host Name of the Router. If no ROUTER_CANONICAL_HOSTNAME is set, you must annotate each route with external-dns.alpha.kubernetes.io/target!</p>"},{"location":"docs/sources/openshift/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=openshift-route\n        - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n        - --provider=aws\n        - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization\n        - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both)\n        - --registry=txt\n        - --txt-owner-id=my-identifier\n</code></pre>"},{"location":"docs/sources/openshift/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\"]\n- apiGroups: [\"route.openshift.io\"]\n  resources: [\"routes\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=openshift-route\n        - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n        - --provider=aws\n        - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization\n        - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both)\n        - --registry=txt\n        - --txt-owner-id=my-identifier\n</code></pre>"},{"location":"docs/sources/openshift/#verify-external-dns-works-openshift-route-example","title":"Verify External DNS works (OpenShift Route example)","text":"<p>The following instructions are based on the Hello Openshift.</p>"},{"location":"docs/sources/openshift/#install-a-sample-service-and-expose-it","title":"Install a sample service and expose it","text":"<pre><code>$ oc apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hello-openshift\n  name: hello-openshift\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hello-openshift\n  template:\n    metadata:\n      labels:\n        app: hello-openshift\n    spec:\n      containers:\n      - image: openshift/hello-openshift\n        name: hello-openshift\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: hello-openshift\n  name: hello-openshift\nspec:\n  ports:\n  - port: 8080\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: hello-openshift\n  sessionAffinity: None\n  type: ClusterIP\n---\napiVersion: route.openshift.io/v1\nkind: Route\nmetadata:\n  name: hello-openshift\nspec:\n  host: hello-openshift.example.com\n  to:\n    kind: Service\n    name: hello-openshift\n    weight: 100\n  wildcardPolicy: None\nEOF\n</code></pre>"},{"location":"docs/sources/openshift/#access-the-sample-route-using-curl","title":"Access the sample route using <code>curl</code>","text":"<pre><code>$ curl -i http://hello-openshift.example.com\nHTTP/1.1 200 OK\nDate: Fri, 10 Apr 2020 09:36:41 GMT\nContent-Length: 17\nContent-Type: text/plain; charset=utf-8\n\nHello OpenShift!\n</code></pre>"},{"location":"docs/sources/pod/","title":"Pod Source","text":"<p>The pod source creates DNS entries based on <code>Pod</code> resources.</p>"},{"location":"docs/sources/pod/#pods-not-running-with-host-networking","title":"Pods not running with host networking","text":"<p>By default, the pod source will not consider the pods that aren\u2019t running with host networking enabled. You can override this behavior by using the <code>--ignore-non-host-network-pods</code> option.</p>"},{"location":"docs/sources/pod/#using-a-default-domain-for-pods","title":"Using a default domain for pods","text":"<p>By default, the pod source will look into the pod annotations to find the FQDN associated with a pod. You can also use the option <code>--pod-source-domain=example.org</code> to build the FQDN of the pods. The pod named \u201ctest-pod\u201d will then be registered as \u201ctest-pod.example.org\u201d.</p>"},{"location":"docs/sources/pod/#configuration-for-registering-all-pods-with-their-associated-ptr-record","title":"Configuration for registering all pods with their associated PTR record","text":"<p>A use case where combining these options can be pertinent is when you are running on-premise Kubernetes clusters without SNAT enabled for the pod network. You might want to register all the pods in the DNS with their associated PTR record so that the source of some traffic outside of the cluster can be rapidly associated with a workload using the \u201cnslookup\u201d or \u201cdig\u201d command on the pod IP. This can be particularly useful if you are running a large number of Kubernetes clusters.</p> <p>You will then use the following mix of options:</p> <ul> <li><code>--domain-filter=example.org</code></li> <li><code>--domain-filter=10.0.0.in-addr.arpa</code></li> <li><code>--source=pod</code></li> <li><code>--pod-source-domain=example.org</code></li> <li><code>--no-ignore-non-host-network-pods</code></li> <li><code>--rfc2136-create-ptr</code></li> <li><code>--rfc2136-zone=example.org</code></li> <li><code>--rfc2136-zone=10.0.0.in-addr.arpa</code></li> </ul>"},{"location":"docs/sources/service/","title":"Service source","text":"<p>The service source creates DNS entries based on <code>Service</code> resources.</p>"},{"location":"docs/sources/service/#filtering-the-services-considered","title":"Filtering the Services considered","text":"<p>The <code>--service-type-filter</code> flag filters Service resources by their <code>spec.type</code>. The flag may be specified multiple times to allow multiple service types.</p> <p>This source supports the <code>--label-filter</code> flag, which filters Service resources by a set of labels.</p>"},{"location":"docs/sources/service/#domain-names","title":"Domain names","text":"<p>The domain names of the DNS entries created from a Service are sourced from the following places:</p> <ol> <li> <p>Adds the domain names from any <code>external-dns.alpha.kubernetes.io/hostname</code> and/or <code>external-dns.alpha.kubernetes.io/internal-hostname</code> annotation. This behavior is suppressed if the <code>--ignore-hostname-annotation</code> flag was specified.</p> </li> <li> <p>If no DNS entries were produced for a Service by the previous steps and the <code>--compatibility</code> flag was specified, then adds DNS entries per the selected compatibility mode.</p> </li> <li> <p>If no DNS entries were produced for a Service by the previous steps or the <code>--combine-fqdn-annotation</code> flag was specified, then adds domain names generated from any<code>--fqdn-template</code> flag.</p> </li> </ol>"},{"location":"docs/sources/service/#domain-names-for-headless-service-pods","title":"Domain names for headless service pods","text":"<p>If a headless Service (without an <code>external-dns.alpha.kubernetes.io/target</code> annotation) creates DNS entries with targets from a Pod that has a non-empty <code>spec.hostname</code> field, additional DNS entries are created for that Pod, containing the targets from that Pod. For each domain name created for the Service, the additional DNS entry for the Pod has that domain name prefixed with the value of the Pod\u2019s <code>spec.hostname</code> field and a <code>.</code>.</p>"},{"location":"docs/sources/service/#targets","title":"Targets","text":"<p>If the Service has an <code>external-dns.alpha.kubernetes.io/target</code> annotation, uses the values from that. Otherwise, the targets of the DNS entries created from a service are sourced depending on the Service\u2019s <code>spec.type</code>:</p>"},{"location":"docs/sources/service/#loadbalancer","title":"LoadBalancer","text":"<ol> <li> <p>If the hostname came from an <code>external-dns.alpha.kubernetes.io/internal-hostname</code> annotation, uses the Service\u2019s <code>spec.clusterIP</code> field. If that field has the value <code>None</code>, does not generate any targets for the hostname.</p> </li> <li> <p>Otherwise, if the Service has one or more <code>spec.externalIPs</code>, uses the values in that field.</p> </li> <li> <p>Otherwise, iterates over each <code>status.loadBalancer.ingress</code>, adding any non-empty <code>ip</code> and/or <code>hostname</code>.</p> </li> </ol> <p>If the <code>--resolve-service-load-balancer-hostname</code> flag was specified, any non-empty <code>hostname</code> is queried through DNS and any resulting IP addresses are added instead. A DNS query failure results in zero targets being added for that load balancer\u2019s ingress hostname.</p>"},{"location":"docs/sources/service/#clusterip-headless","title":"ClusterIP (headless)","text":"<p>Iterates over all of the Service\u2019s Endpoints\u2019s <code>subsets.addresses</code>. If the Service\u2019s <code>spec.publishNotReadyAddresses</code> is <code>true</code> or the <code>--always-publish-not-ready-addresses</code> flag is specified, also iterates over the Endpoints\u2019s <code>subsets.notReadyAddresses</code>.</p> <ol> <li> <p>If an address does not target a <code>Pod</code> that matches the Service\u2019s <code>spec.selector</code>, it is ignored.</p> </li> <li> <p>If the target pod has an <code>external-dns.alpha.kubernetes.io/target</code> annotation, uses the values from that.</p> </li> <li> <p>Otherwise, if the Service has an <code>external-dns.alpha.kubernetes.io/endpoints-type: NodeExternalIP</code> annotation, uses the addresses from the Pod\u2019s Node\u2019s <code>status.addresses</code> that are either of type <code>ExternalIP</code> or IPv6 addresses of type <code>InternalIP</code>.</p> </li> <li> <p>Otherwise, if the Service has an <code>external-dns.alpha.kubernetes.io/endpoints-type: HostIP</code> annotation or the <code>--publish-host-ip</code> flag was specified, uses the Pod\u2019s <code>status.hostIP</code> field.</p> </li> <li> <p>Otherwise uses the <code>ip</code> field of the address from the Endpoints.</p> </li> </ol>"},{"location":"docs/sources/service/#clusterip-not-headless","title":"ClusterIP (not headless)","text":"<ol> <li> <p>If the hostname came from an <code>external-dns.alpha.kubernetes.io/internal-hostname</code> annotation or the <code>--publish-internal-services</code> flag was specified, uses the <code>spec.ClusterIP</code>.</p> </li> <li> <p>Otherwise, does not create any targets.</p> </li> </ol>"},{"location":"docs/sources/service/#nodeport","title":"NodePort","text":"<p>If <code>spec.ExternalTrafficPolicy</code> is <code>Local</code>, iterates over each Node that both matches the Service\u2019s <code>spec.selector</code> and has a <code>status.phase</code> of <code>Running</code>. Otherwise iterates over all Nodes, of any phase.</p> <p>Iterates over each relevant Node\u2019s <code>status.addresses</code>:</p> <ol> <li> <p>If there is an <code>external-dns.alpha.kubernetes.io/access: public</code> annotation on the Service, uses both addresses with a <code>type</code> of <code>ExternalIP</code> and IPv6 addresses with a <code>type</code> of <code>InternalIP</code>.</p> </li> <li> <p>Otherwise, if there is an <code>external-dns.alpha.kubernetes.io/access: private</code> annotation on the Service, uses addresses with a <code>type</code> of <code>InternalIP</code>.</p> </li> <li> <p>Otherwise, if there is at least one address with a <code>type</code> of <code>ExternalIP</code>, uses both addresses with a <code>type</code> of <code>ExternalIP</code> and IPv6 addresses with a <code>type</code> of <code>InternalIP</code>.</p> </li> <li> <p>Otherwise, uses addresses with a <code>type</code> of <code>InternalIP</code>.</p> </li> </ol> <p>Also iterates over the Service\u2019s <code>spec.ports</code>, creating a SRV record for each port which has a <code>nodePort</code>. The SRV record has a service of the Service\u2019s <code>name</code>, a protocol taken from the port\u2019s <code>protocol</code> field, a priority of <code>0</code> and a weight of <code>50</code>. In order for SRV records to be created, the <code>--managed-record-types</code> must have been specified, including <code>SRV</code> as one of the values.</p> <pre><code>external-dns ... --managed-record-types=A --managed-record-types=CNAME --managed-record-types=SRV\n</code></pre>"},{"location":"docs/sources/service/#externalname","title":"ExternalName","text":"<ol> <li>If the Service has one or more <code>spec.externalIPs</code>, uses the values in that field.</li> <li>Otherwise, creates a target with the value of the Service\u2019s <code>externalName</code> field.</li> </ol>"},{"location":"docs/sources/service/#endpoints-reconciliation","title":"Endpoints Reconciliation","text":"<p>By default, ExternalDNS does not watch for endpoint changes and does not automatically reconcile DNS records as the endpoints, as matched by the Service\u2019s selector. To enable reconcile on endpoints changes, you must specify the <code>--listen-endpoint-events</code> flag. However, be aware that this may increase the number of reconciliations performed by the controller, and the number of requests to the DNS provider.</p>"},{"location":"docs/sources/traefik-proxy/","title":"Traefik Proxy Source","text":"<p>This tutorial describes how to configure ExternalDNS to use the Traefik Proxy source. It is meant to supplement the other provider-specific setup tutorials.</p>"},{"location":"docs/sources/traefik-proxy/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        # update this to the desired external-dns version\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=traefik-proxy\n        - --provider=aws\n        - --registry=txt\n        - --txt-owner-id=my-identifier\n</code></pre>"},{"location":"docs/sources/traefik-proxy/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\",\"watch\"]\n- apiGroups: [\"traefik.containo.us\",\"traefik.io\"]\n  resources: [\"ingressroutes\", \"ingressroutetcps\", \"ingressrouteudps\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        # update this to the desired external-dns version\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=traefik-proxy\n        - --provider=aws\n        - --registry=txt\n        - --txt-owner-id=my-identifier\n</code></pre>"},{"location":"docs/sources/traefik-proxy/#deploying-a-traefik-ingressroute","title":"Deploying a Traefik IngressRoute","text":"<p>Create a IngressRoute file called \u2018traefik-ingress.yaml\u2019 with the following contents:</p> <pre><code>apiVersion: traefik.io/v1alpha1\nkind: IngressRoute\nmetadata:\n  name: traefik-ingress\n  annotations:\n    external-dns.alpha.kubernetes.io/target: traefik.example.com\n    kubernetes.io/ingress.class: traefik\nspec:\n  entryPoints:\n    - web\n    - websecure\n  routes:\n    - match: Host(`application.example.com`)\n      kind: Rule\n      services:\n        - name: service\n          namespace: namespace\n          port: port\n</code></pre> <p>Note the annotation on the IngressRoute (<code>external-dns.alpha.kubernetes.io/target</code>); use the same hostname as the traefik DNS.</p> <p>ExternalDNS uses this annotation to determine what services should be registered with DNS.</p> <p>Create the IngressRoute:</p> <pre><code>kubectl create -f traefik-ingress.yaml\n</code></pre> <p>Depending where you run your IngressRoute it can take a little while for ExternalDNS synchronize the DNS record.</p>"},{"location":"docs/sources/traefik-proxy/#cleanup","title":"Cleanup","text":"<p>Now that we have verified that ExternalDNS will automatically manage Traefik DNS records, we can delete the tutorial\u2019s example:</p> <pre><code>kubectl delete -f traefik-ingress.yaml\nkubectl delete -f externaldns.yaml\n</code></pre>"},{"location":"docs/sources/traefik-proxy/#additional-flags","title":"Additional Flags","text":"Flag Description \u2013traefik-disable-legacy Disable listeners on Resources under traefik.containo.us \u2013traefik-disable-new Disable listeners on Resources under traefik.io"},{"location":"docs/sources/traefik-proxy/#disabling-resource-listeners","title":"Disabling Resource Listeners","text":"<p>Traefik has deprecated the legacy API group, traefik.containo.us, in favor of traefik.io. By default the traefik-proxy source will listen for resources under both API groups; however, this may cause timeouts with the following message</p> <pre><code>FATA[0060] failed to sync traefik.io/v1alpha1, Resource=ingressroutes: context deadline exceeded\n</code></pre> <p>In this case you can disable one or the other API groups with <code>--traefik-disable-new</code> or <code>--traefik-disable-legacy</code></p>"},{"location":"docs/sources/txt-record/","title":"Creating TXT record with CRD source","text":"<p>You can create and manage TXT records with the help of CRD source and <code>DNSEndpoint</code> CRD. Currently, this feature is only supported by <code>digitalocean</code> providers.</p> <p>In order to start managing TXT records you need to set the <code>--managed-record-types=TXT</code> flag.</p> <pre><code>external-dns --source crd --provider {digitalocean} --managed-record-types=A --managed-record-types=CNAME --managed-record-types=TXT\n</code></pre> <p>Targets within the CRD need to be specified according to the RFC 1035 (section 3.3.14). Below is an example of <code>example.com</code> DNS TXT two records creation.</p> <p>NOTE Current implementation do not support RFC 6763 (section 6).</p> <pre><code>apiVersion: externaldns.k8s.io/v1alpha1\nkind: DNSEndpoint\nmetadata:\n  name: examplemxrecord\nspec:\n  endpoints:\n    - dnsName: example.com\n      recordTTL: 180\n      recordType: TXT\n      targets:\n        - SOMETXT\n        - ANOTHERTXT\n</code></pre>"},{"location":"docs/tutorials/akamai-edgedns/","title":"Akamai Edge DNS","text":""},{"location":"docs/tutorials/akamai-edgedns/#prerequisites","title":"Prerequisites","text":"<p>External-DNS v0.8.0 or greater.</p>"},{"location":"docs/tutorials/akamai-edgedns/#zones","title":"Zones","text":"<p>External-DNS manages service endpoints in existing DNS zones. The Akamai provider does not add, remove or configure new zones. The Akamai Control Center or Akamai DevOps Tools, Akamai CLI and Akamai Terraform Provider can create and manage Edge DNS zones.</p>"},{"location":"docs/tutorials/akamai-edgedns/#akamai-edge-dns-authentication","title":"Akamai Edge DNS Authentication","text":"<p>The Akamai Edge DNS provider requires valid Akamai Edgegrid API authentication credentials to access zones and manage  DNS records.</p> <p>Either directly by key or indirectly via a file can set credentials for the provider. The Akamai credential keys and mappings to the Akamai provider utilizing different presentation methods are:</p> Edgegrid Auth Key External-DNS Cmd Line Key Environment/ConfigMap Key Description host akamai-serviceconsumerdomain EXTERNAL_DNS_AKAMAI_SERVICECONSUMERDOMAIN Akamai Edgegrid API server access_token akamai-access-token EXTERNAL_DNS_AKAMAI_ACCESS_TOKEN Akamai Edgegrid API access token client_token akamai-client-token EXTERNAL_DNS_AKAMAI_CLIENT_TOKEN Akamai Edgegrid API client token client-secret akamai-client-secret EXTERNAL_DNS_AKAMAI_CLIENT_SECRET Akamai Edgegrid API client secret <p>In addition to specifying auth credentials individually, an Akamai Edgegrid .edgerc file convention can set credentials.</p> External-DNS Cmd Line Environment/ConfigMap Description akamai-edgerc-path EXTERNAL_DNS_AKAMAI_EDGERC_PATH Accessible path to Edgegrid credentials file, e.g /home/test/.edgerc akamai-edgerc-section EXTERNAL_DNS_AKAMAI_EDGERC_SECTION Section in Edgegrid credentials file containing credentials <p>Akamai API Authentication provides an overview and further information about authorization credentials for API base applications and tools.</p>"},{"location":"docs/tutorials/akamai-edgedns/#deploy-external-dns","title":"Deploy External-DNS","text":"<p>An operational External-DNS deployment consists of an External-DNS container and service. The following sections demonstrate the ConfigMap objects that would make up an example functional external DNS kubernetes configuration utilizing NGINX as the service.</p> <p>Connect your <code>kubectl</code> client to the External-DNS cluster.</p> <p>Begin by creating a Kubernetes secret to securely store your  Akamai Edge DNS Access Tokens. This key will enable ExternalDNS to authenticate with Akamai Edge DNS:</p> <pre><code>kubectl create secret generic AKAMAI-DNS --from-literal=EXTERNAL_DNS_AKAMAI_SERVICECONSUMERDOMAIN=YOUR_SERVICECONSUMERDOMAIN --from-literal=EXTERNAL_DNS_AKAMAI_CLIENT_TOKEN=YOUR_CLIENT_TOKEN --from-literal=EXTERNAL_DNS_AKAMAI_CLIENT_SECRET=YOUR_CLIENT_SECRET --from-literal=EXTERNAL_DNS_AKAMAI_ACCESS_TOKEN=YOUR_ACCESS_TOKEN\n</code></pre> <p>Ensure to replace YOUR_SERVICECONSUMERDOMAIN, EXTERNAL_DNS_AKAMAI_CLIENT_TOKEN, YOUR_CLIENT_SECRET and YOUR_ACCESS_TOKEN with your actual Akamai Edge DNS API keys.</p> <p>Then apply one of the following manifests file to deploy ExternalDNS.</p>"},{"location":"docs/tutorials/akamai-edgedns/#using-helm","title":"Using Helm","text":"<p>Create a values.yaml file to configure ExternalDNS to use Akamai Edge DNS as the DNS provider. This file should include the necessary environment variables:</p> <pre><code>provider:\n  name: akamai\nenv:\n  - name: EXTERNAL_DNS_AKAMAI_SERVICECONSUMERDOMAIN\n    valueFrom:\n      secretKeyRef:\n        name: AKAMAI-DNS\n        key: EXTERNAL_DNS_AKAMAI_SERVICECONSUMERDOMAIN\n  - name: EXTERNAL_DNS_AKAMAI_CLIENT_TOKEN\n    valueFrom:\n      secretKeyRef:\n        name: AKAMAI-DNS\n        key: EXTERNAL_DNS_AKAMAI_CLIENT_TOKEN\n  - name: EXTERNAL_DNS_AKAMAI_CLIENT_SECRET\n    valueFrom:\n      secretKeyRef:\n        name: AKAMAI-DNS\n        key: EXTERNAL_DNS_AKAMAI_CLIENT_SECRET\n  - name: EXTERNAL_DNS_AKAMAI_ACCESS_TOKEN\n    valueFrom:\n      secretKeyRef:\n        name: AKAMAI-DNS\n        key: EXTERNAL_DNS_AKAMAI_ACCESS_TOKEN\n</code></pre> <p>Finally, install the ExternalDNS chart with Helm using the configuration specified in your values.yaml file:</p> <pre><code>helm upgrade --install external-dns external-dns/external-dns --values values.yaml\n</code></pre>"},{"location":"docs/tutorials/akamai-edgedns/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service  # or ingress or both\n        - --provider=akamai\n        - --domain-filter=example.com\n        # zone-id-filter may be specified as well to filter on contract ID\n        - --registry=txt\n        - --txt-owner-id={{ owner-id-for-this-external-dns }}\n        - --txt-prefix={{ prefix label for TXT record }}.\n        env:\n        - name: EXTERNAL_DNS_AKAMAI_SERVICECONSUMERDOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: AKAMAI-DNS\n              key: EXTERNAL_DNS_AKAMAI_SERVICECONSUMERDOMAIN\n        - name: EXTERNAL_DNS_AKAMAI_CLIENT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: AKAMAI-DNS\n              key: EXTERNAL_DNS_AKAMAI_CLIENT_TOKEN\n        - name: EXTERNAL_DNS_AKAMAI_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: AKAMAI-DNS\n              key: EXTERNAL_DNS_AKAMAI_CLIENT_SECRET\n        - name: EXTERNAL_DNS_AKAMAI_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: AKAMAI-DNS\n              key: EXTERNAL_DNS_AKAMAI_ACCESS_TOKEN\n</code></pre>"},{"location":"docs/tutorials/akamai-edgedns/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"watch\", \"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service  # or ingress or both\n        - --provider=akamai\n        - --domain-filter=example.com\n        # zone-id-filter may be specified as well to filter on contract ID\n        - --registry=txt\n        - --txt-owner-id={{ owner-id-for-this-external-dns }}\n        - --txt-prefix={{ prefix label for TXT record }}.\n        env:\n        - name: EXTERNAL_DNS_AKAMAI_SERVICECONSUMERDOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: AKAMAI-DNS\n              key: EXTERNAL_DNS_AKAMAI_SERVICECONSUMERDOMAIN\n        - name: EXTERNAL_DNS_AKAMAI_CLIENT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: AKAMAI-DNS\n              key: EXTERNAL_DNS_AKAMAI_CLIENT_TOKEN\n        - name: EXTERNAL_DNS_AKAMAI_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: AKAMAI-DNS\n              key: EXTERNAL_DNS_AKAMAI_CLIENT_SECRET\n        - name: EXTERNAL_DNS_AKAMAI_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: AKAMAI-DNS\n              key: EXTERNAL_DNS_AKAMAI_ACCESS_TOKEN\n</code></pre> <p>Create the deployment for External-DNS:</p> <pre><code>kubectl apply -f externaldns.yaml\n</code></pre>"},{"location":"docs/tutorials/akamai-edgedns/#deploying-an-nginx-service","title":"Deploying an Nginx Service","text":"<p>Create a service file called \u2018nginx.yaml\u2019 with the following contents:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: nginx.example.com\n    external-dns.alpha.kubernetes.io/ttl: \"600\" #optional\nspec:\n  selector:\n    app: nginx\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre> <p>Create the deployment and service object:</p> <pre><code>kubectl apply -f nginx.yaml\n</code></pre>"},{"location":"docs/tutorials/akamai-edgedns/#verify-akamai-edge-dns-records","title":"Verify Akamai Edge DNS Records","text":"<p>Wait 3-5 minutes before validating the records to allow the record changes to propagate to all the Akamai name servers.</p> <p>Validate records using the Akamai Control Center or by executing a dig, nslookup or similar DNS command.</p>"},{"location":"docs/tutorials/akamai-edgedns/#cleanup","title":"Cleanup","text":"<p>Once you successfully configure and verify record management via External-DNS, you can delete the tutorial\u2019s examples:</p> <pre><code>kubectl delete -f nginx.yaml\nkubectl delete -f externaldns.yaml\n</code></pre>"},{"location":"docs/tutorials/akamai-edgedns/#additional-information","title":"Additional Information","text":"<ul> <li>The Akamai provider allows the administrative user to filter zones by both name (<code>domain-filter</code>) and contract Id (<code>zone-id-filter</code>). The Edge DNS API will return a \u2018500 Internal Error\u2019 for invalid contract Ids.</li> <li>The provider will substitute quotes in TXT records with a <code>`</code> (back tick) when writing records with the API.</li> </ul>"},{"location":"docs/tutorials/alibabacloud/","title":"Alibaba Cloud","text":"<p>This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster on Alibaba Cloud. Make sure to use &gt;=0.5.6 version of ExternalDNS for this tutorial</p>"},{"location":"docs/tutorials/alibabacloud/#ram-permissions","title":"RAM Permissions","text":"<pre><code>{\n  \"Version\": \"1\",\n  \"Statement\": [\n    {\n      \"Action\": \"alidns:AddDomainRecord\",\n      \"Resource\": \"*\",\n      \"Effect\": \"Allow\"\n    },\n    {\n      \"Action\": \"alidns:DeleteDomainRecord\",\n      \"Resource\": \"*\",\n      \"Effect\": \"Allow\"\n    },\n    {\n      \"Action\": \"alidns:UpdateDomainRecord\",\n      \"Resource\": \"*\",\n      \"Effect\": \"Allow\"\n    },\n    {\n      \"Action\": \"alidns:DescribeDomainRecords\",\n      \"Resource\": \"*\",\n      \"Effect\": \"Allow\"\n    },\n    {\n      \"Action\": \"alidns:DescribeDomains\",\n      \"Resource\": \"*\",\n      \"Effect\": \"Allow\"\n    },\n    {\n      \"Action\": \"pvtz:AddZoneRecord\",\n      \"Resource\": \"*\",\n      \"Effect\": \"Allow\"\n    },\n    {\n      \"Action\": \"pvtz:DeleteZoneRecord\",\n      \"Resource\": \"*\",\n      \"Effect\": \"Allow\"\n    },\n    {\n      \"Action\": \"pvtz:UpdateZoneRecord\",\n      \"Resource\": \"*\",\n      \"Effect\": \"Allow\"\n    },\n    {\n      \"Action\": \"pvtz:DescribeZoneRecords\",\n      \"Resource\": \"*\",\n      \"Effect\": \"Allow\"\n    },\n    {\n      \"Action\": \"pvtz:DescribeZones\",\n      \"Resource\": \"*\",\n      \"Effect\": \"Allow\"\n    },\n    {\n      \"Action\": \"pvtz:DescribeZoneInfo\",\n      \"Resource\": \"*\",\n      \"Effect\": \"Allow\"\n    }\n  ]\n}\n</code></pre> <p>When running on Alibaba Cloud, you need to make sure that your nodes (on which External DNS runs) have the RAM instance profile with the above RAM role assigned.</p>"},{"location":"docs/tutorials/alibabacloud/#set-up-a-alibaba-cloud-dns-service-or-private-zone-service","title":"Set up a Alibaba Cloud DNS service or Private Zone service","text":"<p>Alibaba Cloud DNS Service is the domain name resolution and management service for public access. It routes access from end-users to the designated web app. Alibaba Cloud Private Zone is the domain name resolution and management service for VPC internal access.</p> <p>If you prefer to try-out ExternalDNS in one of the existing domain or zone you can skip this step</p> <p>Create a DNS domain which will contain the managed DNS records. For public DNS service, the domain name should be valid and owned by yourself.</p> <pre><code>aliyun alidns AddDomain --DomainName \"external-dns-test.com\"\n</code></pre> <p>Make a note of the ID of the hosted zone you just created.</p> <pre><code>aliyun alidns DescribeDomains --KeyWord=\"external-dns-test.com\" | jq -r '.Domains.Domain[0].DomainId'\n</code></pre>"},{"location":"docs/tutorials/alibabacloud/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Connect your <code>kubectl</code> client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.</p>"},{"location":"docs/tutorials/alibabacloud/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service\n        - --source=ingress\n        - --domain-filter=external-dns-test.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n        - --provider=alibabacloud\n        - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization\n        - --alibaba-cloud-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both)\n        - --registry=txt\n        - --txt-owner-id=my-identifier\n        volumeMounts:\n        - mountPath: /usr/share/zoneinfo\n          name: hostpath\n      volumes:\n      - name: hostpath\n        hostPath:\n          path: /usr/share/zoneinfo\n          type: Directory\n</code></pre>"},{"location":"docs/tutorials/alibabacloud/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service\n        - --source=ingress\n        - --domain-filter=external-dns-test.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n        - --provider=alibabacloud\n        - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization\n        - --alibaba-cloud-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both)\n        - --registry=txt\n        - --txt-owner-id=my-identifier\n        - --alibaba-cloud-config-file= # enable sts token\n        volumeMounts:\n        - mountPath: /usr/share/zoneinfo\n          name: hostpath\n      volumes:\n      - name: hostpath\n        hostPath:\n          path: /usr/share/zoneinfo\n          type: Directory\n</code></pre>"},{"location":"docs/tutorials/alibabacloud/#arguments","title":"Arguments","text":"<p>This list is not the full list, but a few arguments that where chosen.</p>"},{"location":"docs/tutorials/alibabacloud/#alibaba-cloud-zone-type","title":"alibaba-cloud-zone-type","text":"<p><code>alibaba-cloud-zone-type</code> allows filtering for private and public zones</p> <ul> <li>If value is <code>public</code>, it will sync with records in Alibaba Cloud DNS Service</li> <li>If value is <code>private</code>, it will sync with records in Alibaba Cloud Private Zone Service</li> </ul>"},{"location":"docs/tutorials/alibabacloud/#verify-externaldns-works-ingress-example","title":"Verify ExternalDNS works (Ingress example)","text":"<p>Create an ingress resource manifest file.</p> <p>For ingress objects ExternalDNS will create a DNS record based on the host specified for the ingress object.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: foo\nspec:\n  ingressClassName: nginx # use the one that corresponds to your ingress controller.\n  rules:\n  - host: foo.external-dns-test.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: foo\n            port:\n              number: 80\n        pathType: Prefix\n</code></pre>"},{"location":"docs/tutorials/alibabacloud/#verify-externaldns-works-service-example","title":"Verify ExternalDNS works (Service example)","text":"<p>Create the following sample application to test that ExternalDNS works.</p> <p>For services ExternalDNS will look for the annotation <code>external-dns.alpha.kubernetes.io/hostname</code> on the service and use the corresponding value.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: nginx.external-dns-test.com.\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    name: http\n    targetPort: 80\n  selector:\n    app: nginx\n\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n          name: http\n</code></pre> <p>After roughly two minutes check that a corresponding DNS record for your service was created.</p> <pre><code>$ aliyun alidns DescribeDomainRecords --DomainName=external-dns-test.com\n{\n  \"PageNumber\": 1,\n  \"TotalCount\": 1,\n  \"PageSize\": 20,\n  \"RequestId\": \"1DBEF426-F771-46C7-9802-4989E9C94EE8\",\n  \"DomainRecords\": {\n    \"Record\": [\n      {\n        \"RR\": \"nginx\",\n        \"Status\": \"ENABLE\",\n        \"Value\": \"1.2.3.4\",\n        \"Weight\": 1,\n        \"RecordId\": \"3994015629411328\",\n        \"Type\": \"A\",\n        \"DomainName\": \"external-dns-test.com\",\n        \"Locked\": false,\n        \"Line\": \"default\",\n        \"TTL\": 600\n      }\uff0c\n      {\n        \"RR\": \"nginx\",\n        \"Status\": \"ENABLE\",\n        \"Value\": \"heritage=external-dns;external-dns/owner=my-identifier\",\n        \"Weight\": 1,\n        \"RecordId\": \"3994015629411329\",\n        \"Type\": \"TTL\",\n        \"DomainName\": \"external-dns-test.com\",\n        \"Locked\": false,\n        \"Line\": \"default\",\n        \"TTL\": 600\n      }\n    ]\n  }\n}\n</code></pre> <p>Note created TXT record alongside ALIAS record. TXT record signifies that the corresponding ALIAS record is managed by ExternalDNS. This makes ExternalDNS safe for running in environments where there are other records managed via other means.</p> <p>Let\u2019s check that we can resolve this DNS name. We\u2019ll ask the nameservers assigned to your zone first.</p> <pre><code>dig nginx.external-dns-test.com.\n</code></pre> <p>If you hooked up your DNS zone with its parent zone correctly you can use <code>curl</code> to access your site.</p> <pre><code>$ curl nginx.external-dns-test.com.\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n...\n&lt;/head&gt;\n&lt;body&gt;\n...\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"docs/tutorials/alibabacloud/#custom-ttl","title":"Custom TTL","text":"<p>The default DNS record TTL (Time-To-Live) is 300 seconds. You can customize this value by setting the annotation <code>external-dns.alpha.kubernetes.io/ttl</code>. e.g., modify the service manifest YAML file above:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: nginx.external-dns-test.com\n    external-dns.alpha.kubernetes.io/ttl: 60\nspec:\n    ...\n</code></pre> <p>This will set the DNS record\u2019s TTL to 60 seconds.</p>"},{"location":"docs/tutorials/alibabacloud/#clean-up","title":"Clean up","text":"<p>Make sure to delete all Service objects before terminating the cluster so all load balancers get cleaned up correctly.</p> <pre><code>kubectl delete service nginx\n</code></pre> <p>Give ExternalDNS some time to clean up the DNS records for you. Then delete the hosted zone if you created one for the testing purpose.</p> <pre><code>aliyun alidns DeleteDomain --DomainName external-dns-test.com\n</code></pre> <p>For more info about Alibaba Cloud external dns, please refer this docs</p>"},{"location":"docs/tutorials/aws-filters/","title":"AWS Filters","text":"<p>This document provides guidance on filtering AWS zones using various strategies and flags.</p>"},{"location":"docs/tutorials/aws-filters/#strategies-for-scoping-zones","title":"Strategies for Scoping Zones","text":"<p>Without specifying these flags, management applies to all zones.</p> <p>In order to manage specific zones,  there is a possibility to combine multiple options</p> Argument Description Flow Control <code>--zone-id-filter</code> Specify multiple times if needed OR <code>--domain-filter</code> By domain suffix - specify multiple times if needed OR <code>--regex-domain-filter</code> By domain suffix but as a regex - overrides domain-filter AND <code>--exclude-domains</code> To exclude a domain or subdomain OR <code>--regex-domain-exclusion</code> Subtracts its matches from <code>regex-domain-filter</code>\u2019s matches AND <code>--aws-zone-type</code> Only sync zones of this type <code>[public\\|private]</code> OR <code>--aws-zone-tags</code> Only sync zones with this tag AND <p>Minimum required configuration</p> <pre><code>args:\n    --provider=aws\n    --registry=txt\n    --source=service\n</code></pre>"},{"location":"docs/tutorials/aws-filters/#filter-by-zone-type","title":"Filter by Zone Type","text":"<p>If this flag is not specified, management applies to both public and private zones.</p> <pre><code>args:\n    --aws-zone-type=private|public # choose between public or private\n    ...\n</code></pre>"},{"location":"docs/tutorials/aws-filters/#filter-by-domain","title":"Filter by Domain","text":"<p>Specify multiple times if needed.</p> <pre><code>args:\n    --domain-filter=example.com\n    --domain-filter=.paradox.example.com\n    ...\n</code></pre> <p>Example <code>--domain-filter=example.com</code> will allow for zone <code>example.com</code> and any zones that end in <code>.example.com</code>, including <code>an.example.com</code>, i.e., the subdomains of example.com.</p> <p>When there are multiple domains, filter <code>--domain-filter=example.com</code> will match domains <code>example.com</code>, <code>ex.par.example.com</code>, <code>par.example.com</code>, <code>x.par.eu-west-1.example.com</code>.</p> <p>And if the filter is prepended with <code>.</code> e.g., <code>--domain-filter=.example.com</code> it will allow only zones that end in <code>.example.com</code>, i.e., the subdomains of example.com but not the <code>example.com</code> zone itself. Example result: <code>ex.par.eu-west-1.example.com</code>, <code>ex.par.example.com</code>, <code>par.example.com</code>.</p> <p>Note: if you prepend the filter with \u201c.\u201d, it will not attempt to match parent zones.</p>"},{"location":"docs/tutorials/aws-filters/#filter-by-zone-id","title":"Filter by Zone ID","text":"<p>Specify multiple times if needed, the flow logic is OR</p> <pre><code>args:\n    --zone-id-filter=ABCDEF12345678\n    --zone-id-filter=XYZDEF12345888\n    ...\n</code></pre>"},{"location":"docs/tutorials/aws-filters/#filter-by-tag","title":"Filter by Tag","text":"<p>Specify multiple times if needed, the flow logic is AND</p> <p>Keys only</p> <pre><code>args:\n    --aws-zone-tags=owner\n    --aws-zone-tags=vertical\n</code></pre> <p>Or specify keys with values</p> <pre><code>args:\n    --aws-zone-tags=owner=k8s\n    --aws-zone-tags=vertical=k8s\n</code></pre> <p>Can\u2019t specify multiple or separate values with commas: <code>key1=val1,key2=val2</code> at the moment. Filter only by value <code>--aws-zone-tags==tag-value</code> is not supported.</p> <pre><code>args:\n    --aws-zone-tags=team=k8s,vertical=platform # this is not supported\n    --aws-zone-tags==tag-value # this is not supported\n</code></pre>"},{"location":"docs/tutorials/aws-filters/#filtering-workflows","title":"Filtering Workflows","text":"<p>Filtering Sequence</p> <p>The diagram describes the sequence for filtering AWS zones.</p> <pre><code>flowchart TD\n    A[\"zones\"] --&gt; B{\"Is zonesCache valid?\"}\n    B -- Yes --&gt; C[\"Return cached zones\"]\n    B -- No --&gt; D[\"Initialize zones map\"]\n    D --&gt; E[\"For each profile and client\"]\n    E --&gt; F[\"Create paginator\"]\n    F --&gt; G{\"Has more pages?\"}\n    G -- Yes --&gt; H[\"Get next page\"]\n    H --&gt; I[\"For each zone in page\"]\n    I --&gt; J{\"Match zoneIDFilter?\"}\n    J -- No --&gt; G\n    J -- Yes --&gt; K{\"Match zoneTypeFilter?\"}\n    K -- No --&gt; G\n    K -- Yes --&gt; L{\"Match domainFilter?\"}\n    L -- No --&gt; M{\"zoneMatchParent?\"}\n    M -- No --&gt; G\n    M -- Yes --&gt; N{\"Match domainFilter parent?\"}\n    N -- No --&gt; G\n    N -- Yes --&gt; O{\"zoneTagFilter specified?\"}\n    O -- Yes --&gt; P[\"Add zone to zonesToValidate\"]\n    O -- No --&gt; Q[\"Add zone to zones map\"]\n    P --&gt; Q\n    Q --&gt; G\n    G -- No --&gt; R{\"zonesToValidate not empty?\"}\n    R -- Yes --&gt; S[\"Get tags for zones\"]\n    S --&gt; T[\"For each zone and tags\"]\n    T --&gt; U{\"Match zoneTagFilter?\"}\n    U -- No --&gt; V[\"Delete zone from zones map\"]\n    U -- Yes --&gt; W[\"Keep zone in zones map\"]\n    V --&gt; W\n    W --&gt; R\n    R -- No --&gt; X[\"Update zonesCache\"]\n    X --&gt; Y[\"Return zones\"]</code></pre> <p>Filtering Flow</p> <p>The is a sequence diagram that describes the interaction between <code>external-dns</code>, <code>AWSProvider</code>, and <code>Route53Client</code> during the filtering process. Here is a high-level description:</p> <pre><code>sequenceDiagram\n    participant external-dns\n    participant AWSProvider\n    participant Route53Client\n\n    external-dns-&gt;&gt;AWSProvider: zones\n    alt Cache is valid\n        AWSProvider--&gt;&gt;external-dns: return cached zones\n    else\n\n        AWSProvider-&gt;&gt;Route53Client: ListHostedZonesPaginator\n        loop While paginator.HasMorePages\n            Route53Client-&gt;&gt;AWSProvider: paginator.NextPage\n            alt ThrottlingException\n                AWSProvider-&gt;&gt;external-dns: error\n            else\n                AWSProvider--&gt;&gt;external-dns: return error\n            end\n            AWSProvider-&gt;&gt;AWSProvider: Filter zones\n            alt Tags need validation\n                AWSProvider-&gt;&gt;Route53Client: ListTagsForResources\n                Route53Client-&gt;&gt;AWSProvider: return tags\n                AWSProvider-&gt;&gt;AWSProvider: Validate tags\n            end\n        end\n        alt Cache duration &gt; 0\n            AWSProvider-&gt;&gt;AWSProvider: Update cache\n        end\n        AWSProvider--&gt;&gt;external-dns: return zones\n    end</code></pre>"},{"location":"docs/tutorials/aws-load-balancer-controller/","title":"AWS Load Balancer Controller","text":"<p>This tutorial describes how to use ExternalDNS with the aws-load-balancer-controller.</p>"},{"location":"docs/tutorials/aws-load-balancer-controller/#setting-up-externaldns-and-aws-load-balancer-controller","title":"Setting up ExternalDNS and aws-load-balancer-controller","text":"<p>Follow the AWS tutorial to setup ExternalDNS for use in Kubernetes clusters running in AWS. Specify the <code>source=ingress</code> argument so that ExternalDNS will look for hostnames in Ingress objects. In addition, you may wish to limit which Ingress objects are used as an ExternalDNS source via the <code>ingress-class</code> argument, but this is not required.</p> <p>For help setting up the AWS Load Balancer Controller, follow the Setup Guide.</p> <p>Note that the AWS Load Balancer Controller uses the same tags for subnet auto-discovery as Kubernetes does with the AWS cloud provider.</p> <p>In the examples that follow, it is assumed that you configured the ALB Ingress Controller with the <code>ingress-class=alb</code> argument (not to be confused with the same argument to ExternalDNS) so that the controller will only respect Ingress objects with the <code>ingressClassName</code> field set to \u201calb\u201d.</p>"},{"location":"docs/tutorials/aws-load-balancer-controller/#deploy-an-example-application","title":"Deploy an example application","text":"<p>Create the following sample \u201cechoserver\u201d application to demonstrate how ExternalDNS works with ALB ingress objects.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echoserver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: echoserver\n  template:\n    metadata:\n      labels:\n        app: echoserver\n    spec:\n      containers:\n      - image: gcr.io/google_containers/echoserver:1.4\n        imagePullPolicy: Always\n        name: echoserver\n        ports:\n        - containerPort: 8080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: echoserver\nspec:\n  ports:\n    - port: 80\n      targetPort: 8080\n      protocol: TCP\n  type: NodePort\n  selector:\n    app: echoserver\n</code></pre> <p>Note that the Service object is of type <code>NodePort</code>. We don\u2019t need a Service of type <code>LoadBalancer</code> here, since we will be using an Ingress to create an ALB.</p>"},{"location":"docs/tutorials/aws-load-balancer-controller/#ingress-examples","title":"Ingress examples","text":"<p>Create the following Ingress to expose the echoserver application to the Internet.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    alb.ingress.kubernetes.io/scheme: internet-facing\n  name: echoserver\nspec:\n  ingressClassName: alb\n  rules:\n  - host: echoserver.mycluster.example.org\n    http: &amp;echoserver_root\n      paths:\n      - path: /\n        backend:\n          service:\n            name: echoserver\n            port:\n              number: 80\n        pathType: Prefix\n  - host: echoserver.example.org\n    http: *echoserver_root\n</code></pre> <p>The above should result in the creation of an (ipv4) ALB in AWS which will forward traffic to the echoserver application.</p> <p>If the <code>--source=ingress</code> argument is specified, then ExternalDNS will create DNS records based on the hosts specified in ingress objects. The above example would result in two alias records (A and AAAA) being created for each of the domains: <code>echoserver.mycluster.example.org</code> and <code>echoserver.example.org</code>. All four records alias the ALB that is associated with the Ingress object. As the ALB is IPv4 only, the AAAA alias records have no effect.</p> <p>If you would like ExternalDNS to not create AAAA records at all, you can add the following command line parameter: <code>--exclude-record-types=AAAA</code>. Please be aware, this will disable AAAA record creation even for dualstack enabled load balancers.</p> <p>Note that the above example makes use of the YAML anchor feature to avoid having to repeat the http section for multiple hosts that use the exact same paths. If this Ingress object will only be fronting one backend Service, we might instead create the following:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    external-dns.alpha.kubernetes.io/hostname: echoserver.mycluster.example.org, echoserver.example.org\n  name: echoserver\nspec:\n  ingressClassName: alb\n  rules:\n  - http:\n      paths:\n      - path: /\n        backend:\n          service:\n            name: echoserver\n            port:\n              number: 80\n        pathType: Prefix\n</code></pre> <p>In the above example we create a default path that works for any hostname, and make use of the <code>external-dns.alpha.kubernetes.io/hostname</code> annotation to create multiple aliases for the resulting ALB.</p>"},{"location":"docs/tutorials/aws-load-balancer-controller/#dualstack-load-balancers","title":"Dualstack Load Balancers","text":"<p>AWS supports both IPv4 and \u201cdualstack\u201d (both IPv4 and IPv6) interfaces for ALBs and NLBs. The AWS Load Balancer Controller uses the <code>alb.ingress.kubernetes.io/ip-address-type</code> annotation (which defaults to <code>ipv4</code>) to determine this. ExternalDNS creates both A and AAAA alias DNS records by default, regardless of this annotation. It\u2019s possible to create only A records with the following command line parameter: <code>--exclude-record-types=AAAA</code></p> <p>Example:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    alb.ingress.kubernetes.io/scheme: internet-facing\n    alb.ingress.kubernetes.io/ip-address-type: dualstack\n  name: echoserver\nspec:\n  ingressClassName: alb\n  rules:\n  - host: echoserver.example.org\n    http:\n      paths:\n      - path: /\n        backend:\n          service:\n            name: echoserver\n            port:\n              number: 80\n        pathType: Prefix\n</code></pre> <p>The above Ingress object will result in the creation of an ALB with a dualstack interface.</p>"},{"location":"docs/tutorials/aws-public-private-route53/","title":"AWS Route53 with same domain for public and private zones","text":"<p>This tutorial describes how to setup ExternalDNS using the same domain for public and private Route53 zones and nginx-ingress-controller. It also outlines how to use cert-manager to automatically issue SSL certificates from Let\u2019s Encrypt for both public and private records.</p>"},{"location":"docs/tutorials/aws-public-private-route53/#deploy-public-nginx-ingress-controller","title":"Deploy public nginx-ingress-controller","text":"<p>You may be interested with GKE with nginx ingress for installation guidelines.</p> <p>Specify <code>ingress-class</code> in nginx-ingress-controller container args:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: external-ingress\n  name: external-ingress-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: external-ingress\n  template:\n    metadata:\n      labels:\n        app: external-ingress\n    spec:\n      containers:\n      - args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n        - --configmap=$(POD_NAMESPACE)/external-ingress-configuration\n        - --tcp-services-configmap=$(POD_NAMESPACE)/external-tcp-services\n        - --udp-services-configmap=$(POD_NAMESPACE)/external-udp-services\n        - --annotations-prefix=nginx.ingress.kubernetes.io\n        - --ingress-class=external-ingress\n        - --publish-service=$(POD_NAMESPACE)/external-ingress\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.11.0\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: external-ingress-controller\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n</code></pre> <p>Set <code>type: LoadBalancer</code> in your public nginx-ingress-controller Service definition.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: \"3600\"\n    service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: '*'\n  labels:\n    app: external-ingress\n  name: external-ingress\nspec:\n  externalTrafficPolicy: Cluster\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n  selector:\n    app: external-ingress\n  sessionAffinity: None\n  type: LoadBalancer\n</code></pre>"},{"location":"docs/tutorials/aws-public-private-route53/#deploy-private-nginx-ingress-controller","title":"Deploy private nginx-ingress-controller","text":"<p>Make sure to specify <code>ingress-class</code> in nginx-ingress-controller container args:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: internal-ingress\n  name: internal-ingress-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: internal-ingress\n  template:\n    metadata:\n      labels:\n        app: internal-ingress\n    spec:\n      containers:\n      - args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n        - --configmap=$(POD_NAMESPACE)/internal-ingress-configuration\n        - --tcp-services-configmap=$(POD_NAMESPACE)/internal-tcp-services\n        - --udp-services-configmap=$(POD_NAMESPACE)/internal-udp-services\n        - --annotations-prefix=nginx.ingress.kubernetes.io\n        - --ingress-class=internal-ingress\n        - --publish-service=$(POD_NAMESPACE)/internal-ingress\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.11.0\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        name: internal-ingress-controller\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n</code></pre> <p>Set additional annotations in your private nginx-ingress-controller Service definition to create an internal load balancer.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: \"3600\"\n    service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0\n    service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: '*'\n  labels:\n    app: internal-ingress\n  name: internal-ingress\nspec:\n  externalTrafficPolicy: Cluster\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n  selector:\n    app: internal-ingress\n  sessionAffinity: None\n  type: LoadBalancer\n</code></pre>"},{"location":"docs/tutorials/aws-public-private-route53/#deploy-the-public-zone-externaldns","title":"Deploy the public zone ExternalDNS","text":"<p>Consult AWS ExternalDNS setup docs for installation guidelines.</p> <p>In ExternalDNS containers args, make sure to specify <code>aws-zone-type</code> and <code>ingress-class</code>:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: external-dns-public\n  name: external-dns-public\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: external-dns-public\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: external-dns-public\n    spec:\n      containers:\n      - args:\n        - --source=ingress\n        - --provider=aws\n        - --registry=txt\n        - --txt-owner-id=external-dns\n        - --ingress-class=external-ingress\n        - --aws-zone-type=public\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        name: external-dns-public\n</code></pre>"},{"location":"docs/tutorials/aws-public-private-route53/#deploy-the-private-zone-externaldns","title":"Deploy the private zone ExternalDNS","text":"<p>Consult AWS ExternalDNS setup docs for installation guidelines.</p> <p>In ExternalDNS containers args, make sure to specify <code>aws-zone-type</code> and <code>ingress-class</code>:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: external-dns-private\n  name: external-dns-private\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: external-dns-private\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: external-dns-private\n    spec:\n      containers:\n      - args:\n        - --source=ingress\n        - --provider=aws\n        - --registry=txt\n        - --txt-owner-id=dev.k8s.nexus\n        - --ingress-class=internal-ingress\n        - --aws-zone-type=private\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        name: external-dns-private\n</code></pre>"},{"location":"docs/tutorials/aws-public-private-route53/#create-application-service-definitions","title":"Create application Service definitions","text":"<p>For this setup to work, you need to create two Ingress definitions for your application.</p> <p>At first, create a public Ingress definition:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  labels:\n    app: app\n  name: app-public\nspec:\n  ingressClassName: external-ingress\n  rules:\n  - host: app.domain.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: app\n            port:\n              number: 80\n        pathType: Prefix\n</code></pre> <p>Then create a private Ingress definition:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  labels:\n    app: app\n  name: app-private\nspec:\n  ingressClassName: internal-ingress\n  rules:\n  - host: app.domain.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: app\n            port:\n              number: 80\n        pathType: Prefix\n</code></pre> <p>Additionally, you may leverage cert-manager to automatically issue SSL certificates from Let\u2019s Encrypt. To do that, request a certificate in public service definition:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    certmanager.k8s.io/acme-challenge-type: \"dns01\"\n    certmanager.k8s.io/acme-dns01-provider: \"route53\"\n    certmanager.k8s.io/cluster-issuer: \"letsencrypt-production\"\n    kubernetes.io/tls-acme: \"true\"\n  labels:\n    app: app\n  name: app-public\nspec:\n  ingressClassName: \"external-ingress\"\n  rules:\n  - host: app.domain.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: app\n            port:\n              number: 80\n        pathType: Prefix\n  tls:\n  - hosts:\n    - app.domain.com\n    secretName: app-tls\n</code></pre> <p>And reuse the requested certificate in private Service definition:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  labels:\n    app: app\n  name: app-private\nspec:\n  ingressClassName: \"internal-ingress\"\n  rules:\n  - host: app.domain.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: app\n            port:\n              number: 80\n        pathType: Prefix\n  tls:\n  - hosts:\n    - app.domain.com\n    secretName: app-tls\n</code></pre>"},{"location":"docs/tutorials/aws-sd/","title":"AWS Cloud Map API","text":"<p>This tutorial describes how to set up ExternalDNS for usage within a Kubernetes cluster with AWS Cloud Map API.</p> <p>AWS Cloud Map API is an alternative approach to managing DNS records directly using the Route53 API. It is more suitable for a dynamic environment where service endpoints change frequently. It abstracts away technical details of the DNS protocol and offers a simplified model. AWS Cloud Map consists of three main API calls:</p> <ul> <li>CreatePublicDnsNamespace \u2013 automatically creates a DNS hosted zone</li> <li>CreateService \u2013 creates a new named service inside the specified namespace</li> <li>RegisterInstance/DeregisterInstance \u2013 can be called multiple times to create a DNS record for the specified Service</li> </ul> <p>Learn more about the API in the AWS Cloud Map API Reference.</p>"},{"location":"docs/tutorials/aws-sd/#iam-permissions","title":"IAM Permissions","text":"<p>To use the AWS Cloud Map API, a user must have permissions to create the DNS namespace. You need to make sure that your nodes (on which External DNS runs) have an IAM instance profile with the <code>AWSCloudMapFullAccess</code> managed policy attached, that provides following permissions:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:GetHostedZone\",\n        \"route53:ListHostedZonesByName\",\n        \"route53:CreateHostedZone\",\n        \"route53:DeleteHostedZone\",\n        \"route53:ChangeResourceRecordSets\",\n        \"route53:CreateHealthCheck\",\n        \"route53:GetHealthCheck\",\n        \"route53:DeleteHealthCheck\",\n        \"route53:UpdateHealthCheck\",\n        \"ec2:DescribeVpcs\",\n        \"ec2:DescribeRegions\",\n        \"servicediscovery:*\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/tutorials/aws-sd/#iam-permissions-with-abac","title":"IAM Permissions with ABAC","text":"<p>You can use Attribute-based access control(ABAC) for advanced deployments.</p> <p>You can define AWS tags that are applied to services created by the controller. By doing so, you can have precise control over your IAM policy to limit the scope of the permissions to services managed by the controller, rather than having to grant full permissions on your entire AWS account. To pass tags to service creation, use either CLI flags or environment variables:</p> <p>cli: <code>--aws-sd-create-tag=key1=value1 --aws-sd-create-tag=key2=value2</code></p> <p>environment: <code>EXTERNAL_DNS_AWS_SD_CREATE_TAG=key1=value1\\nkey2=value2</code></p> <p>Using tags, your <code>servicediscovery</code> policy can become:</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"servicediscovery:ListNamespaces\",\n        \"servicediscovery:ListServices\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"servicediscovery:CreateService\",\n        \"servicediscovery:TagResource\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ],\n      \"Condition\": {\n        \"StringEquals\": {\n          \"aws:RequestTag/YOUR_TAG_KEY\": \"YOUR_TAG_VALUE\"\n        }\n      }\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"servicediscovery:DiscoverInstances\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ],\n      \"Condition\": {\n        \"StringEquals\": {\n          \"servicediscovery:NamespaceName\": \"YOUR_NAMESPACE_NAME\"\n        }\n      }\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"servicediscovery:RegisterInstance\",\n        \"servicediscovery:DeregisterInstance\",\n        \"servicediscovery:DeleteService\",\n        \"servicediscovery:UpdateService\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ],\n      \"Condition\": {\n        \"StringEquals\": {\n          \"aws:ResourceTag/YOUR_TAG_KEY\": \"YOUR_TAG_VALUE\"\n        }\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"docs/tutorials/aws-sd/#set-up-a-namespace","title":"Set up a namespace","text":"<p>Create a DNS namespace using the AWS Cloud Map API:</p> <pre><code>aws servicediscovery create-public-dns-namespace --name \"external-dns-test.my-org.com\"\n</code></pre> <p>Verify that the namespace was truly created</p> <pre><code>aws servicediscovery list-namespaces\n</code></pre>"},{"location":"docs/tutorials/aws-sd/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Connect your <code>kubectl</code> client to the cluster that you want to test ExternalDNS with. Then apply the following manifest file to deploy ExternalDNS.</p>"},{"location":"docs/tutorials/aws-sd/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        env:\n          - name: AWS_REGION\n            value: us-east-1 # put your CloudMap NameSpace region\n        args:\n        - --source=service\n        - --source=ingress\n        - --domain-filter=external-dns-test.my-org.com # Makes ExternalDNS see only the namespaces that match the specified domain. Omit the filter if you want to process all available namespaces.\n        - --provider=aws-sd\n        - --aws-zone-type=public # Only look at public namespaces. Valid values are public, private, or no value for both)\n        - --txt-owner-id=my-identifier\n</code></pre>"},{"location":"docs/tutorials/aws-sd/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\",\"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        env:\n          - name: AWS_REGION\n            value: us-east-1 # put your CloudMap NameSpace region\n        args:\n        - --source=service\n        - --source=ingress\n        - --domain-filter=external-dns-test.my-org.com # Makes ExternalDNS see only the namespaces that match the specified domain. Omit the filter if you want to process all available namespaces.\n        - --provider=aws-sd\n        - --aws-zone-type=public # Only look at public namespaces. Valid values are public, private, or no value for both)\n        - --txt-owner-id=my-identifier\n</code></pre>"},{"location":"docs/tutorials/aws-sd/#verify-that-externaldns-works-service-example","title":"Verify that ExternalDNS works (Service example)","text":"<p>Create the following sample application to test that ExternalDNS works.</p> <p>For services ExternalDNS will look for the annotation <code>external-dns.alpha.kubernetes.io/hostname</code> on the service and use the corresponding value.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: nginx.external-dns-test.my-org.com\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    name: http\n    targetPort: 80\n  selector:\n    app: nginx\n\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n          name: http\n</code></pre> <p>After one minute check that a corresponding DNS record for your service was created in your hosted zone. We recommended that you use the Amazon Route53 console for that purpose.</p>"},{"location":"docs/tutorials/aws-sd/#custom-ttl","title":"Custom TTL","text":"<p>The default DNS record TTL (time to live) is 300 seconds. You can customize this value by setting the annotation <code>external-dns.alpha.kubernetes.io/ttl</code>. For example, modify the service manifest YAML file above:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: nginx.external-dns-test.my-org.com\n    external-dns.alpha.kubernetes.io/ttl: \"60\"\nspec:\n    ...\n</code></pre> <p>This will set the TTL for the DNS record to 60 seconds.</p>"},{"location":"docs/tutorials/aws-sd/#ipv6-support","title":"IPv6 Support","text":"<p>If your Kubernetes cluster is configured with IPv6 support, such as an EKS cluster with IPv6 support, ExternalDNS can also create AAAA DNS records.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: nginx.external-dns-test.my-org.com\n    external-dns.alpha.kubernetes.io/ttl: \"60\"\nspec:\n  ipFamilies:\n    - \"IPv6\"\n  type: NodePort\n  ports:\n    - port: 80\n      name: http\n      targetPort: 80\n  selector:\n    app: nginx\n</code></pre> <p> The AWS-SD provider does not currently support dualstack load balancers and will only create A records for these at this time. See the AWS provider and the AWS Load Balancer Controller Tutorial for dualstack load balancer support.</p>"},{"location":"docs/tutorials/aws-sd/#clean-up","title":"Clean up","text":"<p>Delete all service objects before terminating the cluster so all load balancers get cleaned up correctly.</p> <pre><code>kubectl delete service nginx\n</code></pre> <p>Give ExternalDNS some time to clean up the DNS records for you. Then delete the remaining service and namespace.</p> <pre><code>$ aws servicediscovery list-services\n\n{\n    \"Services\": [\n        {\n            \"Id\": \"srv-6dygt5ywvyzvi3an\",\n            \"Arn\": \"arn:aws:servicediscovery:us-west-2:861574988794:service/srv-6dygt5ywvyzvi3an\",\n            \"Name\": \"nginx\"\n        }\n    ]\n}\n</code></pre> <pre><code>aws servicediscovery delete-service --id srv-6dygt5ywvyzvi3an\n</code></pre> <pre><code>$ aws servicediscovery list-namespaces\n{\n    \"Namespaces\": [\n        {\n            \"Type\": \"DNS_PUBLIC\",\n            \"Id\": \"ns-durf2oxu4gxcgo6z\",\n            \"Arn\": \"arn:aws:servicediscovery:us-west-2:861574988794:namespace/ns-durf2oxu4gxcgo6z\",\n            \"Name\": \"external-dns-test.my-org.com\"\n        }\n    ]\n}\n</code></pre> <pre><code>aws servicediscovery delete-namespace --id ns-durf2oxu4gxcgo6z\n</code></pre>"},{"location":"docs/tutorials/aws/","title":"AWS","text":"<p>This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster on AWS. Make sure to use &gt;=0.15.0 version of ExternalDNS for this tutorial</p>"},{"location":"docs/tutorials/aws/#iam-policy","title":"IAM Policy","text":"<p>The following IAM Policy document allows ExternalDNS to update Route53 Resource Record Sets and Hosted Zones. You\u2019ll want to create this Policy in IAM first. In our example, we\u2019ll call the policy <code>AllowExternalDNSUpdates</code> (but you can call it whatever you prefer).</p> <p>If you prefer, you may fine-tune the policy to permit updates only to explicit Hosted Zone IDs.</p> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ChangeResourceRecordSets\"\n      ],\n      \"Resource\": [\n        \"arn:aws:route53:::hostedzone/*\"\n      ]\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"route53:ListHostedZones\",\n        \"route53:ListResourceRecordSets\",\n        \"route53:ListTagsForResource\"\n      ],\n      \"Resource\": [\n        \"*\"\n      ]\n    }\n  ]\n}\n</code></pre> <p>If you are using the AWS CLI, you can run the following to install the above policy (saved as <code>policy.json</code>).  This can be use in subsequent steps to allow ExternalDNS to access Route53 zones.</p> <pre><code>aws iam create-policy --policy-name \"AllowExternalDNSUpdates\" --policy-document file://policy.json\n\n# example: arn:aws:iam::XXXXXXXXXXXX:policy/AllowExternalDNSUpdates\nexport POLICY_ARN=$(aws iam list-policies \\\n --query 'Policies[?PolicyName==`AllowExternalDNSUpdates`].Arn' --output text)\n</code></pre>"},{"location":"docs/tutorials/aws/#provisioning-a-kubernetes-cluster","title":"Provisioning a Kubernetes cluster","text":"<p>You can use eksctl to easily provision an Amazon Elastic Kubernetes Service (EKS) cluster that is suitable for this tutorial.  See Getting started with Amazon EKS \u2013 eksctl.</p> <pre><code>export EKS_CLUSTER_NAME=\"my-externaldns-cluster\"\nexport EKS_CLUSTER_REGION=\"us-east-2\"\nexport KUBECONFIG=\"$HOME/.kube/${EKS_CLUSTER_NAME}-${EKS_CLUSTER_REGION}.yaml\"\n\neksctl create cluster --name $EKS_CLUSTER_NAME --region $EKS_CLUSTER_REGION\n</code></pre> <p>Feel free to use other provisioning tools or an existing cluster. If Terraform is used, vpc and eks modules are recommended for standing up an EKS cluster. Amazon has a workshop called Amazon EKS Terraform Workshop that may be useful for this process.</p>"},{"location":"docs/tutorials/aws/#permissions-to-modify-dns-zone","title":"Permissions to modify DNS zone","text":"<p>You will need to use the above policy (represented by the <code>POLICY_ARN</code> environment variable) to allow ExternalDNS to update records in Route53 DNS zones. Here are three common ways this can be accomplished:</p> <ul> <li>Node IAM Role</li> <li>Static credentials</li> <li>IAM Roles for Service Accounts</li> </ul> <p>For this tutorial, ExternalDNS will use the environment variable <code>EXTERNALDNS_NS</code> to represent the namespace, defaulted to <code>default</code>. Feel free to change this to something else, such <code>externaldns</code> or <code>kube-addons</code>. Make sure to edit the <code>subjects[0].namespace</code> for the <code>ClusterRoleBinding</code> resource when deploying ExternalDNS with RBAC enabled. See When using clusters with RBAC enabled for more information.</p> <p>Additionally, throughout this tutorial, the example domain of <code>example.com</code> is used.  Change this to appropriate domain under your control.  See Set up a hosted zone section.</p>"},{"location":"docs/tutorials/aws/#node-iam-role","title":"Node IAM Role","text":"<p>In this method, you can attach a policy to the Node IAM Role. This will allow nodes in the Kubernetes cluster to access Route53 zones, which allows ExternalDNS to update DNS records. Given that this allows all containers to access Route53, not just ExternalDNS, running on the node with these privileges, this method is not recommended, and is only suitable for limited test environments.</p> <p>If you are using eksctl to provision a new cluster, you add the policy at creation time with:</p> <pre><code>eksctl create cluster --external-dns-access \\\n  --name $EKS_CLUSTER_NAME --region $EKS_CLUSTER_REGION \\\n</code></pre> <p> WARNING: This will assign allow read-write access to all nodes in the cluster, not just ExternalDNS.  For this reason, this method is only suitable for limited test environments.</p> <p>If you already provisioned a cluster or use other provisioning tools like Terraform, you can use AWS CLI to attach the policy to the Node IAM Role.</p>"},{"location":"docs/tutorials/aws/#get-the-node-iam-role-name","title":"Get the Node IAM role name","text":"<p>The role name of the role associated with the node(s) where ExternalDNS will run is needed.  An easy way to get the role name is to use the AWS web console (https://console.aws.amazon.com/eks/), and find any instance in the target node group and copy the role name associated with that instance.</p>"},{"location":"docs/tutorials/aws/#get-role-name-with-a-single-managed-nodegroup","title":"Get role name with a single managed nodegroup","text":"<p>From the command line, if you have a single managed node group, the default with <code>eksctl create cluster</code>, you can find the role name with the following:</p> <pre><code># get managed node group name (assuming there's only one node group)\nGROUP_NAME=$(aws eks list-nodegroups --cluster-name $EKS_CLUSTER_NAME \\\n  --query nodegroups --out text)\n# fetch role arn given node group name\nROLE_ARN=$(aws eks describe-nodegroup --cluster-name $EKS_CLUSTER_NAME \\\n  --nodegroup-name $GROUP_NAME --query nodegroup.nodeRole --out text)\n# extract just the name part of role arn\nROLE_NAME=${NODE_ROLE_ARN##*/}\n</code></pre>"},{"location":"docs/tutorials/aws/#get-role-name-with-other-configurations","title":"Get role name with other configurations","text":"<p>If you have multiple node groups or any unmanaged node groups, the process gets more complex.  The first step is to get the instance host name of the desired node to where ExternalDNS will be deployed or is already deployed:</p> <pre><code># node instance name of one of the external dns pods currently running\nINSTANCE_NAME=$(kubectl get pods --all-namespaces \\\n  --selector app.kubernetes.io/instance=external-dns \\\n  --output jsonpath='{.items[0].spec.nodeName}')\n\n# instance name of one of the nodes (change if node group is different)\nINSTANCE_NAME=$(kubectl get nodes --output name | cut -d'/' -f2 | tail -1)\n</code></pre> <p>With the instance host name, you can then get the instance id:</p> <pre><code>get_instance_id() {\n  INSTANCE_NAME=$1 # example: ip-192-168-74-34.us-east-2.compute.internal\n\n  # get list of nodes\n  # ip-192-168-74-34.us-east-2.compute.internal aws:///us-east-2a/i-xxxxxxxxxxxxxxxxx\n  # ip-192-168-86-105.us-east-2.compute.internal aws:///us-east-2a/i-xxxxxxxxxxxxxxxxx\n  NODES=$(kubectl get nodes \\\n   --output jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.spec.providerID}{\"\\n\"}{end}')\n\n  # print instance id from matching node\n  grep $INSTANCE_NAME &lt;&lt;&lt; \"$NODES\" | cut -d'/' -f5\n}\n\nINSTANCE_ID=$(get_instance_id $INSTANCE_NAME)\n</code></pre> <p>With the instance id, you can get the associated role name:</p> <pre><code>findRoleName() {\n  INSTANCE_ID=$1\n\n  # get all of the roles\n  ROLES=($(aws iam list-roles --query Roles[*].RoleName --out text))\n  for ROLE in ${ROLES[*]}; do\n    # get instance profile arn\n    PROFILE_ARN=$(aws iam list-instance-profiles-for-role \\\n      --role-name $ROLE --query InstanceProfiles[0].Arn --output text)\n    # if there is an instance profile\n    if [[ \"$PROFILE_ARN\" != \"None\" ]]; then\n      # get all the instances with this associated instance profile\n      INSTANCES=$(aws ec2 describe-instances \\\n        --filters Name=iam-instance-profile.arn,Values=$PROFILE_ARN \\\n        --query Reservations[*].Instances[0].InstanceId --out text)\n      # find instances that match the instant profile\n      for INSTANCE in ${INSTANCES[*]}; do\n        # set role name value if there is a match\n        if [[ \"$INSTANCE_ID\" == \"$INSTANCE\" ]]; then ROLE_NAME=$ROLE; fi\n      done\n    fi\n  done\n\n  echo $ROLE_NAME\n}\n\nNODE_ROLE_NAME=$(findRoleName $INSTANCE_ID)\n</code></pre> <p>Using the role name, you can associate the policy that was created earlier:</p> <pre><code># attach policy arn created earlier to node IAM role\naws iam attach-role-policy --role-name $NODE_ROLE_NAME --policy-arn $POLICY_ARN\n</code></pre> <p> WARNING: This will assign allow read-write access to all pods running on the same node pool, not just the ExternalDNS pod(s).</p>"},{"location":"docs/tutorials/aws/#deploy-externaldns-with-attached-policy-to-node-iam-role","title":"Deploy ExternalDNS with attached policy to Node IAM Role","text":"<p>If ExternalDNS is not yet deployed, follow the steps under Deploy ExternalDNS using either RBAC or non-RBAC.</p> <p>NOTE: Before deleting the cluster during, be sure to run <code>aws iam detach-role-policy</code>.  Otherwise, there can be errors as the provisioning system, such as <code>eksctl</code> or <code>terraform</code>, will not be able to delete the roles with the attached policy.</p>"},{"location":"docs/tutorials/aws/#static-credentials","title":"Static credentials","text":"<p>In this method, the policy is attached to an IAM user, and the credentials secrets for the IAM user are then made available using a Kubernetes secret.</p> <p>This method is not the preferred method as the secrets in the credential file could be copied and used by an unauthorized threat actor. However, if the Kubernetes cluster is not hosted on AWS, it may be the only method available. Given this situation, it is important to limit the associated privileges to just minimal required privileges, i.e. read-write access to Route53, and not used a credentials file that has extra privileges beyond what is required.</p>"},{"location":"docs/tutorials/aws/#create-iam-user-and-attach-the-policy","title":"Create IAM user and attach the policy","text":"<pre><code># create IAM user\naws iam create-user --user-name \"externaldns\"\n\n# attach policy arn created earlier to IAM user\naws iam attach-user-policy --user-name \"externaldns\" --policy-arn $POLICY_ARN\n</code></pre>"},{"location":"docs/tutorials/aws/#create-the-static-credentials","title":"Create the static credentials","text":"<pre><code>SECRET_ACCESS_KEY=$(aws iam create-access-key --user-name \"externaldns\")\nACCESS_KEY_ID=$(echo $SECRET_ACCESS_KEY | jq -r '.AccessKey.AccessKeyId')\n\ncat &lt;&lt;-EOF &gt; credentials\n\n[default]\naws_access_key_id = $(echo $ACCESS_KEY_ID)\naws_secret_access_key = $(echo $SECRET_ACCESS_KEY | jq -r '.AccessKey.SecretAccessKey')\nEOF\n</code></pre>"},{"location":"docs/tutorials/aws/#create-kubernetes-secret-from-credentials","title":"Create Kubernetes secret from credentials","text":"<pre><code>kubectl create secret generic external-dns \\\n  --namespace ${EXTERNALDNS_NS:-\"default\"} --from-file /local/path/to/credentials\n</code></pre>"},{"location":"docs/tutorials/aws/#deploy-externaldns-using-static-credentials","title":"Deploy ExternalDNS using static credentials","text":"<p>Follow the steps under Deploy ExternalDNS using either RBAC or non-RBAC.  Make sure to uncomment the section that mounts volumes, so that the credentials can be mounted.</p> <p>[!TIP] By default ExternalDNS takes the profile named <code>default</code> from the credentials file. If you want to use a different profile, you can set the environment variable <code>EXTERNAL_DNS_AWS_PROFILE</code> to the desired profile name or use the <code>--aws-profile</code> command line argument. It is even possible to use more than one profile at ones, separated by space in the environment variable <code>EXTERNAL_DNS_AWS_PROFILE</code> or by using <code>--aws-profile</code> multiple times. In this case ExternalDNS looks for the hosted zones in all profiles and keeps maintaining a mapping table between zone and profile in order to be able to modify the zones in the correct profile.</p>"},{"location":"docs/tutorials/aws/#iam-roles-for-service-accounts","title":"IAM Roles for Service Accounts","text":"<p>IRSA (IAM roles for Service Accounts) allows cluster operators to map AWS IAM Roles to Kubernetes Service Accounts. This essentially allows only ExternalDNS pods to access Route53 without exposing any static credentials.</p> <p>This is the preferred method as it implements PoLP (Principle of Least Privilege).</p> <p>[!IMPORTANT] This method requires using KSA (Kubernetes service account) and RBAC.</p> <p>This method requires deploying with RBAC.  See When using clusters with RBAC enabled when ready to deploy ExternalDNS.</p> <p>[!NOTE] Similar methods to IRSA on AWS are kiam, which is in maintenence mode, and has instructions for creating an IAM role, and also kube2iam. IRSA is the officially supported method for EKS clusters, and so for non-EKS clusters on AWS, these other tools could be an option.</p>"},{"location":"docs/tutorials/aws/#verify-oidc-is-supported","title":"Verify OIDC is supported","text":"<pre><code>aws eks describe-cluster --name $EKS_CLUSTER_NAME \\\n  --query \"cluster.identity.oidc.issuer\" --output text\n</code></pre>"},{"location":"docs/tutorials/aws/#associate-oidc-to-cluster","title":"Associate OIDC to cluster","text":"<p>Configure the cluster with an OIDC provider and add support for IRSA (IAM roles for Service Accounts).</p> <p>If you used <code>eksctl</code> to provision the EKS cluster, you can update it with the following command:</p> <pre><code>eksctl utils associate-iam-oidc-provider \\\n  --cluster $EKS_CLUSTER_NAME --approve\n</code></pre> <p>If the cluster was provisioned with Terraform, you can use the <code>iam_openid_connect_provider</code> resource (ref) to associate to the OIDC provider.</p>"},{"location":"docs/tutorials/aws/#create-an-iam-role-bound-to-a-service-account","title":"Create an IAM role bound to a service account","text":"<p>For the next steps in this process, we will need to associate the <code>external-dns</code> service account and a role used to grant access to Route53.  This requires the following steps:</p> <ol> <li>Create a role with a trust relationship to the cluster\u2019s OIDC provider</li> <li>Attach the <code>AllowExternalDNSUpdates</code> policy to the role</li> <li>Create the <code>external-dns</code> service account</li> <li>Add annotation to the service account with the role arn</li> </ol>"},{"location":"docs/tutorials/aws/#use-eksctl-with-eksctl-created-eks-cluster","title":"Use eksctl with eksctl created EKS cluster","text":"<p>If <code>eksctl</code> was used to provision the EKS cluster, you can perform all of these steps with the following command:</p> <pre><code>eksctl create iamserviceaccount \\\n  --cluster $EKS_CLUSTER_NAME \\\n  --name \"external-dns\" \\\n  --namespace ${EXTERNALDNS_NS:-\"default\"} \\\n  --attach-policy-arn $POLICY_ARN \\\n  --approve\n</code></pre>"},{"location":"docs/tutorials/aws/#use-aws-cli-with-any-eks-cluster","title":"Use aws cli with any EKS cluster","text":"<p>Otherwise, we can do the following steps using <code>aws</code> commands (also see Creating an IAM role and policy for your service account):</p> <pre><code>ACCOUNT_ID=$(aws sts get-caller-identity \\\n  --query \"Account\" --output text)\nOIDC_PROVIDER=$(aws eks describe-cluster --name $EKS_CLUSTER_NAME \\\n  --query \"cluster.identity.oidc.issuer\" --output text | sed -e 's|^https://||')\n\ncat &lt;&lt;-EOF &gt; trust.json\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Federated\": \"arn:aws:iam::$ACCOUNT_ID:oidc-provider/$OIDC_PROVIDER\"\n            },\n            \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n            \"Condition\": {\n                \"StringEquals\": {\n                    \"$OIDC_PROVIDER:sub\": \"system:serviceaccount:${EXTERNALDNS_NS:-\"default\"}:external-dns\",\n                    \"$OIDC_PROVIDER:aud\": \"sts.amazonaws.com\"\n                }\n            }\n        }\n    ]\n}\nEOF\n\nIRSA_ROLE=\"external-dns-irsa-role\"\naws iam create-role --role-name $IRSA_ROLE --assume-role-policy-document file://trust.json\naws iam attach-role-policy --role-name $IRSA_ROLE --policy-arn $POLICY_ARN\n\nROLE_ARN=$(aws iam get-role --role-name $IRSA_ROLE --query Role.Arn --output text)\n\n# Create service account (skip is already created)\nkubectl create serviceaccount \"external-dns\" --namespace ${EXTERNALDNS_NS:-\"default\"}\n\n# Add annotation referencing IRSA role\nkubectl patch serviceaccount \"external-dns\" --namespace ${EXTERNALDNS_NS:-\"default\"} --patch \\\n \"{\\\"metadata\\\": { \\\"annotations\\\": { \\\"eks.amazonaws.com/role-arn\\\": \\\"$ROLE_ARN\\\" }}}\"\n</code></pre> <p>If any part of this step is misconfigured, such as the role with incorrect namespace configured in the trust relationship, annotation pointing the the wrong role, etc., you will see errors like <code>WebIdentityErr: failed to retrieve credentials</code>. Check the configuration and make corrections.</p> <p>When the service account annotations are updated, then the current running pods will have to be terminated, so that new pod(s) with proper configuration (environment variables) will be created automatically.</p> <p>When annotation is added to service account, the ExternalDNS pod(s) scheduled will have <code>AWS_ROLE_ARN</code>, <code>AWS_STS_REGIONAL_ENDPOINTS</code>, and <code>AWS_WEB_IDENTITY_TOKEN_FILE</code> environment variables injected automatically.</p>"},{"location":"docs/tutorials/aws/#deploy-externaldns-using-irsa","title":"Deploy ExternalDNS using IRSA","text":"<p>Follow the steps under When using clusters with RBAC enabled.  Make sure to comment out the service account section if this has been created already.</p> <p>If you deployed ExternalDNS before adding the service account annotation and the corresponding role, you will likely see error with <code>failed to list hosted zones: AccessDenied: User</code>. You can delete the current running ExternalDNS pod(s) after updating the annotation, so that new pods scheduled will have appropriate configuration to access Route53.</p>"},{"location":"docs/tutorials/aws/#set-up-a-hosted-zone","title":"Set up a hosted zone","text":"<p>If you prefer to try-out ExternalDNS in one of the existing hosted-zones you can skip this step</p> <p>Create a DNS zone which will contain the managed DNS records.  This tutorial will use the fictional domain of <code>example.com</code>.</p> <pre><code>aws route53 create-hosted-zone --name \"example.com.\" \\\n  --caller-reference \"external-dns-test-$(date +%s)\"\n</code></pre> <p>Make a note of the nameservers that were assigned to your new zone.</p> <pre><code>ZONE_ID=$(aws route53 list-hosted-zones-by-name --output json \\\n  --dns-name \"example.com.\" --query HostedZones[0].Id --out text)\n\naws route53 list-resource-record-sets --output text \\\n --hosted-zone-id $ZONE_ID --query \\\n \"ResourceRecordSets[?Type == 'NS'].ResourceRecords[*].Value | []\" | tr '\\t' '\\n'\n</code></pre> <p>This should yield something similar this:</p> <pre><code>ns-695.awsdns-22.net.\nns-1313.awsdns-36.org.\nns-350.awsdns-43.com.\nns-1805.awsdns-33.co.uk.\n</code></pre> <p>If using your own domain that was registered with a third-party domain registrar, you should point your domain\u2019s name servers to the values in the from the list above.  Please consult your registrar\u2019s documentation on how to do that.</p>"},{"location":"docs/tutorials/aws/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Connect your <code>kubectl</code> client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS. You can check if your cluster has RBAC by <code>kubectl api-versions | grep rbac.authorization.k8s.io</code>.</p> <p>For clusters with RBAC enabled, be sure to choose the correct <code>namespace</code>.  For this tutorial, the enviornment variable <code>EXTERNALDNS_NS</code> will refer to the namespace.  You can set this to a value of your choice:</p> <pre><code>export EXTERNALDNS_NS=\"default\" # externaldns, kube-addons, etc\n\n# create namespace if it does not yet exist\nkubectl get namespaces | grep -q $EXTERNALDNS_NS || \\\n  kubectl create namespace $EXTERNALDNS_NS\n</code></pre>"},{"location":"docs/tutorials/aws/#using-helm-with-oidc","title":"Using Helm (with OIDC)","text":"<p>Create a values.yaml file to configure ExternalDNS:</p> <pre><code>provider:\n  name: aws\nenv:\n  - name: AWS_DEFAULT_REGION\n    value: us-east-1 # change to region where EKS is installed\n</code></pre> <p>Finally, install the ExternalDNS chart with Helm using the configuration specified in your values.yaml file:</p> <pre><code>helm upgrade --install external-dns external-dns/external-dns --values values.yaml\n</code></pre>"},{"location":"docs/tutorials/aws/#when-using-clusters-without-rbac-enabled","title":"When using clusters without RBAC enabled","text":"<p>Save the following below as <code>externaldns-no-rbac.yaml</code>.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\n  labels:\n    app.kubernetes.io/name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-dns\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: external-dns\n    spec:\n      containers:\n        - name: external-dns\n          image: registry.k8s.io/external-dns/external-dns:v0.15.1\n          args:\n            - --source=service\n            - --source=ingress\n            - --domain-filter=example.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n            - --provider=aws\n            - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization\n            - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both)\n            - --registry=txt\n            - --txt-owner-id=my-hostedzone-identifier\n          env:\n            - name: AWS_DEFAULT_REGION\n              value: us-east-1 # change to region where EKS is installed\n      # # Uncomment below if using static credentials\n      #       - name: AWS_SHARED_CREDENTIALS_FILE\n      #        value: /.aws/credentials\n      #     volumeMounts:\n      #       - name: aws-credentials\n      #         mountPath: /.aws\n      #         readOnly: true\n      # volumes:\n      #   - name: aws-credentials\n      #     secret:\n      #       secretName: external-dns\n</code></pre> <p>When ready you can deploy:</p> <pre><code>kubectl create --filename externaldns-no-rbac.yaml \\\n  --namespace ${EXTERNALDNS_NS:-\"default\"}\n</code></pre>"},{"location":"docs/tutorials/aws/#when-using-clusters-with-rbac-enabled","title":"When using clusters with RBAC enabled","text":"<p>If you\u2019re using EKS, you can update the <code>values.yaml</code> file you created earlier to include the annotations to link the Role ARN you created before.</p> <pre><code>provider:\n  name: aws\nserviceAccount:\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::${ACCOUNT_ID}:role/${EXTERNALDNS_ROLE_NAME:-\"external-dns\"}\n</code></pre> <p>If you need to provide credentials directly using a secret (ie. You\u2019re not using EKS), you can change the <code>values.yaml</code> file to include volume and volume mounts.</p> <pre><code>provider:\n  name: aws\nenv:\n  - name: AWS_SHARED_CREDENTIALS_FILE\n    value: /etc/aws/credentials/my_credentials\nextraVolumes:\n  - name: aws-credentials\n    secret:\n      secretName: external-dns # In this example, the secret will have the data stored in a key named `my_credentials`\nextraVolumeMounts:\n  - name: aws-credentials\n    mountPath: /etc/aws/credentials\n    readOnly: true\n</code></pre> <p>When ready, update your Helm installation:</p> <pre><code>helm upgrade --install external-dns external-dns/external-dns --values values.yaml\n</code></pre>"},{"location":"docs/tutorials/aws/#arguments","title":"Arguments","text":"<p>This list is not the full list, but a few arguments that where chosen.</p>"},{"location":"docs/tutorials/aws/#aws-zone-type","title":"aws-zone-type","text":"<p><code>aws-zone-type</code> allows filtering for private and public zones</p>"},{"location":"docs/tutorials/aws/#annotations","title":"Annotations","text":"<p>Annotations which are specific to AWS.</p>"},{"location":"docs/tutorials/aws/#alias","title":"alias","text":"<p><code>external-dns.alpha.kubernetes.io/alias</code> if set to <code>true</code> on an ingress, it will create two ALIAS records (one \u2018A\u2019 for IPv4 and one \u2018AAAA\u2019 for IPv6) when the target is an ALIAS as well. To make the target an alias, the ingress needs to be configured correctly as described in the docs. In particular, the argument <code>--publish-service=default/nginx-ingress-controller</code> has to be set on the <code>nginx-ingress-controller</code> container. If one uses the <code>nginx-ingress</code> Helm chart, this flag can be set with the <code>controller.publishService.enabled</code> configuration option.</p>"},{"location":"docs/tutorials/aws/#target-hosted-zone","title":"target-hosted-zone","text":"<p><code>external-dns.alpha.kubernetes.io/aws-target-hosted-zone</code> can optionally be set to the ID of a Route53 hosted zone. This will force external-dns to use the specified hosted zone when creating an ALIAS target.</p>"},{"location":"docs/tutorials/aws/#aws-zone-match-parent","title":"aws-zone-match-parent","text":"<p><code>aws-zone-match-parent</code> allows support subdomains within the same zone by using their parent domain, i.e \u2013domain-filter=x.example.com would create a DNS entry for x.example.com (and subdomains thereof).</p> <pre><code>## hosted zone domain: example.com\n--domain-filter=x.example.com,example.com\n--aws-zone-match-parent\n</code></pre>"},{"location":"docs/tutorials/aws/#verify-externaldns-works-service-example","title":"Verify ExternalDNS works (Service example)","text":"<p>Create the following sample application to test that ExternalDNS works.</p> <p>For services ExternalDNS will look for the annotation <code>external-dns.alpha.kubernetes.io/hostname</code> on the service and use the corresponding value. If you want to give multiple names to service, you can set it to external-dns.alpha.kubernetes.io/hostname with a comma <code>,</code> separator.</p> <p>For this verification phase, you can use default or another namespace for the nginx demo, for example:</p> <pre><code>NGINXDEMO_NS=\"nginx\"\nkubectl get namespaces | grep -q $NGINXDEMO_NS || kubectl create namespace $NGINXDEMO_NS\n</code></pre> <p>Save the following manifest below as <code>nginx.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: nginx.example.com\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    name: http\n    targetPort: 80\n  selector:\n    app: nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n          name: http\n</code></pre> <p>Deploy the nginx deployment and service with:</p> <pre><code>kubectl create --filename nginx.yaml --namespace ${NGINXDEMO_NS:-\"default\"}\n</code></pre> <p>Verify that the load balancer was allocated with:</p> <pre><code>kubectl get service nginx --namespace ${NGINXDEMO_NS:-\"default\"}\n</code></pre> <p>This should show something like:</p> <pre><code>NAME    TYPE           CLUSTER-IP     EXTERNAL-IP                                                                   PORT(S)        AGE\nnginx   LoadBalancer   10.100.47.41   ae11c2360188411e7951602725593fd1-1224345803.eu-central-1.elb.amazonaws.com.   80:32749/TCP   12m\n</code></pre> <p>After roughly two minutes check that a corresponding DNS record for your service that was created.</p> <pre><code>aws route53 list-resource-record-sets --output json --hosted-zone-id $ZONE_ID \\\n  --query \"ResourceRecordSets[?Name == 'nginx.example.com.']|[?Type == 'A']\"\n</code></pre> <p>This should show something like:</p> <pre><code>[\n    {\n        \"Name\": \"nginx.example.com.\",\n        \"Type\": \"A\",\n        \"AliasTarget\": {\n            \"HostedZoneId\": \"ZEWFWZ4R16P7IB\",\n            \"DNSName\": \"ae11c2360188411e7951602725593fd1-1224345803.eu-central-1.elb.amazonaws.com.\",\n            \"EvaluateTargetHealth\": true\n        }\n    }\n]\n</code></pre> <p>Or for IPv6 (AAAA) records:</p> <pre><code>aws route53 list-resource-record-sets --output json --hosted-zone-id $ZONE_ID \\\n  --query \"ResourceRecordSets[?Name == 'nginx.example.com.']|[?Type == 'AAAA']\"\n</code></pre> <p>This should show something like:</p> <pre><code>[\n    {\n        \"Name\": \"nginx.example.com.\",\n        \"Type\": \"AAAA\",\n        \"AliasTarget\": {\n            \"HostedZoneId\": \"ZEWFWZ4R16P7IB\",\n            \"DNSName\": \"ae11c2360188411e7951602725593fd1-1224345803.eu-central-1.elb.amazonaws.com.\",\n            \"EvaluateTargetHealth\": true\n        }\n    }\n]\n</code></pre> <p>IPv6 (AAAA) records are created when ALIAS is enabled even for load balancers that do not have dualstack enabled. However, Route53 returns empty sets when querying such records, meaning they are harmless and IPv4 will work as normal.</p> <p>You can also fetch the corresponding text records:</p> <pre><code>aws route53 list-resource-record-sets --output json --hosted-zone-id $ZONE_ID \\\n  --query \"ResourceRecordSets[?Name == 'nginx.example.com.']|[?Type == 'TXT']\"\n</code></pre> <p>This will show something like:</p> <pre><code>[\n    {\n        \"Name\": \"nginx.example.com.\",\n        \"Type\": \"TXT\",\n        \"TTL\": 300,\n        \"ResourceRecords\": [\n            {\n                \"Value\": \"\\\"heritage=external-dns,external-dns/owner=external-dns,external-dns/resource=service/default/nginx\\\"\"\n            }\n        ]\n    }\n]\n</code></pre> <p>Note created TXT record alongside ALIAS records. TXT record signifies that the corresponding ALIAS records are managed by ExternalDNS. This makes ExternalDNS safe for running in environments where there are other records managed via other means.</p> <p>For more information about ALIAS records, see Choosing between alias and non-alias records.</p> <p>Let\u2019s check that we can resolve this DNS name. We\u2019ll ask the nameservers assigned to your zone first.</p> <pre><code>dig +short @ns-5514.awsdns-53.org. nginx.example.com.\n</code></pre> <p>This should return 1+ IP addresses that correspond to the ELB FQDN, i.e. <code>ae11c2360188411e7951602725593fd1-1224345803.eu-central-1.elb.amazonaws.com.</code>.</p> <p>Next try the public nameservers configured by DNS client on your system:</p> <pre><code>dig +short nginx.example.com.\n</code></pre> <p>If you hooked up your DNS zone with its parent zone correctly you can use <code>curl</code> to access your site.</p> <pre><code>curl nginx.example.com.\n</code></pre> <p>This should show something like:</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n...\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n...\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"docs/tutorials/aws/#verify-externaldns-works-ingress-example","title":"Verify ExternalDNS works (Ingress example)","text":"<p>With the previous <code>deployment</code> and <code>service</code> objects deployed, we can add an <code>ingress</code> object and configure a FQDN value for the <code>host</code> key.  The ingress controller will match incoming HTTP traffic, and route it to the appropriate backend service based on the <code>host</code> key.</p> <p>For ingress objects ExternalDNS will create a DNS record based on the host specified for the ingress object.</p> <p>For this tutorial, we have two endpoints, the service with <code>LoadBalancer</code> type and an ingress.  For practical purposes, if an ingress is used, the service type can be changed to <code>ClusterIP</code> as two endpoints are unecessary in this scenario.</p> <p>[!IMPORTANT] This requires that an ingress controller has been installed in your Kubernetes cluster. EKS does not come with an ingress controller by default. A popular ingress controller is ingress-nginx, which can be installed by a helm chart or by manifests.</p> <p>Create an ingress resource manifest file named <code>ingress.yaml</code> with the contents below:</p> <pre><code>---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nginx\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: server.example.com\n      http:\n        paths:\n          - backend:\n              service:\n                name: nginx\n                port:\n                  number: 80\n            path: /\n            pathType: Prefix\n</code></pre> <p>When ready, you can deploy this with:</p> <pre><code>kubectl create --filename ingress.yaml --namespace ${NGINXDEMO_NS:-\"default\"}\n</code></pre> <p>Watch the status of the ingress until the ADDRESS field is populated.</p> <pre><code>kubectl get ingress --watch --namespace ${NGINXDEMO_NS:-\"default\"}\n</code></pre> <p>You should see something like this:</p> <pre><code>NAME    CLASS    HOSTS                ADDRESS   PORTS   AGE\nnginx   &lt;none&gt;   server.example.com             80      47s\nnginx   &lt;none&gt;   server.example.com   ae11c2360188411e7951602725593fd1-1224345803.eu-central-1.elb.amazonaws.com.   80      54s\n</code></pre> <p>For the ingress test, run through similar checks, but using domain name used for the ingress:</p> <pre><code># check records on route53\naws route53 list-resource-record-sets --output json --hosted-zone-id $ZONE_ID \\\n  --query \"ResourceRecordSets[?Name == 'server.example.com.']\"\n\n# query using a route53 name server\ndig +short @ns-5514.awsdns-53.org. server.example.com.\n# query using the default name server\ndig +short server.example.com.\n\n# connect to the nginx web server through the ingress\ncurl server.example.com.\n</code></pre>"},{"location":"docs/tutorials/aws/#more-service-annotation-options","title":"More service annotation options","text":""},{"location":"docs/tutorials/aws/#custom-ttl","title":"Custom TTL","text":"<p>The default DNS record TTL (Time-To-Live) is 300 seconds. You can customize this value by setting the annotation <code>external-dns.alpha.kubernetes.io/ttl</code>. e.g., modify the service manifest YAML file above:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: nginx.example.com\n    external-dns.alpha.kubernetes.io/ttl: \"60\"\nspec:\n    ...\n</code></pre> <p>This will set the DNS record\u2019s TTL to 60 seconds.</p>"},{"location":"docs/tutorials/aws/#routing-policies","title":"Routing policies","text":"<p>Route53 offers different routing policies. The routing policy for a record can be controlled with the following annotations:</p> <ul> <li><code>external-dns.alpha.kubernetes.io/set-identifier</code>: this needs to be set to use any of the following routing policies</li> </ul> <p>For any given DNS name, only one of the following routing policies can be used:</p> <ul> <li>Weighted records: <code>external-dns.alpha.kubernetes.io/aws-weight</code></li> <li>Latency-based routing: <code>external-dns.alpha.kubernetes.io/aws-region</code></li> <li>Failover:<code>external-dns.alpha.kubernetes.io/aws-failover</code></li> <li>Geolocation-based routing:</li> <li><code>external-dns.alpha.kubernetes.io/aws-geolocation-continent-code</code></li> <li><code>external-dns.alpha.kubernetes.io/aws-geolocation-country-code</code></li> <li><code>external-dns.alpha.kubernetes.io/aws-geolocation-subdivision-code</code></li> <li>Multi-value answer:<code>external-dns.alpha.kubernetes.io/aws-multi-value-answer</code></li> </ul>"},{"location":"docs/tutorials/aws/#associating-dns-records-with-healthchecks","title":"Associating DNS records with healthchecks","text":"<p>You can configure Route53 to associate DNS records with healthchecks for automated DNS failover using <code>external-dns.alpha.kubernetes.io/aws-health-check-id: &lt;health-check-id&gt;</code> annotation.</p> <p>Note: ExternalDNS does not support creating healthchecks, and assumes that <code>&lt;health-check-id&gt;</code> already exists.</p>"},{"location":"docs/tutorials/aws/#canonical-hosted-zones","title":"Canonical Hosted Zones","text":"<p>When creating ALIAS type records in Route53 it is required that external-dns be aware of the canonical hosted zone in which the specified hostname is created. External-dns is able to automatically identify the canonical hosted zone for many hostnames based upon known hostname suffixes which are defined in aws.go. If a hostname does not have a known suffix then the suffix can be added into <code>aws.go</code> or the target-hosted-zone annotation can be used to manually define the ID of the canonical hosted zone.</p>"},{"location":"docs/tutorials/aws/#govcloud-caveats","title":"Govcloud caveats","text":"<p>Due to the special nature with how Route53 runs in Govcloud, there are a few tweaks in the deployment settings.</p> <ul> <li>An Environment variable with name of <code>AWS_REGION</code> set to either <code>us-gov-west-1</code> or <code>us-gov-east-1</code> is required. Otherwise it tries to lookup a region that does not exist in Govcloud and it errors out.</li> </ul> <pre><code>env:\n- name: AWS_REGION\n  value: us-gov-west-1\n</code></pre> <ul> <li>Route53 in Govcloud does not allow aliases. Therefore, container args must be set so that it uses CNAMES and a txt-prefix must be set to something. Otherwise, it will try to create a TXT record with the same value than the CNAME itself, which is not allowed.</li> </ul> <pre><code>args:\n- --aws-prefer-cname\n- --txt-prefix={{ YOUR_PREFIX }}\n</code></pre> <ul> <li>The first two changes are needed if you use Route53 in Govcloud, which only supports private zones. There are also no cross account IAM whatsoever between Govcloud and commercial AWS accounts.</li> <li>If services and ingresses need to make Route 53 entries to an public zone in a commercial account, you will have set env variables of <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> with a key and secret to the commercial account that has the sufficient rights.</li> </ul> <pre><code>env:\n- name: AWS_ACCESS_KEY_ID\n  value: XXXXXXXXX\n- name: AWS_SECRET_ACCESS_KEY\n  valueFrom:\n    secretKeyRef:\n      name: {{ YOUR_SECRET_NAME }}\n      key: {{ YOUR_SECRET_KEY }}\n</code></pre>"},{"location":"docs/tutorials/aws/#dynamodb-registry","title":"DynamoDB Registry","text":"<p>The DynamoDB Registry can be used to store dns records metadata. See the DynamoDB Registry Tutorial for more information.</p>"},{"location":"docs/tutorials/aws/#disable-aaaa-record-creation","title":"Disable AAAA Record Creation","text":"<p>If you would like ExternalDNS to not create AAAA records at all, you can add the following command line parameter: <code>--exclude-record-types=AAAA</code>. Please be aware, this will disable AAAA record creation even for dualstack enabled load balancers.</p>"},{"location":"docs/tutorials/aws/#clean-up","title":"Clean up","text":"<p>Make sure to delete all Service objects before terminating the cluster so all load balancers get cleaned up correctly.</p> <pre><code>kubectl delete service nginx\n</code></pre> <p>IMPORTANT If you attached a policy to the Node IAM Role, then you will want to detach this before deleting the EKS cluster.  Otherwise, the role resource will be locked, and the cluster cannot be deleted, especially if it was provisioned by automation like <code>terraform</code> or <code>eksctl</code>.</p> <pre><code>aws iam detach-role-policy --role-name $NODE_ROLE_NAME --policy-arn $POLICY_ARN\n</code></pre> <p>If the cluster was provisioned using <code>eksctl</code>, you can delete the cluster with:</p> <pre><code>eksctl delete cluster --name $EKS_CLUSTER_NAME --region $EKS_CLUSTER_REGION\n</code></pre> <p>Give ExternalDNS some time to clean up the DNS records for you. Then delete the hosted zone if you created one for the testing purpose.</p> <pre><code>aws route53 delete-hosted-zone --id $ZONE_ID # e.g /hostedzone/ZEWFWZ4R16P7IB\n</code></pre> <p>If IAM user credentials were used, you can remove the user with:</p> <pre><code>aws iam detach-user-policy --user-name \"externaldns\" --policy-arn $POLICY_ARN\n\n# If static credentials were used\naws iam delete-access-key --user-name \"externaldns\" --access-key-id $ACCESS_KEY_ID\n\naws iam delete-user --user-name \"externaldns\"\n</code></pre> <p>If IRSA was used, you can remove the IRSA role with:</p> <pre><code>aws iam detach-role-policy --role-name $IRSA_ROLE --policy-arn $POLICY_ARN\naws iam delete-role --role-name $IRSA_ROLE\n</code></pre> <p>Delete any unneeded policies:</p> <pre><code>aws iam delete-policy --policy-arn $POLICY_ARN\n</code></pre>"},{"location":"docs/tutorials/aws/#throttling","title":"Throttling","text":"<p>Route53 has a 5 API requests per second per account hard quota. Running several fast polling ExternalDNS instances in a given account can easily hit that limit. Some ways to reduce the request rate include:</p> <ul> <li>Reduce the polling loop\u2019s synchronization interval at the possible cost of slower change propagation (but see <code>--events</code> below to reduce the impact).</li> <li><code>--interval=5m</code> (default <code>1m</code>)</li> <li>Enable a Cache to store the zone records list. It comes with a cost: slower propagation when the zone gets modified from other sources such as the AWS console, terraform, cloudformation or anything similar.</li> <li><code>--provider-cache-time=15m</code> (default <code>0m</code>)</li> <li>Trigger the polling loop on changes to K8s objects, rather than only at <code>interval</code> and ensure a minimum of time between events, to have responsive updates with long poll intervals</li> <li><code>--events</code></li> <li><code>--min-event-sync-interval=5m</code> (default <code>5s</code>)</li> <li>Limit the sources watched when the <code>--events</code> flag is specified to specific types, namespaces, labels, or annotations</li> <li><code>--source=ingress --source=service</code> - specify multiple times for multiple sources</li> <li><code>--namespace=my-app</code></li> <li><code>--label-filter=app in (my-app)</code></li> <li><code>--ingress-class=nginx-external</code></li> <li>Limit services watched by type (not applicable to ingress or other types)</li> <li><code>--service-type-filter=LoadBalancer</code> default <code>all</code></li> <li>Limit the hosted zones considered</li> <li><code>--zone-id-filter=ABCDEF12345678</code> - specify multiple times if needed</li> <li><code>--domain-filter=example.com</code> by domain suffix - specify multiple times if needed</li> <li><code>--regex-domain-filter=example*</code> by domain suffix but as a regex - overrides domain-filter</li> <li><code>--exclude-domains=ignore.this.example.com</code> to exclude a domain or subdomain</li> <li><code>--regex-domain-exclusion=ignore*</code> subtracts it\u2019s matches from <code>regex-domain-filter</code>\u2019s matches</li> <li><code>--aws-zone-type=public</code> only sync zones of this type <code>[public|private]</code></li> <li><code>--aws-zone-tags=owner=k8s</code> only sync zones with this tag</li> <li>If the list of zones managed by ExternalDNS doesn\u2019t change frequently, cache it by setting a TTL.</li> <li><code>--aws-zones-cache-duration=3h</code> (default <code>0</code> - disabled)</li> <li>Increase the number of changes applied to Route53 in each batch</li> <li><code>--aws-batch-change-size=4000</code> (default <code>1000</code>)</li> <li>Increase the interval between changes</li> <li><code>--aws-batch-change-interval=10s</code> (default <code>1s</code>)</li> <li>Introducing some jitter to the pod initialization, so that when multiple instances of ExternalDNS are updated at the same time they do not make their requests on the same second.</li> </ul> <p>A simple way to implement randomised startup is with an init container:</p> <pre><code>...\n    spec:\n      initContainers:\n      - name: init-jitter\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        command:\n        - /bin/sh\n        - -c\n        - 'FOR=$((RANDOM % 10))s;echo \"Sleeping for $FOR\";sleep $FOR'\n      containers:\n...\n</code></pre>"},{"location":"docs/tutorials/aws/#eks","title":"EKS","text":"<p>An effective starting point for EKS with an ingress controller might look like:</p> <pre><code>--interval=5m\n--events\n--source=ingress\n--domain-filter=example.com\n--aws-zones-cache-duration=1h\n</code></pre>"},{"location":"docs/tutorials/aws/#batch-size-options","title":"Batch size options","text":"<p>After external-dns generates all changes, it will perform a task to group those changes into batches. Each change will be validated against batch-change-size limits. If at least one of those parameters out of range - the change will be moved to a separate batch. If the change can\u2019t fit into any batch - it will be skipped.</p> <p>There are 3 options to control batch size for AWS provider:</p> <ul> <li>Maximum amount of changes added to one batch</li> <li><code>--aws-batch-change-size</code> (default <code>1000</code>)</li> <li>Maximum size of changes in bytes added to one batch</li> <li><code>--aws-batch-change-size-bytes</code> (default <code>32000</code>)</li> <li>Maximum value count of changes added to one batch</li> <li><code>aws-batch-change-size-values</code> (default <code>1000</code>)</li> </ul> <p><code>aws-batch-change-size</code> can be very useful for throttling purposes and can be set to any value.</p> <p>Default values for flags <code>aws-batch-change-size-bytes</code> and <code>aws-batch-change-size-values</code> are taken from AWS documentation for Route53 API.</p> <p>[!WARNING] You should not change those values until you really have to. Because those limits are in place, <code>aws-batch-change-size</code> can be set to any value: Even if your batch size is <code>4000</code> records, your change will be split to separate batches due to bytes/values size limits and apply request will be finished without issues.</p>"},{"location":"docs/tutorials/aws/#using-crd-source-to-manage-dns-records-in-aws","title":"Using CRD source to manage DNS records in AWS","text":"<p>Please refer to the CRD source documentation for more information.</p>"},{"location":"docs/tutorials/azure-private-dns/","title":"Azure Private DNS","text":"<p>This tutorial describes how to set up ExternalDNS for managing records in Azure Private DNS.</p> <p>It comprises of the following steps:</p> <p>1) Provision Azure Private DNS 2) Configure service principal for managing the zone 3) Deploy ExternalDNS 4) Expose an NGINX service with a LoadBalancer and annotate it with the desired DNS name 5) Install NGINX Ingress Controller (Optional) 6) Expose an nginx service with an ingress (Optional) 7) Verify the DNS records</p> <p>Everything will be deployed on Kubernetes. Therefore, please see the subsequent prerequisites.</p>"},{"location":"docs/tutorials/azure-private-dns/#prerequisites","title":"Prerequisites","text":"<ul> <li>Azure Kubernetes Service is deployed and ready</li> <li>Azure CLI 2.0 and <code>kubectl</code> installed on the box to execute the subsequent steps</li> </ul>"},{"location":"docs/tutorials/azure-private-dns/#provision-azure-private-dns","title":"Provision Azure Private DNS","text":"<p>The provider will find suitable zones for domains it manages. It will not automatically create zones.</p> <p>For this tutorial, we will create a Azure resource group named \u2018externaldns\u2019 that can easily be deleted later.</p> <pre><code>az group create -n externaldns -l westeurope\n</code></pre> <p>Substitute a more suitable location for the resource group if desired.</p> <p>As a prerequisite for Azure Private DNS to resolve records is to define links with VNETs. Thus, first create a VNET.</p> <pre><code>$ az network vnet create \\\n  --name myvnet \\\n  --resource-group externaldns \\\n  --location westeurope \\\n  --address-prefix 10.2.0.0/16 \\\n  --subnet-name mysubnet \\\n  --subnet-prefixes 10.2.0.0/24\n</code></pre> <p>Next, create a Azure Private DNS zone for \u201cexample.com\u201d:</p> <pre><code>az network private-dns zone create -g externaldns -n example.com\n</code></pre> <p>Substitute a domain you own for \u201cexample.com\u201d if desired.</p> <p>Finally, create the mentioned link with the VNET.</p> <pre><code>$ az network private-dns link vnet create -g externaldns -n mylink \\\n   -z example.com -v myvnet --registration-enabled false\n</code></pre>"},{"location":"docs/tutorials/azure-private-dns/#configure-service-principal-for-managing-the-zone","title":"Configure service principal for managing the zone","text":"<p>ExternalDNS needs permissions to make changes in Azure Private DNS. These permissions are roles assigned to the service principal used by ExternalDNS.</p> <p>A service principal with a minimum access level of <code>Private DNS Zone Contributor</code> to the Private DNS zone(s) and <code>Reader</code> to the resource group containing the Azure Private DNS zone(s) is necessary. More powerful role-assignments like <code>Owner</code> or assignments on subscription-level work too.</p> <p>Start off by creating the service principal without role-assignments.</p> <pre><code>$ az ad sp create-for-rbac --skip-assignment -n http://externaldns-sp\n{\n  \"appId\": \"appId GUID\",  &lt;-- aadClientId value\n  ...\n  \"password\": \"password\",  &lt;-- aadClientSecret value\n  \"tenant\": \"AzureAD Tenant Id\"  &lt;-- tenantId value\n}\n</code></pre> <p>Note: Alternatively, you can issue <code>az account show --query \"tenantId\"</code> to retrieve the id of your AAD Tenant too.</p> <p>Next, assign the roles to the service principal. But first retrieve the ID\u2019s of the objects to assign roles on.</p> <pre><code># find out the resource ids of the resource group where the dns zone is deployed, and the dns zone itself\n$ az group show --name externaldns --query id -o tsv\n/subscriptions/id/resourceGroups/externaldns\n\n$ az network private-dns zone show --name example.com -g externaldns --query id -o tsv\n/subscriptions/.../resourceGroups/externaldns/providers/Microsoft.Network/privateDnsZones/example.com\n</code></pre> <p>Now, create role assignments.</p> <pre><code># 1. as a reader to the resource group\n$ az role assignment create --role \"Reader\" --assignee &lt;appId GUID&gt; --scope &lt;resource group resource id&gt;\n\n# 2. as a contributor to DNS Zone itself\n$ az role assignment create --role \"Private DNS Zone Contributor\" --assignee &lt;appId GUID&gt; --scope &lt;dns zone resource id&gt;\n</code></pre>"},{"location":"docs/tutorials/azure-private-dns/#throttling","title":"Throttling","text":"<p>When the ExternalDNS managed zones list doesn\u2019t change frequently, one can set <code>--azure-zones-cache-duration</code> (zones list cache time-to-live). The zones list cache is disabled by default, with a value of 0s.</p>"},{"location":"docs/tutorials/azure-private-dns/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Configure <code>kubectl</code> to be able to communicate and authenticate with your cluster. This is per default done through the file <code>~/.kube/config</code>.</p> <p>For general background information on this see kubernetes-docs. Azure-CLI features functionality for automatically maintaining this file for AKS-Clusters. See Azure-Docs.</p> <p>Follow the steps for azure-dns provider to create a configuration file.</p> <p>Then apply one of the following manifests depending on whether you use RBAC or not.</p> <p>The credentials of the service principal are provided to ExternalDNS as environment-variables.</p>"},{"location":"docs/tutorials/azure-private-dns/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: externaldns\nspec:\n  selector:\n    matchLabels:\n      app: externaldns\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: externaldns\n    spec:\n      containers:\n      - name: externaldns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service\n        - --source=ingress\n        - --domain-filter=example.com\n        - --provider=azure-private-dns\n        - --azure-resource-group=externaldns\n        - --azure-subscription-id=&lt;use the id of your subscription&gt;\n        volumeMounts:\n        - name: azure-config-file\n          mountPath: /etc/kubernetes\n          readOnly: true\n      volumes:\n      - name: azure-config-file\n        secret:\n          secretName: azure-config-file\n</code></pre>"},{"location":"docs/tutorials/azure-private-dns/#manifest-for-clusters-with-rbac-enabled-cluster-access","title":"Manifest (for clusters with RBAC enabled, cluster access)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: externaldns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: externaldns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: externaldns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: externaldns\nsubjects:\n- kind: ServiceAccount\n  name: externaldns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: externaldns\nspec:\n  selector:\n    matchLabels:\n      app: externaldns\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: externaldns\n    spec:\n      serviceAccountName: externaldns\n      containers:\n      - name: externaldns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service\n        - --source=ingress\n        - --domain-filter=example.com\n        - --provider=azure-private-dns\n        - --azure-resource-group=externaldns\n        - --azure-subscription-id=&lt;use the id of your subscription&gt;\n        volumeMounts:\n        - name: azure-config-file\n          mountPath: /etc/kubernetes\n          readOnly: true\n      volumes:\n      - name: azure-config-file\n        secret:\n          secretName: azure-config-file\n</code></pre>"},{"location":"docs/tutorials/azure-private-dns/#manifest-for-clusters-with-rbac-enabled-namespace-access","title":"Manifest (for clusters with RBAC enabled, namespace access)","text":"<p>This configuration is the same as above, except it only requires privileges for the current namespace, not for the whole cluster. However, access to nodes requires cluster access, so when using this manifest, services with type <code>NodePort</code> will be skipped!</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: externaldns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: externaldns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: externaldns\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: externaldns\nsubjects:\n- kind: ServiceAccount\n  name: externaldns\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: externaldns\nspec:\n  selector:\n    matchLabels:\n      app: externaldns\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: externaldns\n    spec:\n      serviceAccountName: externaldns\n      containers:\n      - name: externaldns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service\n        - --source=ingress\n        - --domain-filter=example.com\n        - --provider=azure-private-dns\n        - --azure-resource-group=externaldns\n        - --azure-subscription-id=&lt;use the id of your subscription&gt;\n        volumeMounts:\n        - name: azure-config-file\n          mountPath: /etc/kubernetes\n          readOnly: true\n      volumes:\n      - name: azure-config-file\n        secret:\n          secretName: azure-config-file\n</code></pre> <p>Create the deployment for ExternalDNS:</p> <pre><code>kubectl create -f externaldns.yaml\n</code></pre>"},{"location":"docs/tutorials/azure-private-dns/#create-an-nginx-deployment","title":"Create an nginx deployment","text":"<p>This step creates a demo workload in your cluster. Apply the following manifest to create a deployment that we are going to expose later in this tutorial in multiple ways:</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - image: nginx\n          name: nginx\n          ports:\n          - containerPort: 80\n</code></pre>"},{"location":"docs/tutorials/azure-private-dns/#expose-the-nginx-deployment-with-a-load-balancer","title":"Expose the nginx deployment with a load balancer","text":"<p>Apply the following manifest to create a service of type <code>LoadBalancer</code>. This will create a public load balancer in Azure that will forward traffic to the nginx pods.</p> <pre><code>---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-svc\n  annotations:\n    service.beta.kubernetes.io/azure-load-balancer-internal: \"true\"\n    external-dns.alpha.kubernetes.io/hostname: server.example.com\n    external-dns.alpha.kubernetes.io/internal-hostname: server-clusterip.example.com\nspec:\n  ports:\n    - port: 80\n      protocol: TCP\n      targetPort: 80\n  selector:\n    app: nginx\n  type: LoadBalancer\n</code></pre> <p>In the service we used multiple annotations. The annotation <code>service.beta.kubernetes.io/azure-load-balancer-internal</code> is used to create an internal load balancer. The annotation <code>external-dns.alpha.kubernetes.io/hostname</code> is used to create a DNS record for the load balancer that will point to the internal IP address in the VNET allocated by the internal load balancer. The annotation <code>external-dns.alpha.kubernetes.io/internal-hostname</code> is used to create a private DNS record for the load balancer that will point to the cluster IP.</p>"},{"location":"docs/tutorials/azure-private-dns/#install-nginx-ingress-controller-optional","title":"Install NGINX Ingress Controller (Optional)","text":"<p>Helm is used to deploy the ingress controller.</p> <p>We employ the popular chart ingress-nginx.</p> <pre><code>$ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n$ helm repo update\n$ helm install [RELEASE_NAME] ingress-nginx/ingress-nginx\n     --set controller.publishService.enabled=true\n</code></pre> <p>The parameter <code>controller.publishService.enabled</code> needs to be set to <code>true.</code></p> <p>It will make the ingress controller update the endpoint records of ingress-resources to contain the external-ip of the loadbalancer serving the ingress-controller. This is crucial as ExternalDNS reads those endpoints records when creating DNS-Records from ingress-resources. In the subsequent parameter we will make use of this. If you don\u2019t want to work with ingress-resources in your later use, you can leave the parameter out.</p> <p>Verify the correct propagation of the loadbalancer\u2019s ip by listing the ingresses.</p> <pre><code>kubectl get ingress\n</code></pre> <p>The address column should contain the ip for each ingress. ExternalDNS will pick up exactly this piece of information.</p> <pre><code>NAME     HOSTS             ADDRESS          PORTS   AGE\nnginx1   sample1.aks.com   52.167.195.110   80      6d22h\nnginx2   sample2.aks.com   52.167.195.110   80      6d21h\n</code></pre> <p>If you do not want to deploy the ingress controller with Helm, ensure to pass the following cmdline-flags to it through the mechanism of your choice:</p> <pre><code>flags:\n--publish-service=&lt;namespace of ingress-controller &gt;/&lt;svcname of ingress-controller&gt;\n--update-status=true (default-value)\n\nexample:\n./nginx-ingress-controller --publish-service=default/nginx-ingress-controller\n</code></pre>"},{"location":"docs/tutorials/azure-private-dns/#expose-the-nginx-deployment-with-the-ingress-optional","title":"Expose the nginx deployment with the ingress (Optional)","text":"<p>Apply the following manifest to create an ingress resource that will expose the nginx deployment. The ingress resource backend points to a <code>ClusterIP</code> service that is needed to select the pods that will receive the traffic.</p> <pre><code>---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-svc-clusterip\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: nginx\n  type: ClusterIP\n\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nginx\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: server.example.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: nginx-svc-clusterip\n            port:\n              number: 80\n        pathType: Prefix\n</code></pre> <p>When you use ExternalDNS with Ingress resources, it automatically creates DNS records based on the hostnames listed in those Ingress objects. Those hostnames must match the filters that you defined (if any):</p> <ul> <li>By default, <code>--domain-filter</code> filters Azure Private DNS zone.</li> <li>If you use <code>--domain-filter</code> together with <code>--zone-name-filter</code>, the behavior changes: <code>--domain-filter</code> then filters Ingress domains, not the Azure Private DNS zone name.</li> </ul> <p>When those hostnames are removed or renamed the corresponding DNS records are also altered.</p> <p>Create the deployment, service and ingress object:</p> <pre><code>kubectl create -f nginx.yaml\n</code></pre> <p>Since your external IP would have already been assigned to the nginx-ingress service, the DNS records pointing to the IP of the nginx-ingress service should be created within a minute.</p>"},{"location":"docs/tutorials/azure-private-dns/#verify-created-records","title":"Verify created records","text":"<p>Run the following command to view the A records for your Azure Private DNS zone:</p> <pre><code>az network private-dns record-set a list -g externaldns -z example.com\n</code></pre> <p>Substitute the zone for the one created above if a different domain was used.</p> <p>This should show the external IP address of the service as the A record for your domain (\u2018@\u2019 indicates the record is for the zone itself).</p>"},{"location":"docs/tutorials/azure/","title":"Azure DNS","text":"<p>This tutorial describes how to setup ExternalDNS for Azure DNS with Azure Kubernetes Service.</p> <p>Make sure to use &gt;=0.11.0 version of ExternalDNS for this tutorial.</p> <p>This tutorial uses Azure CLI 2.0 for all Azure commands and assumes that the Kubernetes cluster was created via Azure Container Services and <code>kubectl</code> commands are being run on an orchestration node.</p>"},{"location":"docs/tutorials/azure/#creating-an-azure-dns-zone","title":"Creating an Azure DNS zone","text":"<p>The Azure provider for ExternalDNS will find suitable zones for domains it manages; it will not automatically create zones.</p> <p>For this tutorial, we will create a Azure resource group named <code>MyDnsResourceGroup</code> that can easily be deleted later:</p> <pre><code>az group create --name \"MyDnsResourceGroup\" --location \"eastus\"\n</code></pre> <p>Substitute a more suitable location for the resource group if desired.</p> <p>Next, create a Azure DNS zone for <code>example.com</code>:</p> <pre><code>az network dns zone create --resource-group \"MyDnsResourceGroup\" --name \"example.com\"\n</code></pre> <p>Substitute a domain you own for <code>example.com</code> if desired.</p> <p>If using your own domain that was registered with a third-party domain registrar, you should point your domain\u2019s name servers to the values in the <code>nameServers</code> field from the JSON data returned by the <code>az network dns zone create</code> command. Please consult your registrar\u2019s documentation on how to do that.</p>"},{"location":"docs/tutorials/azure/#internal-load-balancer","title":"Internal Load Balancer","text":"<p>To create internal load balancers, one can set the annotation <code>service.beta.kubernetes.io/azure-load-balancer-internal</code> to <code>true</code> on the resource. Note: AKS cluster\u2019s control plane managed identity needs to be granted <code>Network Contributor</code> role to update the subnet. For more details refer to Use an internal load balancer with Azure Kubernetes Service (AKS)</p>"},{"location":"docs/tutorials/azure/#configuration-file","title":"Configuration file","text":"<p>The azure provider will reference a configuration file called <code>azure.json</code>.  The preferred way to inject the configuration file is by using a Kubernetes secret. The secret should contain an object named <code>azure.json</code> with content similar to this:</p> <pre><code>{\n  \"tenantId\": \"01234abc-de56-ff78-abc1-234567890def\",\n  \"subscriptionId\": \"01234abc-de56-ff78-abc1-234567890def\",\n  \"resourceGroup\": \"MyDnsResourceGroup\",\n  \"aadClientId\": \"01234abc-de56-ff78-abc1-234567890def\",\n  \"aadClientSecret\": \"uKiuXeiwui4jo9quae9o\"\n}\n</code></pre> <p>The following fields are used:</p> <ul> <li><code>tenantId</code> (required) - run <code>az account show --query \"tenantId\"</code> or by selecting Azure Active Directory in the Azure Portal and checking the Directory ID under Properties.</li> <li><code>subscriptionId</code> (required) - run <code>az account show --query \"id\"</code> or by selecting Subscriptions in the Azure Portal.</li> <li><code>resourceGroup</code> (required) is the Resource Group created in a previous step that contains the Azure DNS Zone.</li> <li><code>aadClientID</code> is associated with the Service Principal. This is used with Service Principal or Workload Identity methods documented in the next section.</li> <li><code>aadClientSecret</code> is associated with the Service Principal. This is only used with Service Principal method documented in the next section.</li> <li><code>useManagedIdentityExtension</code> - this is set to <code>true</code> if you use either AKS Kubelet Identity or AAD Pod Identities methods documented in the next section.</li> <li><code>userAssignedIdentityID</code> - this contains the client id from the Managed identity when using the AAD Pod Identities method documented in the next setion.</li> <li><code>activeDirectoryAuthorityHost</code> - this contains the uri to overwrite the default provided AAD Endpoint. This is useful for providing additional support where the endpoint is not available in the default cloud config from the azure-sdk-for-go.</li> <li><code>useWorkloadIdentityExtension</code> - this is set to <code>true</code> if you use Workload Identity method documented in the next section.</li> </ul> <p>The Azure DNS provider expects, by default, that the configuration file is at <code>/etc/kubernetes/azure.json</code>.  This can be overridden with the <code>--azure-config-file</code> option when starting ExternalDNS.</p>"},{"location":"docs/tutorials/azure/#permissions-to-modify-dns-zone","title":"Permissions to modify DNS zone","text":"<p>ExternalDNS needs permissions to make changes to the Azure DNS zone. There are four ways configure the access needed:</p> <ul> <li>Service Principal</li> <li>Managed Identity Using AKS Kubelet Identity</li> <li>Managed Identity Using AAD Pod Identities</li> <li>Managed Identity Using Workload Identity</li> </ul>"},{"location":"docs/tutorials/azure/#service-principal","title":"Service Principal","text":"<p>These permissions are defined in a Service Principal that should be made available to ExternalDNS as a configuration file <code>azure.json</code>.</p>"},{"location":"docs/tutorials/azure/#creating-a-service-principal","title":"Creating a service principal","text":"<p>A Service Principal with a minimum access level of <code>DNS Zone Contributor</code> or <code>Contributor</code> to the DNS zone(s) and <code>Reader</code> to the resource group containing the Azure DNS zone(s) is necessary for ExternalDNS to be able to edit DNS records. However, other more permissive access levels will work too (e.g. <code>Contributor</code> to the resource group or the whole subscription).</p> <p>This is an Azure CLI example on how to query the Azure API for the information required for the Resource Group and DNS zone you would have already created in previous steps (requires <code>azure-cli</code> and <code>jq</code>)</p> <pre><code>$ EXTERNALDNS_NEW_SP_NAME=\"ExternalDnsServicePrincipal\" # name of the service principal\n$ AZURE_DNS_ZONE_RESOURCE_GROUP=\"MyDnsResourceGroup\" # name of resource group where dns zone is hosted\n$ AZURE_DNS_ZONE=\"example.com\" # DNS zone name like example.com or sub.example.com\n\n# Create the service principal\n$ DNS_SP=$(az ad sp create-for-rbac --name $EXTERNALDNS_NEW_SP_NAME)\n$ EXTERNALDNS_SP_APP_ID=$(echo $DNS_SP | jq -r '.appId')\n$ EXTERNALDNS_SP_PASSWORD=$(echo $DNS_SP | jq -r '.password')\n</code></pre>"},{"location":"docs/tutorials/azure/#assign-the-rights-for-the-service-principal","title":"Assign the rights for the service principal","text":"<p>Grant access to Azure DNS zone for the service principal.</p> <pre><code># fetch DNS id used to grant access to the service principal\nDNS_ID=$(az network dns zone show --name $AZURE_DNS_ZONE \\\n --resource-group $AZURE_DNS_ZONE_RESOURCE_GROUP --query \"id\" --output tsv)\n\n# 1. as a reader to the resource group\n$ az role assignment create --role \"Reader\" --assignee $EXTERNALDNS_SP_APP_ID --scope $DNS_ID\n\n# 2. as a contributor to DNS Zone itself\n$ az role assignment create --role \"Contributor\" --assignee $EXTERNALDNS_SP_APP_ID --scope $DNS_ID\n</code></pre>"},{"location":"docs/tutorials/azure/#creating-a-configuration-file-for-the-service-principal","title":"Creating a configuration file for the service principal","text":"<p>Create the file <code>azure.json</code> with values gather from previous steps.</p> <pre><code>cat &lt;&lt;-EOF &gt; /local/path/to/azure.json\n{\n  \"tenantId\": \"$(az account show --query tenantId -o tsv)\",\n  \"subscriptionId\": \"$(az account show --query id -o tsv)\",\n  \"resourceGroup\": \"$AZURE_DNS_ZONE_RESOURCE_GROUP\",\n  \"aadClientId\": \"$EXTERNALDNS_SP_APP_ID\",\n  \"aadClientSecret\": \"$EXTERNALDNS_SP_PASSWORD\"\n}\nEOF\n</code></pre> <p>Use this file to create a Kubernetes secret:</p> <pre><code>kubectl create secret generic azure-config-file --namespace \"default\" --from-file /local/path/to/azure.json\n</code></pre>"},{"location":"docs/tutorials/azure/#managed-identity-using-aks-kubelet-identity","title":"Managed identity using AKS Kubelet identity","text":"<p>The managed identity that is assigned to the underlying node pool in the AKS cluster can be given permissions to access Azure DNS. Managed identities are essentially a service principal whose lifecycle is managed, such as deleting the AKS cluster will also delete the service principals associated with the AKS cluster. The managed identity assigned Kubernetes node pool, or specifically the VMSS, is called the Kubelet identity.</p> <p>The managed identites were previously called MSI (Managed Service Identity) and are enabled by default when creating an AKS cluster.</p> <p>Note that permissions granted to this identity will be accessible to all containers running inside the Kubernetes cluster, not just the ExternalDNS container(s).</p> <p>For the managed identity, the contents of <code>azure.json</code> should be similar to this:</p> <pre><code>{\n  \"tenantId\": \"01234abc-de56-ff78-abc1-234567890def\",\n  \"subscriptionId\": \"01234abc-de56-ff78-abc1-234567890def\",\n  \"resourceGroup\": \"MyDnsResourceGroup\",\n  \"useManagedIdentityExtension\": true,\n  \"userAssignedIdentityID\": \"01234abc-de56-ff78-abc1-234567890def\"\n}\n</code></pre>"},{"location":"docs/tutorials/azure/#fetching-the-kubelet-identity","title":"Fetching the Kubelet identity","text":"<p>For this process, you will need to get the kubelet identity:</p> <pre><code>$ PRINCIPAL_ID=$(az aks show --resource-group $CLUSTER_GROUP --name $CLUSTERNAME \\\n  --query \"identityProfile.kubeletidentity.objectId\" --output tsv)\n$ IDENTITY_CLIENT_ID=$(az aks show --resource-group $CLUSTER_GROUP --name $CLUSTERNAME \\\n  --query \"identityProfile.kubeletidentity.clientId\" --output tsv)\n</code></pre>"},{"location":"docs/tutorials/azure/#assign-rights-for-the-kubelet-identity","title":"Assign rights for the Kubelet identity","text":"<p>Grant access to Azure DNS zone for the kubelet identity.</p> <pre><code>$ AZURE_DNS_ZONE=\"example.com\" # DNS zone name like example.com or sub.example.com\n$ AZURE_DNS_ZONE_RESOURCE_GROUP=\"MyDnsResourceGroup\" # resource group where DNS zone is hosted\n\n# fetch DNS id used to grant access to the kubelet identity\n$ DNS_ID=$(az network dns zone show --name $AZURE_DNS_ZONE \\\n  --resource-group $AZURE_DNS_ZONE_RESOURCE_GROUP --query \"id\" --output tsv)\n\n$ az role assignment create --role \"DNS Zone Contributor\" --assignee $PRINCIPAL_ID --scope $DNS_ID\n</code></pre>"},{"location":"docs/tutorials/azure/#creating-a-configuration-file-for-the-kubelet-identity","title":"Creating a configuration file for the kubelet identity","text":"<p>Create the file <code>azure.json</code> with values gather from previous steps.</p> <pre><code>cat &lt;&lt;-EOF &gt; /local/path/to/azure.json\n{\n  \"tenantId\": \"$(az account show --query tenantId -o tsv)\",\n  \"subscriptionId\": \"$(az account show --query id -o tsv)\",\n  \"resourceGroup\": \"$AZURE_DNS_ZONE_RESOURCE_GROUP\",\n  \"useManagedIdentityExtension\": true,\n  \"userAssignedIdentityID\": \"$IDENTITY_CLIENT_ID\"\n}\nEOF\n</code></pre> <p>Use the <code>azure.json</code> file to create a Kubernetes secret:</p> <pre><code>kubectl create secret generic azure-config-file --namespace \"default\" --from-file /local/path/to/azure.json\n</code></pre>"},{"location":"docs/tutorials/azure/#managed-identity-using-aad-pod-identities","title":"Managed identity using AAD Pod Identities","text":"<p>For this process, we will create a managed identity that will be explicitly used by the ExternalDNS container. This process is similar to Kubelet identity except that this managed identity is not associated with the Kubernetes node pool, but rather associated with explicit ExternalDNS containers.</p>"},{"location":"docs/tutorials/azure/#enable-the-aad-pod-identities-feature","title":"Enable the AAD Pod Identities feature","text":"<p>For this solution, AAD Pod Identities preview feature can be enabled.  The commands below should do the trick to enable this feature:</p> <pre><code>az feature register --name EnablePodIdentityPreview --namespace Microsoft.ContainerService\naz feature register --name AutoUpgradePreview --namespace Microsoft.ContainerService\naz extension add --name aks-preview\naz extension update --name aks-preview\naz provider register --namespace Microsoft.ContainerService\n</code></pre>"},{"location":"docs/tutorials/azure/#deploy-the-aad-pod-identities-service","title":"Deploy the AAD Pod Identities service","text":"<p>Once enabled, you can update your cluster and install needed services for the AAD Pod Identities feature.</p> <pre><code>AZURE_AKS_RESOURCE_GROUP=\"my-aks-cluster-group\" # name of resource group where aks cluster was created\nAZURE_AKS_CLUSTER_NAME=\"my-aks-cluster\" # name of aks cluster previously created\n\naz aks update --resource-group ${AZURE_AKS_RESOURCE_GROUP} --name ${AZURE_AKS_CLUSTER_NAME} --enable-pod-identity\n</code></pre> <p>Note that, if you use the default network plugin <code>kubenet</code>, then you need to add the command line option <code>--enable-pod-identity-with-kubenet</code> to the above command.</p>"},{"location":"docs/tutorials/azure/#creating-the-managed-identity","title":"Creating the managed identity","text":"<p>After this process is finished, create a managed identity.</p> <pre><code>$ IDENTITY_RESOURCE_GROUP=$AZURE_AKS_RESOURCE_GROUP # custom group or reuse AKS group\n$ IDENTITY_NAME=\"example-com-identity\"\n\n# create a managed identity\n$ az identity create --resource-group \"${IDENTITY_RESOURCE_GROUP}\" --name \"${IDENTITY_NAME}\"\n</code></pre>"},{"location":"docs/tutorials/azure/#assign-rights-for-the-managed-identity","title":"Assign rights for the managed identity","text":"<p>Grant access to Azure DNS zone for the managed identity.</p> <pre><code>$ AZURE_DNS_ZONE_RESOURCE_GROUP=\"MyDnsResourceGroup\" # name of resource group where dns zone is hosted\n$ AZURE_DNS_ZONE=\"example.com\" # DNS zone name like example.com or sub.example.com\n\n# fetch identity client id from managed identity created earlier\n$ IDENTITY_CLIENT_ID=$(az identity show --resource-group \"${IDENTITY_RESOURCE_GROUP}\" \\\n  --name \"${IDENTITY_NAME}\" --query \"clientId\" --output tsv)\n# fetch DNS id used to grant access to the managed identity\n$ DNS_ID=$(az network dns zone show --name \"${AZURE_DNS_ZONE}\" \\\n  --resource-group \"${AZURE_DNS_ZONE_RESOURCE_GROUP}\" --query \"id\" --output tsv)\n\n$ az role assignment create --role \"DNS Zone Contributor\" \\\n  --assignee \"${IDENTITY_CLIENT_ID}\" --scope \"${DNS_ID}\"\n</code></pre>"},{"location":"docs/tutorials/azure/#creating-a-configuration-file-for-the-managed-identity","title":"Creating a configuration file for the managed identity","text":"<p>Create the file <code>azure.json</code> with the values from previous steps:</p> <pre><code>cat &lt;&lt;-EOF &gt; /local/path/to/azure.json\n{\n  \"tenantId\": \"$(az account show --query tenantId -o tsv)\",\n  \"subscriptionId\": \"$(az account show --query id -o tsv)\",\n  \"resourceGroup\": \"$AZURE_DNS_ZONE_RESOURCE_GROUP\",\n  \"useManagedIdentityExtension\": true,\n  \"userAssignedIdentityID\": \"$IDENTITY_CLIENT_ID\"\n}\nEOF\n</code></pre> <p>Use the <code>azure.json</code> file to create a Kubernetes secret:</p> <pre><code>kubectl create secret generic azure-config-file --namespace \"default\" --from-file /local/path/to/azure.json\n</code></pre>"},{"location":"docs/tutorials/azure/#creating-an-azure-identity-binding","title":"Creating an Azure identity binding","text":"<p>A binding between the managed identity and the ExternalDNS pods needs to be setup by creating <code>AzureIdentity</code> and <code>AzureIdentityBinding</code> resources. This will allow appropriately labeled ExternalDNS pods to authenticate using the managed identity.  When AAD Pod Identity feature is enabled from previous steps above, the <code>az aks pod-identity add</code> can be used to create these resources:</p> <pre><code>$ IDENTITY_RESOURCE_ID=$(az identity show --resource-group ${IDENTITY_RESOURCE_GROUP} \\\n  --name ${IDENTITY_NAME} --query id --output tsv)\n\n$ az aks pod-identity add --resource-group ${AZURE_AKS_RESOURCE_GROUP}  \\\n  --cluster-name ${AZURE_AKS_CLUSTER_NAME} --namespace \"default\" \\\n  --name \"external-dns\" --identity-resource-id ${IDENTITY_RESOURCE_ID}\n</code></pre> <p>This will add something similar to the following resources:</p> <pre><code>apiVersion: aadpodidentity.k8s.io/v1\nkind: AzureIdentity\nmetadata:\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.azure.com/managedby: aks\n  name: external-dns\nspec:\n  clientID: $IDENTITY_CLIENT_ID\n  resourceID: $IDENTITY_RESOURCE_ID\n  type: 0\n---\napiVersion: aadpodidentity.k8s.io/v1\nkind: AzureIdentityBinding\nmetadata:\n  annotations:\n  labels:\n    addonmanager.kubernetes.io/mode: Reconcile\n    kubernetes.azure.com/managedby: aks\n  name: external-dns-binding\nspec:\n  azureIdentity: external-dns\n  selector: external-dns\n</code></pre>"},{"location":"docs/tutorials/azure/#update-externaldns-labels","title":"Update ExternalDNS labels","text":"<p>When deploying ExternalDNS, you want to make sure that deployed pod(s) will have the label <code>aadpodidbinding: external-dns</code> to enable AAD Pod Identities. You can patch an existing deployment of ExternalDNS with this command:</p> <pre><code>kubectl patch deployment external-dns --namespace \"default\" --patch \\\n '{\"spec\": {\"template\": {\"metadata\": {\"labels\": {\"aadpodidbinding\": \"external-dns\"}}}}}'\n</code></pre>"},{"location":"docs/tutorials/azure/#managed-identity-using-workload-identity","title":"Managed identity using Workload Identity","text":"<p>For this process, we will create a managed identity that will be explicitly used by the ExternalDNS container. This process is somewhat similar to Pod Identity except that this managed identity is associated with a kubernetes service account.</p>"},{"location":"docs/tutorials/azure/#deploy-oidc-issuer-and-workload-identity-services","title":"Deploy OIDC issuer and Workload Identity services","text":"<p>Update your cluster to install OIDC Issuer and Workload Identity:</p> <pre><code>AZURE_AKS_RESOURCE_GROUP=\"my-aks-cluster-group\" # name of resource group where aks cluster was created\nAZURE_AKS_CLUSTER_NAME=\"my-aks-cluster\" # name of aks cluster previously created\n\naz aks update --resource-group ${AZURE_AKS_RESOURCE_GROUP} --name ${AZURE_AKS_CLUSTER_NAME} --enable-oidc-issuer --enable-workload-identity\n</code></pre>"},{"location":"docs/tutorials/azure/#create-a-managed-identity","title":"Create a managed identity","text":"<p>Create a managed identity:</p> <pre><code>$ IDENTITY_RESOURCE_GROUP=$AZURE_AKS_RESOURCE_GROUP # custom group or reuse AKS group\n$ IDENTITY_NAME=\"example-com-identity\"\n\n# create a managed identity\n$ az identity create --resource-group \"${IDENTITY_RESOURCE_GROUP}\" --name \"${IDENTITY_NAME}\"\n</code></pre>"},{"location":"docs/tutorials/azure/#assign-a-role-to-the-managed-identity","title":"Assign a role to the managed identity","text":"<p>Grant access to Azure DNS zone for the managed identity:</p> <pre><code>$ AZURE_DNS_ZONE_RESOURCE_GROUP=\"MyDnsResourceGroup\" # name of resource group where dns zone is hosted\n$ AZURE_DNS_ZONE=\"example.com\" # DNS zone name like example.com or sub.example.com\n\n# fetch identity client id from managed identity created earlier\n$ IDENTITY_CLIENT_ID=$(az identity show --resource-group \"${IDENTITY_RESOURCE_GROUP}\" \\\n  --name \"${IDENTITY_NAME}\" --query \"clientId\" --output tsv)\n# fetch DNS id used to grant access to the managed identity\n$ DNS_ID=$(az network dns zone show --name \"${AZURE_DNS_ZONE}\" \\\n  --resource-group \"${AZURE_DNS_ZONE_RESOURCE_GROUP}\" --query \"id\" --output tsv)\n$ RESOURCE_GROUP_ID=$(az group show --name \"${AZURE_DNS_ZONE_RESOURCE_GROUP}\" --query \"id\" --output tsv)\n\n$ az role assignment create --role \"DNS Zone Contributor\" \\\n  --assignee \"${IDENTITY_CLIENT_ID}\" --scope \"${DNS_ID}\"\n$ az role assignment create --role \"Reader\" \\\n  --assignee \"${IDENTITY_CLIENT_ID}\" --scope \"${RESOURCE_GROUP_ID}\"\n</code></pre>"},{"location":"docs/tutorials/azure/#create-a-federated-identity-credential","title":"Create a federated identity credential","text":"<p>A binding between the managed identity and the ExternalDNS service account needs to be setup by creating a federated identity resource:</p> <pre><code>OIDC_ISSUER_URL=\"$(az aks show -n myAKSCluster -g myResourceGroup --query \"oidcIssuerProfile.issuerUrl\" -otsv)\"\n\naz identity federated-credential create --name ${IDENTITY_NAME} --identity-name ${IDENTITY_NAME} --resource-group $AZURE_AKS_RESOURCE_GROUP} --issuer \"$OIDC_ISSUER_URL\" --subject \"system:serviceaccount:default:external-dns\"\n</code></pre> <p>NOTE: make sure federated credential refers to correct namespace and service account (<code>system:serviceaccount:&lt;NAMESPACE&gt;:&lt;SERVICE_ACCOUNT&gt;</code>)</p>"},{"location":"docs/tutorials/azure/#helm","title":"Helm","text":"<p>When deploying external-dns with Helm you need to create a secret to store the Azure config (see below) and create a workload identity (out of scope here) before you can install the chart.</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: external-dns-azure\ntype: Opaque\ndata:\n  azure.json: |\n    {\n      \"tenantId\": \"&lt;TENANT_ID&gt;\",\n      \"subscriptionId\": \"&lt;SUBSCRIPTION_ID&gt;\",\n      \"resourceGroup\": \"&lt;AZURE_DNS_ZONE_RESOURCE_GROUP&gt;\",\n      \"useWorkloadIdentityExtension\": true\n    }\n</code></pre> <p>Once you have created the secret and have a workload identity you can install the chart with the following values.</p> <pre><code>fullnameOverride: external-dns\n\nserviceAccount:\n  labels:\n    azure.workload.identity/use: \"true\"\n  annotations:\n    azure.workload.identity/client-id: &lt;IDENTITY_CLIENT_ID&gt;\n\npodLabels:\n  azure.workload.identity/use: \"true\"\n\nextraVolumes:\n  - name: azure-config-file\n    secret:\n      secretName: external-dns-azure\n\nextraVolumeMounts:\n  - name: azure-config-file\n    mountPath: /etc/kubernetes\n    readOnly: true\n\nprovider:\n  name: azure\n</code></pre> <p>NOTE: make sure the pod is restarted whenever you make a configuration change.</p>"},{"location":"docs/tutorials/azure/#kubectl-alternative","title":"kubectl (alternative)","text":""},{"location":"docs/tutorials/azure/#create-a-configuration-file-for-the-managed-identity","title":"Create a configuration file for the managed identity","text":"<p>Create the file <code>azure.json</code> with the values from previous steps:</p> <pre><code>cat &lt;&lt;-EOF &gt; /local/path/to/azure.json\n{\n  \"subscriptionId\": \"$(az account show --query id -o tsv)\",\n  \"resourceGroup\": \"$AZURE_DNS_ZONE_RESOURCE_GROUP\",\n  \"useWorkloadIdentityExtension\": true\n}\nEOF\n</code></pre> <p>NOTE: it\u2019s also possible to specify (or override) ClientID specified in the next section through <code>aadClientId</code> field in this <code>azure.json</code> file.</p> <p>Use the <code>azure.json</code> file to create a Kubernetes secret:</p> <pre><code>kubectl create secret generic azure-config-file --namespace \"default\" --from-file /local/path/to/azure.json\n</code></pre>"},{"location":"docs/tutorials/azure/#update-labels-and-annotations-on-externaldns-service-account","title":"Update labels and annotations on ExternalDNS service account","text":"<p>To instruct Workload Identity webhook to inject a projected token into the ExternalDNS pod, the pod needs to have a label <code>azure.workload.identity/use: \"true\"</code> (before Workload Identity 1.0.0, this label was supposed to be set on the service account instead). Also, the service account needs to have an annotation <code>azure.workload.identity/client-id: &lt;IDENTITY_CLIENT_ID&gt;</code>:</p> <p>To patch the existing serviceaccount and deployment, use the following command:</p> <pre><code>$ kubectl patch serviceaccount external-dns --namespace \"default\" --patch \\\n \"{\\\"metadata\\\": {\\\"annotations\\\": {\\\"azure.workload.identity/client-id\\\": \\\"${IDENTITY_CLIENT_ID}\\\"}}}\"\n$ kubectl patch deployment external-dns --namespace \"default\" --patch \\\n '{\"spec\": {\"template\": {\"metadata\": {\"labels\": {\\\"azure.workload.identity/use\\\": \\\"true\\\"}}}}}'\n</code></pre> <p>NOTE: it\u2019s also possible to specify (or override) ClientID through <code>aadClientId</code> field in <code>azure.json</code>.</p> <p>NOTE: make sure the pod is restarted whenever you make a configuration change.</p>"},{"location":"docs/tutorials/azure/#throttling","title":"Throttling","text":"<p>When the ExternalDNS managed zones list doesn\u2019t change frequently, one can set <code>--azure-zones-cache-duration</code> (zones list cache time-to-live). The zones list cache is disabled by default, with a value of 0s.</p>"},{"location":"docs/tutorials/azure/#ingress-used-with-externaldns","title":"Ingress used with ExternalDNS","text":"<p>This deployment assumes that you will be using nginx-ingress. When using nginx-ingress do not deploy it as a Daemon Set. This causes nginx-ingress to write the Cluster IP of the backend pods in the ingress status.loadbalancer.ip property which then has external-dns write the Cluster IP(s) in DNS vs. the nginx-ingress service external IP.</p> <p>Ensure that your nginx-ingress deployment has the following arg: added to it:</p> <pre><code>- --publish-service=namespace/nginx-ingress-controller-svcname\n</code></pre> <p>For more details see here: nginx-ingress external-dns</p>"},{"location":"docs/tutorials/azure/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Connect your <code>kubectl</code> client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.</p> <p>The deployment assumes that ExternalDNS will be installed into the <code>default</code> namespace.  If this namespace is different, the <code>ClusterRoleBinding</code> will need to be updated to reflect the desired alternative namespace, such as <code>external-dns</code>, <code>kube-addons</code>, etc.</p>"},{"location":"docs/tutorials/azure/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service\n        - --source=ingress\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=azure\n        - --azure-resource-group=MyDnsResourceGroup # (optional) use the DNS zones from the tutorial's resource group\n        volumeMounts:\n        - name: azure-config-file\n          mountPath: /etc/kubernetes\n          readOnly: true\n      volumes:\n      - name: azure-config-file\n        secret:\n          secretName: azure-config-file\n</code></pre>"},{"location":"docs/tutorials/azure/#manifest-for-clusters-with-rbac-enabled-cluster-access","title":"Manifest (for clusters with RBAC enabled, cluster access)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"services\",\"endpoints\",\"pods\", \"nodes\"]\n    verbs: [\"get\",\"watch\",\"list\"]\n  - apiGroups: [\"extensions\",\"networking.k8s.io\"]\n    resources: [\"ingresses\"]\n    verbs: [\"get\",\"watch\",\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n  - kind: ServiceAccount\n    name: external-dns\n    namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n        - name: external-dns\n          image: registry.k8s.io/external-dns/external-dns:v0.15.1\n          args:\n            - --source=service\n            - --source=ingress\n            - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n            - --provider=azure\n            - --azure-resource-group=MyDnsResourceGroup # (optional) use the DNS zones from the tutorial's resource group\n            - --txt-prefix=externaldns-\n          volumeMounts:\n            - name: azure-config-file\n              mountPath: /etc/kubernetes\n              readOnly: true\n      volumes:\n        - name: azure-config-file\n          secret:\n            secretName: azure-config-file\n</code></pre>"},{"location":"docs/tutorials/azure/#manifest-for-clusters-with-rbac-enabled-namespace-access","title":"Manifest (for clusters with RBAC enabled, namespace access)","text":"<p>This configuration is the same as above, except it only requires privileges for the current namespace, not for the whole cluster. However, access to nodes requires cluster access, so when using this manifest, services with type <code>NodePort</code> will be skipped!</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: external-dns\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"services\",\"endpoints\",\"pods\"]\n    verbs: [\"get\",\"watch\",\"list\"]\n  - apiGroups: [\"extensions\",\"networking.k8s.io\"]\n    resources: [\"ingresses\"]\n    verbs: [\"get\",\"watch\",\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: external-dns\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: external-dns\nsubjects:\n  - kind: ServiceAccount\n    name: external-dns\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n        - name: external-dns\n          image: registry.k8s.io/external-dns/external-dns:v0.15.1\n          args:\n            - --source=service\n            - --source=ingress\n            - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n            - --provider=azure\n            - --azure-resource-group=MyDnsResourceGroup # (optional) use the DNS zones from the tutorial's resource group\n          volumeMounts:\n            - name: azure-config-file\n              mountPath: /etc/kubernetes\n              readOnly: true\n      volumes:\n        - name: azure-config-file\n          secret:\n            secretName: azure-config-file\n</code></pre> <p>Create the deployment for ExternalDNS:</p> <pre><code>kubectl create --namespace \"default\" --filename externaldns.yaml\n</code></pre>"},{"location":"docs/tutorials/azure/#ingress-option-expose-an-nginx-service-with-an-ingress","title":"Ingress Option: Expose an nginx service with an ingress","text":"<p>Create a file called <code>nginx.yaml</code> with the following contents:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - image: nginx\n          name: nginx\n          ports:\n          - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-svc\nspec:\n  ports:\n    - port: 80\n      protocol: TCP\n      targetPort: 80\n  selector:\n    app: nginx\n  type: ClusterIP\n\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nginx\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: server.example.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: nginx-svc\n                port:\n                  number: 80\n</code></pre> <p>When you use ExternalDNS with Ingress resources, it automatically creates DNS records based on the hostnames listed in those Ingress objects. Those hostnames must match the filters that you defined (if any):</p> <ul> <li>By default, <code>--domain-filter</code> filters Azure DNS zone.</li> <li>If you use <code>--domain-filter</code> together with <code>--zone-name-filter</code>, the behavior changes: <code>--domain-filter</code> then filters Ingress domains, not the Azure DNS zone name.</li> </ul> <p>When those hostnames are removed or renamed the corresponding DNS records are also altered.</p> <p>Create the deployment, service and ingress object:</p> <pre><code>kubectl create --namespace \"default\" --filename nginx.yaml\n</code></pre> <p>Since your external IP would have already been assigned to the nginx-ingress service, the DNS records pointing to the IP of the nginx-ingress service should be created within a minute.</p>"},{"location":"docs/tutorials/azure/#azure-load-balancer-option-expose-an-nginx-service-with-a-load-balancer","title":"Azure Load Balancer option: Expose an nginx service with a load balancer","text":"<p>Create a file called <code>nginx.yaml</code> with the following contents:</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - image: nginx\n          name: nginx\n          ports:\n          - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-svc\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: server.example.com\nspec:\n  ports:\n    - port: 80\n      protocol: TCP\n      targetPort: 80\n  selector:\n    app: nginx\n  type: LoadBalancer\n</code></pre> <p>The annotation <code>external-dns.alpha.kubernetes.io/hostname</code> is used to specify the DNS name that should be created for the service. The annotation value is a comma separated list of host names.</p>"},{"location":"docs/tutorials/azure/#verifying-azure-dns-records","title":"Verifying Azure DNS records","text":"<p>Run the following command to view the A records for your Azure DNS zone:</p> <pre><code>az network dns record-set a list --resource-group \"MyDnsResourceGroup\" --zone-name example.com\n</code></pre> <p>Substitute the zone for the one created above if a different domain was used.</p> <p>This should show the external IP address of the service as the A record for your domain (\u2018@\u2019 indicates the record is for the zone itself).</p>"},{"location":"docs/tutorials/azure/#delete-azure-resource-group","title":"Delete Azure Resource Group","text":"<p>Now that we have verified that ExternalDNS will automatically manage Azure DNS records, we can delete the tutorial\u2019s resource group:</p> <pre><code>az group delete --name \"MyDnsResourceGroup\"\n</code></pre>"},{"location":"docs/tutorials/azure/#more-tutorials","title":"More tutorials","text":"<p>A video explanation is available here: https://www.youtube.com/watch?v=VSn6DPKIhM8&amp;list=PLpbcUe4chE79sB7Jg7B4z3HytqUUEwcNE</p> <p></p>"},{"location":"docs/tutorials/civo/","title":"Civo DNS","text":"<p>This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using Civo DNS Manager.</p> <p>Make sure to use &gt;0.13.5 version of ExternalDNS for this tutorial.</p>"},{"location":"docs/tutorials/civo/#managing-dns-with-civo","title":"Managing DNS with Civo","text":"<p>If you want to learn about how to use Civo DNS Manager read the following tutorials:</p> <p>An Introduction to Managing DNS</p>"},{"location":"docs/tutorials/civo/#get-civo-token","title":"Get Civo Token","text":"<p>Copy the token in the settings for your account The environment variable <code>CIVO_TOKEN</code> will be needed to run ExternalDNS with Civo.</p>"},{"location":"docs/tutorials/civo/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Connect your <code>kubectl</code> client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.</p>"},{"location":"docs/tutorials/civo/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=civo\n        env:\n        - name: CIVO_TOKEN\n          value: \"YOUR_CIVO_API_TOKEN\"\n</code></pre>"},{"location":"docs/tutorials/civo/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=civo\n        env:\n        - name: CIVO_TOKEN\n          value: \"YOUR_CIVO_API_TOKEN\"\n</code></pre>"},{"location":"docs/tutorials/civo/#deploying-an-nginx-service","title":"Deploying an Nginx Service","text":"<p>Create a service file called \u2018nginx.yaml\u2019 with the following contents:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: my-app.example.com\nspec:\n  selector:\n    app: nginx\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre> <p>Note the annotation on the service; use the same hostname as the Civo DNS zone created above.</p> <p>ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records.</p> <p>Create the deployment and service:</p> <pre><code>kubectl create -f nginx.yaml\n</code></pre> <p>Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service.</p> <p>Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Civo DNS records.</p>"},{"location":"docs/tutorials/civo/#verifying-civo-dns-records","title":"Verifying Civo DNS records","text":"<p>Check your Civo UI to view the records for your Civo DNS zone.</p> <p>Click on the zone for the one created above if a different domain was used.</p> <p>This should show the external IP address of the service as the A record for your domain.</p>"},{"location":"docs/tutorials/civo/#cleanup","title":"Cleanup","text":"<p>Now that we have verified that ExternalDNS will automatically manage Civo DNS records, we can delete the tutorial\u2019s example:</p> <pre><code>kubectl delete service -f nginx.yaml\nkubectl delete service -f externaldns.yaml\n</code></pre>"},{"location":"docs/tutorials/cloudflare/","title":"Cloudflare DNS","text":"<p>This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using Cloudflare DNS.</p> <p>Make sure to use &gt;=0.4.2 version of ExternalDNS for this tutorial.</p>"},{"location":"docs/tutorials/cloudflare/#creating-a-cloudflare-dns-zone","title":"Creating a Cloudflare DNS zone","text":"<p>We highly recommend to read this tutorial if you haven\u2019t used Cloudflare before:</p> <p>Create a Cloudflare account and add a website</p>"},{"location":"docs/tutorials/cloudflare/#creating-cloudflare-credentials","title":"Creating Cloudflare Credentials","text":"<p>Snippet from Cloudflare - Getting Started:</p> <p>Cloudflare\u2019s API exposes the entire Cloudflare infrastructure via a standardized programmatic interface. Using Cloudflare\u2019s API, you can do just about anything you can do on cloudflare.com via the customer dashboard. The Cloudflare API is a RESTful API based on HTTPS requests and JSON responses. If you are registered with Cloudflare, you can obtain your API key from the bottom of the \u201cMy Account\u201d page, found here: Go to My account.</p> <p>API Token will be preferred for authentication if <code>CF_API_TOKEN</code> environment variable is set. Otherwise <code>CF_API_KEY</code> and <code>CF_API_EMAIL</code> should be set to run ExternalDNS with Cloudflare. You may provide the Cloudflare API token through a file by setting the <code>CF_API_TOKEN=\"file:/path/to/token\"</code>.</p> <p>Note. The <code>CF_API_KEY</code> and <code>CF_API_EMAIL</code> should not be present, if you are using a <code>CF_API_TOKEN</code>.</p> <p>When using API Token authentication, the token should be granted Zone <code>Read</code>, DNS <code>Edit</code> privileges, and access to <code>All zones</code>.</p> <p>If you would like to further restrict the API permissions to a specific zone (or zones), you also need to use the <code>--zone-id-filter</code> so that the underlying API requests only access the zones that you explicitly specify, as opposed to accessing all zones.</p>"},{"location":"docs/tutorials/cloudflare/#throttling","title":"Throttling","text":"<p>Cloudflare API has a global rate limit of 1,200 requests per five minutes. Running several fast polling ExternalDNS instances in a given account can easily hit that limit. The AWS Provider docs has some recommendations that can be followed here too, but in particular, consider passing <code>--cloudflare-dns-records-per-page</code> with a high value (maximum is 5,000).</p>"},{"location":"docs/tutorials/cloudflare/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Connect your <code>kubectl</code> client to the cluster you want to test ExternalDNS with.</p> <p>Begin by creating a Kubernetes secret to securely store your CloudFlare API key. This key will enable ExternalDNS to authenticate with CloudFlare:</p> <pre><code>kubectl create secret generic cloudflare-api-key --from-literal=apiKey=YOUR_API_KEY --from-literal=email=YOUR_CLOUDFLARE_EMAIL\n</code></pre> <p>And for API Token it should look like :</p> <pre><code>kubectl create secret generic cloudflare-api-key --from-literal=apiKey=YOUR_API_TOKEN\n</code></pre> <p>Ensure to replace YOUR_API_KEY with your actual CloudFlare API key and YOUR_CLOUDFLARE_EMAIL with the email associated with your CloudFlare account.</p> <p>Then apply one of the following manifests file to deploy ExternalDNS.</p>"},{"location":"docs/tutorials/cloudflare/#using-helm","title":"Using Helm","text":"<p>Create a values.yaml file to configure ExternalDNS to use CloudFlare as the DNS provider. This file should include the necessary environment variables:</p> <pre><code>provider:\n  name: cloudflare\nenv:\n  - name: CF_API_KEY\n    valueFrom:\n      secretKeyRef:\n        name: cloudflare-api-key\n        key: apiKey\n  - name: CF_API_EMAIL\n    valueFrom:\n      secretKeyRef:\n        name: cloudflare-api-key\n        key: email\n</code></pre> <p>Use this in your values.yaml, if you are using API Token:</p> <pre><code>provider:\n  name: cloudflare\nenv:\n  - name: CF_API_TOKEN\n    valueFrom:\n      secretKeyRef:\n        name: cloudflare-api-key\n        key: apiKey\n</code></pre> <p>Finally, install the ExternalDNS chart with Helm using the configuration specified in your values.yaml file:</p> <pre><code>helm repo add external-dns https://kubernetes-sigs.github.io/external-dns/\n</code></pre> <pre><code>helm repo update\n</code></pre> <pre><code>helm upgrade --install external-dns external-dns/external-dns --values values.yaml\n</code></pre>"},{"location":"docs/tutorials/cloudflare/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n        - name: external-dns\n          image: registry.k8s.io/external-dns/external-dns:v0.15.1\n          args:\n            - --source=service # ingress is also possible\n            - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n            - --zone-id-filter=023e105f4ecef8ad9ca31a8372d0c353 # (optional) limit to a specific zone.\n            - --provider=cloudflare\n            - --cloudflare-proxied # (optional) enable the proxy feature of Cloudflare (DDOS protection, CDN...)\n            - --cloudflare-dns-records-per-page=5000 # (optional) configure how many DNS records to fetch per request\n            - --cloudflare-region-key=\"eu\" # (optional) configure which region can decrypt HTTPS requests\n         env:\n            - name: CF_API_KEY\n              valueFrom:\n                secretKeyRef:\n                  name: cloudflare-api-key\n                  key: apiKey\n            - name: CF_API_EMAIL\n              valueFrom:\n                secretKeyRef:\n                  name: cloudflare-api-key\n                  key: email\n</code></pre>"},{"location":"docs/tutorials/cloudflare/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"services\",\"endpoints\",\"pods\"]\n    verbs: [\"get\",\"watch\",\"list\"]\n  - apiGroups: [\"extensions\",\"networking.k8s.io\"]\n    resources: [\"ingresses\"]\n    verbs: [\"get\",\"watch\",\"list\"]\n  - apiGroups: [\"\"]\n    resources: [\"nodes\"]\n    verbs: [\"list\", \"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n        - name: external-dns\n          image: registry.k8s.io/external-dns/external-dns:v0.15.1\n          args:\n            - --source=service # ingress is also possible\n            - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n            - --zone-id-filter=023e105f4ecef8ad9ca31a8372d0c353 # (optional) limit to a specific zone.\n            - --provider=cloudflare\n            - --cloudflare-proxied # (optional) enable the proxy feature of Cloudflare (DDOS protection, CDN...)\n            - --cloudflare-dns-records-per-page=5000 # (optional) configure how many DNS records to fetch per request\n            - --cloudflare-region-key=\"eu\" # (optional) configure which region can decrypt HTTPS requests\n          env:\n            - name: CF_API_KEY\n              valueFrom:\n                secretKeyRef:\n                  name: cloudflare-api-key\n                  key: apiKey\n            - name: CF_API_EMAIL\n              valueFrom:\n                secretKeyRef:\n                  name: cloudflare-api-key\n                  key: email\n</code></pre>"},{"location":"docs/tutorials/cloudflare/#deploying-an-nginx-service","title":"Deploying an Nginx Service","text":"<p>Create a service file called \u2018nginx.yaml\u2019 with the following contents:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: example.com\n    external-dns.alpha.kubernetes.io/ttl: \"120\" #optional\nspec:\n  selector:\n    app: nginx\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre> <p>Note the annotation on the service; use the same hostname as the Cloudflare DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019).</p> <p>By setting the TTL annotation on the service, you have to pass a valid TTL, which must be 120 or above. This annotation is optional, if you won\u2019t set it, it will be 1 (automatic) which is 300. For Cloudflare proxied entries, set the TTL annotation to 1 (automatic), or do not set it.</p> <p>ExternalDNS uses this annotation to determine what services should be registered with DNS.  Removing the annotation will cause ExternalDNS to remove the corresponding DNS records.</p> <p>Create the deployment and service:</p> <pre><code>kubectl create -f nginx.yaml\n</code></pre> <p>Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service.</p> <p>Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Cloudflare DNS records.</p>"},{"location":"docs/tutorials/cloudflare/#verifying-cloudflare-dns-records","title":"Verifying Cloudflare DNS records","text":"<p>Check your Cloudflare dashboard to view the records for your Cloudflare DNS zone.</p> <p>Substitute the zone for the one created above if a different domain was used.</p> <p>This should show the external IP address of the service as the A record for your domain.</p>"},{"location":"docs/tutorials/cloudflare/#cleanup","title":"Cleanup","text":"<p>Now that we have verified that ExternalDNS will automatically manage Cloudflare DNS records, we can delete the tutorial\u2019s example:</p> <pre><code>kubectl delete -f nginx.yaml\nkubectl delete -f externaldns.yaml\n</code></pre>"},{"location":"docs/tutorials/cloudflare/#setting-cloudflare-proxied-on-a-per-ingress-basis","title":"Setting cloudflare-proxied on a per-ingress basis","text":"<p>Using the <code>external-dns.alpha.kubernetes.io/cloudflare-proxied: \"true\"</code> annotation on your ingress, you can specify if the proxy feature of Cloudflare should be enabled for that record. This setting will override the global <code>--cloudflare-proxied</code> setting.</p>"},{"location":"docs/tutorials/cloudflare/#setting-cloudflare-region-key-to-configure-regional-services","title":"Setting cloudflare-region-key to configure regional services","text":"<p>Using the <code>external-dns.alpha.kubernetes.io/cloudflare-region-key</code> annotation on your ingress, you can restrict which data centers can decrypt and serve HTTPS traffic. A list of available options can be seen here. Currently, requires SuperAdmin or Admin role.</p> <p>If not set the value will default to <code>global</code>.</p>"},{"location":"docs/tutorials/cloudflare/#setting-cloudflare-custom-hostname","title":"Setting cloudflare-custom-hostname","text":"<p>You can automatically configure custom hostnames for A/CNAME DNS records (as custom origins) using the <code>--cloudflare-custom-hostnames</code> flag and the <code>external-dns.alpha.kubernetes.io/cloudflare-custom-hostname: \"&lt;custom hostname&gt;\"</code> annotation.</p> <p>See Cloudflare for Platforms for more information on custom hostnames.</p> <p>This feature is disabled by default and supports the <code>--cloudflare-custom-hostnames-min-tls-version</code> and <code>--cloudflare-custom-hostnames-certificate-authority</code> flags.</p> <p>The custom hostname DNS must resolve to the Cloudflare DNS record (<code>external-dns.alpha.kubernetes.io/hostname</code>) for automatic certificate validation via the HTTP method. It\u2019s important to note that the TXT method does not allow automatic validation and is not supported.</p> <p>Requires Cloudflare for SaaS product and \u201cSSL and Certificates\u201d API permission.</p> <p>Due to a limitation within the cloudflare-go v0 API, the custom hostname page size is fixed at 50.</p>"},{"location":"docs/tutorials/cloudflare/#using-crd-source-to-manage-dns-records-in-cloudflare","title":"Using CRD source to manage DNS records in Cloudflare","text":"<p>Please refer to the CRD source documentation for more information.</p>"},{"location":"docs/tutorials/contour/","title":"Contour HTTPProxy","text":"<p>This tutorial describes how to configure External DNS to use the Contour <code>HTTPProxy</code> source. Using the <code>HTTPProxy</code> resource with External DNS requires Contour version 1.5 or greater.</p>"},{"location":"docs/tutorials/contour/#example-manifests-for-external-dns","title":"Example manifests for External DNS","text":""},{"location":"docs/tutorials/contour/#without-rbac","title":"Without RBAC","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service\n        - --source=ingress\n        - --source=contour-httpproxy\n        - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n        - --provider=aws\n        - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization\n        - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both)\n        - --registry=txt\n        - --txt-owner-id=my-identifier\n</code></pre>"},{"location":"docs/tutorials/contour/#with-rbac","title":"With RBAC","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\"]\n- apiGroups: [\"projectcontour.io\"]\n  resources: [\"httpproxies\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service\n        - --source=ingress\n        - --source=contour-httpproxy\n        - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n        - --provider=aws\n        - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization\n        - --aws-zone-type=public # only look at public hosted zones (valid values are public, private or no value for both)\n        - --registry=txt\n        - --txt-owner-id=my-identifier\n</code></pre>"},{"location":"docs/tutorials/contour/#verify-external-dns-works","title":"Verify External DNS works","text":"<p>The following instructions are based on the Contour example workload.</p>"},{"location":"docs/tutorials/contour/#install-a-sample-service","title":"Install a sample service","text":"<pre><code>$ kubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: kuard\n  name: kuard\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: kuard\n  template:\n    metadata:\n      labels:\n        app: kuard\n    spec:\n      containers:\n      - image: gcr.io/kuar-demo/kuard-amd64:1\n        name: kuard\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: kuard\n  name: kuard\nspec:\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: kuard\n  sessionAffinity: None\n  type: ClusterIP\nEOF\n</code></pre> <p>Then create an <code>HTTPProxy</code>:</p> <pre><code>$ kubectl apply -f - &lt;&lt;EOF\napiVersion: projectcontour.io/v1\nkind: HTTPProxy\nmetadata:\n  labels:\n    app: kuard\n  name: kuard\n  namespace: default\nspec:\n  virtualhost:\n    fqdn: kuard.example.com\n  routes:\n    - conditions:\n      - prefix: /\n      services:\n        - name: kuard\n          port: 80\nEOF\n</code></pre>"},{"location":"docs/tutorials/contour/#access-the-sample-service-using-curl","title":"Access the sample service using <code>curl</code>","text":"<pre><code>$ curl -i http://kuard.example.com/healthy\nHTTP/1.1 200 OK\nContent-Type: text/plain\nDate: Thu, 27 Jun 2019 19:42:26 GMT\nContent-Length: 2\n\nok\n</code></pre>"},{"location":"docs/tutorials/coredns/","title":"CoreDNS with minikube","text":"<p> This tutorial is out of date.</p> <p> PRs to update it are welcome !</p> <p>This tutorial describes how to setup ExternalDNS for usage within a minikube cluster that makes use of CoreDNS and nginx ingress controller.</p> <p>You need to:</p> <ul> <li>install CoreDNS with etcd enabled</li> <li>install external-dns with coredns as a provider</li> <li>enable ingress controller for the minikube cluster</li> </ul>"},{"location":"docs/tutorials/coredns/#creating-a-cluster","title":"Creating a cluster","text":"<pre><code>minikube start\n</code></pre>"},{"location":"docs/tutorials/coredns/#installing-coredns-with-etcd-enabled","title":"Installing CoreDNS with etcd enabled","text":"<p>Helm chart is used to install etcd and CoreDNS.</p>"},{"location":"docs/tutorials/coredns/#initializing-helm-chart","title":"Initializing helm chart","text":"<pre><code>helm init\n</code></pre>"},{"location":"docs/tutorials/coredns/#installing-etcd","title":"Installing etcd","text":"<p>etcd operator is used to manage etcd clusters.</p> <pre><code>helm install stable/etcd-operator --name my-etcd-op\n</code></pre> <p>etcd cluster is installed with example yaml from etcd operator website.</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/coreos/etcd-operator/HEAD/example/example-etcd-cluster.yaml\n</code></pre>"},{"location":"docs/tutorials/coredns/#installing-coredns","title":"Installing CoreDNS","text":"<p>In order to make CoreDNS work with etcd backend, values.yaml of the chart should be changed with corresponding configurations.</p> <pre><code>wget https://raw.githubusercontent.com/helm/charts/HEAD/stable/coredns/values.yaml\n</code></pre> <p>You need to edit/patch the file with below diff</p> <pre><code>diff --git a/values.yaml b/values.yaml\nindex 964e72b..e2fa934 100644\n--- a/values.yaml\n+++ b/values.yaml\n@@ -27,12 +27,12 @@ service:\n\n rbac:\n   # If true, create &amp; use RBAC resources\n-  create: false\n+  create: true\n   # Ignored if rbac.create is true\n   serviceAccountName: default\n\n # isClusterService specifies whether chart should be deployed as cluster-service or normal k8s app.\n-isClusterService: true\n+isClusterService: false\n\n servers:\n - zones:\n@@ -51,6 +51,12 @@ servers:\n     parameters: 0.0.0.0:9153\n   - name: proxy\n     parameters: . /etc/resolv.conf\n+  - name: etcd\n+    parameters: example.org\n+    configBlock: |-\n+      stubzones\n+      path /skydns\n+      endpoint http://10.105.68.165:2379\n\n # Complete example with all the options:\n # - zones:                 # the `zones` block can be left out entirely, defaults to \".\"\n</code></pre> <p>Note:</p> <ul> <li>IP address of etcd\u2019s endpoint should be get from etcd client service. It should be \u201cexample-etcd-cluster-client\u201d in this example. This IP address is used through this document for etcd endpoint configuration.</li> </ul> <pre><code>$ kubectl get svc example-etcd-cluster-client\nNAME                          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE\nexample-etcd-cluster-client   ClusterIP   10.105.68.165   &lt;none&gt;        2379/TCP   16m\n</code></pre> <ul> <li>Parameters should configure your own domain. \u201cexample.org\u201d is used in this example.</li> </ul> <p>After configuration done in values.yaml, you can install coredns chart.</p> <pre><code>helm install --name my-coredns --values values.yaml stable/coredns\n</code></pre>"},{"location":"docs/tutorials/coredns/#installing-externaldns","title":"Installing ExternalDNS","text":""},{"location":"docs/tutorials/coredns/#install-external-externaldns","title":"Install external ExternalDNS","text":"<p>ETCD_URLS is configured to etcd client service address. Optionally, you can configure ETCD_USERNAME and ETCD_PASSWORD for authenticating to etcd. It is also possible to connect to the etcd cluster via HTTPS using the following environment variables: ETCD_CA_FILE, ETCD_CERT_FILE, ETCD_KEY_FILE, ETCD_TLS_SERVER_NAME, ETCD_TLS_INSECURE.</p>"},{"location":"docs/tutorials/coredns/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\n  namespace: kube-system\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=ingress\n        - --provider=coredns\n        - --log-level=debug # debug only\n        env:\n        - name: ETCD_URLS\n          value: http://10.105.68.165:2379\n</code></pre>"},{"location":"docs/tutorials/coredns/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: kube-system\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n  namespace: kube-system\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\n  namespace: kube-system\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=ingress\n        - --provider=coredns\n        - --log-level=debug # debug only\n        env:\n        - name: ETCD_URLS\n          value: http://10.105.68.165:2379\n</code></pre>"},{"location":"docs/tutorials/coredns/#enable-the-ingress-controller","title":"Enable the ingress controller","text":"<p>You can use the ingress controller in minikube cluster. It needs to enable ingress addon in the cluster.</p> <pre><code>minikube addons enable ingress\n</code></pre>"},{"location":"docs/tutorials/coredns/#testing-ingress-example","title":"Testing ingress example","text":"<pre><code>$ cat ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nginx\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: nginx.example.org\n    http:\n      paths:\n      - backend:\n          serviceName: nginx\n          servicePort: 80\n\n$ kubectl apply -f ingress.yaml\ningress.extensions \"nginx\" created\n</code></pre> <p>Wait a moment until DNS has the ingress IP. The DNS service IP is from CoreDNS service. It is \u201cmy-coredns-coredns\u201d in this example.</p> <pre><code>$ kubectl get svc my-coredns-coredns\nNAME                 TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE\nmy-coredns-coredns   ClusterIP   10.100.4.143   &lt;none&gt;        53/UDP    12m\n\n$ kubectl get ingress\nNAME      HOSTS               ADDRESS     PORTS     AGE\nnginx     nginx.example.org   10.0.2.15   80        2m\n\n$ kubectl run -it --rm --restart=Never --image=infoblox/dnstools:latest dnstools\nIf you don't see a command prompt, try pressing enter.\ndnstools# dig @10.100.4.143 nginx.example.org +short\n10.0.2.15\ndnstools#\n</code></pre>"},{"location":"docs/tutorials/designate/","title":"Designate DNS from OpenStack","text":"<p>This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using OpenStack Designate DNS.</p>"},{"location":"docs/tutorials/designate/#authenticating-with-openstack","title":"Authenticating with OpenStack","text":"<p>We are going to use OpenStack CLI - <code>openstack</code> utility, which is an umbrella application for most of OpenStack clients including <code>designate</code>.</p> <p>All OpenStack CLIs require authentication parameters to be provided. These parameters include:</p> <ul> <li>URL of the OpenStack identity service (<code>keystone</code>) which is responsible for user authentication and also served as a registry for other   OpenStack services. Designate endpoints must be registered in <code>keystone</code> in order to ExternalDNS and OpenStack CLI be able to find them.</li> <li>OpenStack region name</li> <li>User login name.</li> <li>User project (tenant) name.</li> <li>User domain (only when using keystone API v3)</li> </ul> <p>Although these parameters can be passed explicitly through the CLI flags, traditionally it is done by sourcing <code>openrc</code> file (<code>source ~/openrc</code>) that is a shell snippet that sets environment variables that all OpenStack CLI understand by convention.</p> <p>Recent versions of OpenStack Dashboard have a nice UI to download <code>openrc</code> file for both v2 and v3 auth protocols. Both protocols can be used with ExternalDNS. v3 is generally preferred over v2, but might not be available in some OpenStack installations.</p>"},{"location":"docs/tutorials/designate/#installing-openstack-designate","title":"Installing OpenStack Designate","text":"<p>Please refer to the Designate deployment tutorial for instructions on how to install and test Designate with BIND backend. You will be required to have admin rights in existing OpenStack installation to do this. One convenient way to get yourself an OpenStack installation to play with is to use DevStack.</p>"},{"location":"docs/tutorials/designate/#creating-dns-zones","title":"Creating DNS zones","text":"<p>All domain names that are ExternalDNS is going to create must belong to one of DNS zones created in advance. Here is an example of how to create <code>example.com</code> DNS zone:</p> <pre><code>openstack zone create --email dnsmaster@example.com example.com.\n</code></pre> <p>It is important to manually create all the zones that are going to be used for kubernetes entities (ExternalDNS sources) before starting ExternalDNS.</p>"},{"location":"docs/tutorials/designate/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Create a deployment file called <code>externaldns.yaml</code> with the following contents:</p>"},{"location":"docs/tutorials/designate/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=designate\n        env: # values from openrc file\n        - name: OS_AUTH_URL\n          value: https://controller/identity/v3\n        - name: OS_REGION_NAME\n          value: RegionOne\n        - name: OS_USERNAME\n          value: admin\n        - name: OS_PASSWORD\n          value: p@ssw0rd\n        - name: OS_PROJECT_NAME\n          value: demo\n        - name: OS_USER_DOMAIN_NAME\n          value: Default\n</code></pre>"},{"location":"docs/tutorials/designate/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"watch\",\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  selector:\n    matchLabels:\n      app: external-dns\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=designate\n        env: # values from openrc file\n        - name: OS_AUTH_URL\n          value: https://controller/identity/v3\n        - name: OS_REGION_NAME\n          value: RegionOne\n        - name: OS_USERNAME\n          value: admin\n        - name: OS_PASSWORD\n          value: p@ssw0rd\n        - name: OS_PROJECT_NAME\n          value: demo\n        - name: OS_USER_DOMAIN_NAME\n          value: Default\n</code></pre> <p>Create the deployment for ExternalDNS:</p> <pre><code>kubectl create -f externaldns.yaml\n</code></pre>"},{"location":"docs/tutorials/designate/#optional-trust-self-sign-certificates","title":"Optional: Trust self-sign certificates","text":"<p>If your OpenStack-Installation is configured with a self-sign certificate, you could extend the <code>pod.spec</code> with following secret-mount:</p> <pre><code>        volumeMounts:\n        - mountPath: /etc/ssl/certs/\n          name: cacerts\n      volumes:\n      - name: cacerts\n        secret:\n          defaultMode: 420\n          secretName: self-sign-certs\n</code></pre> <p>content of the secret <code>self-sign-certs</code> must be the certificate/chain in PEM format.</p>"},{"location":"docs/tutorials/designate/#deploying-an-nginx-service","title":"Deploying an Nginx Service","text":"<p>Create a service file called \u2018nginx.yaml\u2019 with the following contents:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: my-app.example.com\nspec:\n  selector:\n    app: nginx\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre> <p>Note the annotation on the service; use the same hostname as the DNS zone created above.</p> <p>ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records.</p> <p>Create the deployment and service:</p> <pre><code>kubectl create -f nginx.yaml\n</code></pre> <p>Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and notify Designate, which in turn synchronize DNS records with underlying DNS server backend.</p>"},{"location":"docs/tutorials/designate/#verifying-dns-records","title":"Verifying DNS records","text":"<p>To verify that DNS record was indeed created, you can use the following command:</p> <pre><code>openstack recordset list example.com.\n</code></pre> <p>There should be a record for my-app.example.com having <code>ACTIVE</code> status. And of course, the ultimate method to verify is to issue a DNS query:</p> <pre><code>dig my-app.example.com @controller\n</code></pre>"},{"location":"docs/tutorials/designate/#cleanup","title":"Cleanup","text":"<p>Now that we have verified that ExternalDNS created all DNS records, we can delete the tutorial\u2019s example:</p> <pre><code>kubectl delete service -f nginx.yaml\nkubectl delete service -f externaldns.yaml\n</code></pre>"},{"location":"docs/tutorials/digitalocean/","title":"DigitalOcean DNS","text":"<p>This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using DigitalOcean DNS.</p> <p>Make sure to use &gt;=0.4.2 version of ExternalDNS for this tutorial.</p>"},{"location":"docs/tutorials/digitalocean/#creating-a-digitalocean-dns-zone","title":"Creating a DigitalOcean DNS zone","text":"<p>If you want to learn about how to use DigitalOcean\u2019s DNS service read the following tutorial series:</p> <p>An Introduction to Managing DNS, and specifically How To Set Up a Host Name with DigitalOcean DNS</p> <p>Create a new DNS zone where you want to create your records in. Let\u2019s use <code>example.com</code> as an example here.</p>"},{"location":"docs/tutorials/digitalocean/#creating-digitalocean-credentials","title":"Creating DigitalOcean Credentials","text":"<p>Generate a new personal token by going to the API settings or follow How To Use the DigitalOcean API v2 if you need more information. Give the token a name and choose read and write access. The token needs to be passed to ExternalDNS so make a note of it for later use.</p> <p>The environment variable <code>DO_TOKEN</code> will be needed to run ExternalDNS with DigitalOcean.</p>"},{"location":"docs/tutorials/digitalocean/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Connect your <code>kubectl</code> client to the cluster you want to test ExternalDNS with.</p> <p>Begin by creating a Kubernetes secret to securely store your DigitalOcean API key. This key will enable ExternalDNS to authenticate with DigitalOcean:</p> <pre><code>kubectl create secret generic DO_TOKEN --from-literal=DO_TOKEN=YOUR_DIGITALOCEAN_API_KEY\n</code></pre> <p>Ensure to replace YOUR_DIGITALOCEAN_API_KEY with your actual DigitalOcean API key.</p> <p>Then apply one of the following manifests file to deploy ExternalDNS.</p>"},{"location":"docs/tutorials/digitalocean/#using-helm","title":"Using Helm","text":"<p>Create a values.yaml file to configure ExternalDNS to use DigitalOcean as the DNS provider. This file should include the necessary environment variables:</p> <pre><code>provider:\n  name: digitalocean\nenv:\n  - name: DO_TOKEN\n    valueFrom:\n      secretKeyRef:\n        name: DO_TOKEN\n        key: DO_TOKEN\n</code></pre>"},{"location":"docs/tutorials/digitalocean/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: external-dns\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=digitalocean\n        env:\n        - name: DO_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: DO_TOKEN\n              key: DO_TOKEN\n</code></pre>"},{"location":"docs/tutorials/digitalocean/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: external-dns\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=digitalocean\n        env:\n        - name: DO_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: DO_TOKEN\n              key: DO_TOKEN\n</code></pre>"},{"location":"docs/tutorials/digitalocean/#deploying-an-nginx-service","title":"Deploying an Nginx Service","text":"<p>Create a service file called \u2018nginx.yaml\u2019 with the following contents:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: my-app.example.com\nspec:\n  selector:\n    app: nginx\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre> <p>Note the annotation on the service; use the same hostname as the DigitalOcean DNS zone created above.</p> <p>ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records.</p> <p>Create the deployment and service:</p> <pre><code>kubectl create -f nginx.yaml\n</code></pre> <p>Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service.</p> <p>Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the DigitalOcean DNS records.</p>"},{"location":"docs/tutorials/digitalocean/#verifying-digitalocean-dns-records","title":"Verifying DigitalOcean DNS records","text":"<p>Check your DigitalOcean UI to view the records for your DigitalOcean DNS zone.</p> <p>Click on the zone for the one created above if a different domain was used.</p> <p>This should show the external IP address of the service as the A record for your domain.</p>"},{"location":"docs/tutorials/digitalocean/#cleanup","title":"Cleanup","text":"<p>Now that we have verified that ExternalDNS will automatically manage DigitalOcean DNS records, we can delete the tutorial\u2019s example:</p> <pre><code>kubectl delete service -f nginx.yaml\nkubectl delete service -f externaldns.yaml\n</code></pre>"},{"location":"docs/tutorials/digitalocean/#advanced-usage","title":"Advanced Usage","text":""},{"location":"docs/tutorials/digitalocean/#api-page-size","title":"API Page Size","text":"<p>If you have a large number of domains and/or records within a domain, you may encounter API rate limiting because of the number of API calls that external-dns must make to the DigitalOcean API to retrieve the current DNS configuration during every reconciliation loop. If this is the case, use the <code>--digitalocean-api-page-size</code> option to increase the size of the pages used when querying the DigitalOcean API. (Note: external-dns uses a default of 50.)</p>"},{"location":"docs/tutorials/dnsimple/","title":"DNSimple","text":"<p>This tutorial describes how to setup ExternalDNS for usage with DNSimple.</p> <p>Make sure to use &gt;=0.4.6 version of ExternalDNS for this tutorial.</p>"},{"location":"docs/tutorials/dnsimple/#create-a-dnsimple-api-access-token","title":"Create a DNSimple API Access Token","text":"<p>A DNSimple API access token can be acquired by following the provided documentation from DNSimple</p> <p>The environment variable <code>DNSIMPLE_OAUTH</code> must be set to the generated API token to run ExternalDNS with DNSimple.</p> <p>When the generated DNSimple API access token is a User token, as opposed to an Account token, the following environment variables must also be set:</p> <ul> <li><code>DNSIMPLE_ACCOUNT_ID</code>: Set this to the account ID which the domains to be managed by ExternalDNS belong to (eg. <code>1001234</code>).</li> <li><code>DNSIMPLE_ZONES</code>: Set this to a comma separated list of DNS zones to be managed by ExternalDNS (eg. <code>mydomain.com,example.com</code>).</li> </ul>"},{"location":"docs/tutorials/dnsimple/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Connect your <code>kubectl</code> client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.</p>"},{"location":"docs/tutorials/dnsimple/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone you create in DNSimple.\n        - --provider=dnsimple\n        - --registry=txt\n        env:\n        - name: DNSIMPLE_OAUTH\n          value: \"YOUR_DNSIMPLE_API_KEY\"\n        - name: DNSIMPLE_ACCOUNT_ID\n          value: \"SET THIS IF USING A DNSIMPLE USER ACCESS TOKEN\"\n        - name: DNSIMPLE_ZONES\n          value: \"SET THIS IF USING A DNSIMPLE USER ACCESS TOKEN\"\n</code></pre>"},{"location":"docs/tutorials/dnsimple/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone you create in DNSimple.\n        - --provider=dnsimple\n        - --registry=txt\n        env:\n        - name: DNSIMPLE_OAUTH\n          value: \"YOUR_DNSIMPLE_API_KEY\"\n        - name: DNSIMPLE_ACCOUNT_ID\n          value: \"SET THIS IF USING A DNSIMPLE USER ACCESS TOKEN\"\n        - name: DNSIMPLE_ZONES\n          value: \"SET THIS IF USING A DNSIMPLE USER ACCESS TOKEN\"\n</code></pre>"},{"location":"docs/tutorials/dnsimple/#deploying-an-nginx-service","title":"Deploying an Nginx Service","text":"<p>Create a service file called \u2018nginx.yaml\u2019 with the following contents:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: validate-external-dns.example.com\nspec:\n  selector:\n    app: nginx\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre> <p>Note the annotation on the service; use the same hostname as the DNSimple DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019).</p> <p>ExternalDNS uses this annotation to determine what services should be registered with DNS.  Removing the annotation will cause ExternalDNS to remove the corresponding DNS records.</p> <p>Create the deployment and service:</p> <pre><code>kubectl create -f nginx.yaml\n</code></pre> <p>Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service. Check the status by running <code>kubectl get services nginx</code>.  If the <code>EXTERNAL-IP</code> field shows an address, the service is ready to be accessed externally.</p> <p>Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the DNSimple DNS records.</p>"},{"location":"docs/tutorials/dnsimple/#verifying-dnsimple-dns-records","title":"Verifying DNSimple DNS records","text":""},{"location":"docs/tutorials/dnsimple/#getting-your-dnsimple-account-id","title":"Getting your DNSimple Account ID","text":"<p>If you do not know your DNSimple account ID it can be acquired using the whoami endpoint from the DNSimple Identity API</p> <pre><code>curl -H \"Authorization: Bearer $DNSIMPLE_ACCOUNT_TOKEN\" \\\n    -H 'Accept: application/json' \\\n    https://api.dnsimple.com/v2/whoami\n{\n  \"data\": {\n    \"user\": null,\n    \"account\": {\n      \"id\": 1,\n      \"email\": \"example-account@example.com\",\n      \"plan_identifier\": \"dnsimple-professional\",\n      \"created_at\": \"2015-09-18T23:04:37Z\",\n      \"updated_at\": \"2016-06-09T20:03:39Z\"\n    }\n  }\n}\n</code></pre>"},{"location":"docs/tutorials/dnsimple/#looking-at-the-dnsimple-dashboard","title":"Looking at the DNSimple Dashboard","text":"<p>You can view your DNSimple Record Editor at https://dnsimple.com/a/YOUR_ACCOUNT_ID/domains/example.com/records. Ensure you substitute the value <code>YOUR_ACCOUNT_ID</code> with the ID of your DNSimple account and <code>example.com</code> with the correct domain that you used during validation.</p>"},{"location":"docs/tutorials/dnsimple/#using-the-dnsimple-zone-records-api","title":"Using the DNSimple Zone Records API","text":"<p>This approach allows for you to use the DNSimple List records for a zone endpoint to verify the creation of the A and TXT record. Ensure you substitute the value <code>YOUR_ACCOUNT_ID</code> with the ID of your DNSimple account and <code>example.com</code> with the correct domain that you used during validation.</p> <pre><code>curl -H \"Authorization: Bearer $DNSIMPLE_ACCOUNT_TOKEN\" \\\n    -H 'Accept: application/json' \\\n    'https://api.dnsimple.com/v2/YOUR_ACCOUNT_ID/zones/example.com/records&amp;name=validate-external-dns'\n</code></pre>"},{"location":"docs/tutorials/dnsimple/#clean-up","title":"Clean up","text":"<p>Now that we have verified that ExternalDNS will automatically manage DNSimple DNS records, we can delete the tutorial\u2019s example:</p> <pre><code>kubectl delete -f nginx.yaml\nkubectl delete -f externaldns.yaml\n</code></pre>"},{"location":"docs/tutorials/dnsimple/#deleting-created-records","title":"Deleting Created Records","text":"<p>The created records can be deleted using the record IDs from the verification step and the Delete a zone record endpoint.</p>"},{"location":"docs/tutorials/exoscale/","title":"Exoscale","text":""},{"location":"docs/tutorials/exoscale/#prerequisites","title":"Prerequisites","text":"<p>Exoscale provider support was added via this PR, thus you need to use external-dns v0.5.5.</p> <p>The Exoscale provider expects that your Exoscale zones, you wish to add records to, already exists and are configured correctly. It does not add, remove or configure new zones in anyway.</p> <p>To do this please refer to the Exoscale DNS documentation.</p> <p>Additionally you will have to provide the Exoscale\u2026:</p> <ul> <li>API Key</li> <li>API Secret</li> <li>Elastic IP address, to access the workers</li> </ul>"},{"location":"docs/tutorials/exoscale/#deployment","title":"Deployment","text":"<p>Deploying external DNS for Exoscale is actually nearly identical to deploying it for other providers. This is what a sample <code>deployment.yaml</code> looks like:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      # Only use if you're also using RBAC\n      # serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=ingress # or service or both\n        - --provider=exoscale\n        - --domain-filter={{ my-domain }}\n        - --policy=sync # if you want DNS entries to get deleted as well\n        - --txt-owner-id={{ owner-id-for-this-external-dns }}\n        - --exoscale-apikey={{ api-key}}\n        - --exoscale-apisecret={{ api-secret }}\n        # - --exoscale-apizone={{ api-zone }}\n        # - --exoscale-apienv={{ api-env }}\n</code></pre> <p>Optional arguments <code>--exoscale-apizone</code> and <code>--exoscale-apienv</code> define Exoscale API Zone (default <code>ch-gva-2</code>) and Exoscale API environment (default <code>api</code>, can be used to target non-production API server) respectively.</p>"},{"location":"docs/tutorials/exoscale/#rbac","title":"RBAC","text":"<p>If your cluster is RBAC enabled, you also need to setup the following, before you can run external-dns:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n  namespace: default\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\"]\n\n---\n\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n</code></pre>"},{"location":"docs/tutorials/exoscale/#testing-and-verification","title":"Testing and Verification","text":"<p>Important!: Remember to change <code>example.com</code> with your own domain throughout the following text.</p> <p>Spin up a simple nginx HTTP server with the following spec (<code>kubectl apply -f</code>):</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/target: {{ Elastic-IP-address }}\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: via-ingress.example.com\n    http:\n      paths:\n      - backend:\n          service:\n            name: \"nginx\"\n            port:\n              number: 80\n        path: /\n        pathType: Prefix\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\nspec:\n  ports:\n  - port: 80\n    targetPort: 80\n  selector:\n    app: nginx\n\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n</code></pre> <p>Important!: Don\u2019t run dig, nslookup or similar immediately (until you\u2019ve confirmed the record exists). You\u2019ll get hit by negative DNS caching, which is hard to flush.</p> <p>Wait about 30s-1m (interval for external-dns to kick in), then check Exoscales portal\u2026 via-ingress.example.com should appear as a A and TXT record with your Elastic-IP-address.</p>"},{"location":"docs/tutorials/externalname/","title":"ExternalName Services","text":"<p>This tutorial describes how to setup ExternalDNS for usage in conjunction with an ExternalName service.</p>"},{"location":"docs/tutorials/externalname/#use-cases","title":"Use cases","text":"<p>The main use cases that inspired this feature is the necessity for having a subdomain pointing to an external domain. In this scenario, it makes sense for the subdomain to have a CNAME record pointing to the external domain.</p>"},{"location":"docs/tutorials/externalname/#setup","title":"Setup","text":""},{"location":"docs/tutorials/externalname/#external-dns","title":"External DNS","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --log-level=debug\n        - --source=service\n        - --source=ingress\n        - --namespace=dev\n        - --domain-filter=example.org.\n        - --provider=aws\n        - --registry=txt\n        - --txt-owner-id=dev.example.org\n</code></pre>"},{"location":"docs/tutorials/externalname/#externalname-service","title":"ExternalName Service","text":"<pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: aws-service\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: tenant1.example.org,tenant2.example.org\nspec:\n  type: ExternalName\n  externalName: aws.example.org\n</code></pre> <p>This will create 2 CNAME records pointing to <code>aws.example.org</code>:</p> <pre><code>tenant1.example.org\ntenant2.example.org\n</code></pre>"},{"location":"docs/tutorials/externalname/#externalname-service-with-an-ip-address","title":"ExternalName Service with an IP address","text":"<p>If <code>externalName</code> is an IP address, External DNS will create A records instead of CNAME.</p> <pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: aws-service\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: tenant1.example.org,tenant2.example.org\nspec:\n  type: ExternalName\n  externalName: 111.111.111.111\n</code></pre> <p>This will create 2 A records pointing to <code>111.111.111.111</code>:</p> <pre><code>tenant1.example.org\ntenant2.example.org\n</code></pre>"},{"location":"docs/tutorials/gandi/","title":"Gandi","text":"<p>This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using Gandi.</p> <p>Make sure to use &gt;=0.7.7 version of ExternalDNS for this tutorial.</p>"},{"location":"docs/tutorials/gandi/#creating-a-gandi-dns-zone-domain","title":"Creating a Gandi DNS zone (domain)","text":"<p>Create a new DNS zone where you want to create your records in. Let\u2019s use <code>example.com</code> as an example here. Make sure the zone uses</p>"},{"location":"docs/tutorials/gandi/#creating-gandi-personal-access-token-pat","title":"Creating Gandi Personal Access Token (PAT)","text":"<p>Generate a Personal Access Token on your account (click on \u201cUser Settings\u201d) with <code>Manage domain name technical configurations</code> permission.</p> <p>The environment variable <code>GANDI_PAT</code> will be needed to run ExternalDNS with Gandi.</p> <p>You can also set <code>GANDI_KEY</code> if you have an old API key.</p>"},{"location":"docs/tutorials/gandi/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Connect your <code>kubectl</code> client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.</p>"},{"location":"docs/tutorials/gandi/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: external-dns\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=gandi\n        env:\n        - name: GANDI_PAT\n          value: \"YOUR_GANDI_PAT\"\n</code></pre>"},{"location":"docs/tutorials/gandi/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\",\"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: external-dns\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=gandi\n        env:\n        - name: GANDI_PAT\n          value: \"YOUR_GANDI_PAT\"\n</code></pre>"},{"location":"docs/tutorials/gandi/#deploying-an-nginx-service","title":"Deploying an Nginx Service","text":"<p>Create a service file called \u2018nginx.yaml\u2019 with the following contents:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: my-app.example.com\nspec:\n  selector:\n    app: nginx\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre> <p>Note the annotation on the service; use the same hostname as the Gandi Domain. Make sure that your Domain is configured to use Live-DNS.</p> <p>ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records.</p> <p>Create the deployment and service:</p> <pre><code>kubectl create -f nginx.yaml\n</code></pre> <p>Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service.</p> <p>Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Gandi DNS records.</p>"},{"location":"docs/tutorials/gandi/#verifying-gandi-dns-records","title":"Verifying Gandi DNS records","text":"<p>Check your Gandi Dashboard to view the records for your Gandi DNS zone.</p> <p>Click on the zone for the one created above if a different domain was used.</p> <p>This should show the external IP address of the service as the A record for your domain.</p>"},{"location":"docs/tutorials/gandi/#cleanup","title":"Cleanup","text":"<p>Now that we have verified that ExternalDNS will automatically manage Gandi DNS records, we can delete the tutorial\u2019s example:</p> <pre><code>kubectl delete service -f nginx.yaml\nkubectl delete service -f externaldns.yaml\n</code></pre>"},{"location":"docs/tutorials/gandi/#additional-options","title":"Additional options","text":"<p>If you\u2019re using organizations to separate your domains, you can pass the organization\u2019s ID in an environment variable called <code>GANDI_SHARING_ID</code> to get access to it.</p>"},{"location":"docs/tutorials/gke-nginx/","title":"GKE with nginx-ingress-controller","text":"<p>This tutorial describes how to setup ExternalDNS for usage within a GKE cluster that doesn\u2019t make use of Google\u2019s default ingress controller but rather uses nginx-ingress-controller for that task.</p>"},{"location":"docs/tutorials/gke-nginx/#set-up-your-environment","title":"Set up your environment","text":"<p>Setup your environment to work with Google Cloud Platform. Fill in your values as needed, e.g. target project.</p> <pre><code>gcloud config set project \"zalando-external-dns-test\"\ngcloud config set compute/region \"europe-west1\"\ngcloud config set compute/zone \"europe-west1-d\"\n</code></pre>"},{"location":"docs/tutorials/gke-nginx/#gke-node-scopes","title":"GKE Node Scopes","text":"<p>The following instructions use instance scopes to provide ExternalDNS with the permissions it needs to manage DNS records. Note that since these permissions are associated with the instance, all pods in the cluster will also have these permissions. As such, this approach is not suitable for anything but testing environments.</p> <p>Create a GKE cluster without using the default ingress controller.</p> <pre><code>$ gcloud container clusters create \"external-dns\" \\\n    --num-nodes 1 \\\n    --scopes \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\"\n</code></pre> <p>Create a DNS zone which will contain the managed DNS records.</p> <pre><code>$ gcloud dns managed-zones create \"external-dns-test-gcp-zalan-do\" \\\n    --dns-name \"external-dns-test.gcp.zalan.do.\" \\\n    --description \"Automatically managed zone by ExternalDNS\"\n</code></pre> <p>Make a note of the nameservers that were assigned to your new zone.</p> <pre><code>$ gcloud dns record-sets list \\\n    --zone \"external-dns-test-gcp-zalan-do\" \\\n    --name \"external-dns-test.gcp.zalan.do.\" \\\n    --type NS\nNAME                             TYPE  TTL    DATA\nexternal-dns-test.gcp.zalan.do.  NS    21600  ns-cloud-e1.googledomains.com.,ns-cloud-e2.googledomains.com.,ns-cloud-e3.googledomains.com.,ns-cloud-e4.googledomains.com.\n</code></pre> <p>In this case it\u2019s <code>ns-cloud-{e1-e4}.googledomains.com.</code> but your\u2019s could slightly differ, e.g. <code>{a1-a4}</code>, <code>{b1-b4}</code> etc.</p> <p>Tell the parent zone where to find the DNS records for this zone by adding the corresponding NS records there. Assuming the parent zone is \u201cgcp-zalan-do\u201d and the domain is \u201cgcp.zalan.do\u201d and that it\u2019s also hosted at Google we would do the following.</p> <pre><code>$ gcloud dns record-sets transaction start --zone \"gcp-zalan-do\"\n$ gcloud dns record-sets transaction add ns-cloud-e{1..4}.googledomains.com. \\\n    --name \"external-dns-test.gcp.zalan.do.\" --ttl 300 --type NS --zone \"gcp-zalan-do\"\n$ gcloud dns record-sets transaction execute --zone \"gcp-zalan-do\"\n</code></pre> <p>Connect your <code>kubectl</code> client to the cluster you just created and bind your GCP user to the cluster admin role in Kubernetes.</p> <pre><code>$ gcloud container clusters get-credentials \"external-dns\"\n$ kubectl create clusterrolebinding cluster-admin-me \\\n    --clusterrole=cluster-admin --user=\"$(gcloud config get-value account)\"\n</code></pre>"},{"location":"docs/tutorials/gke-nginx/#deploy-the-nginx-ingress-controller","title":"Deploy the nginx ingress controller","text":"<p>First, you need to deploy the nginx-based ingress controller. It can be deployed in at least two modes: Leveraging a Layer 4 load balancer in front of the nginx proxies or directly targeting pods with hostPorts on your worker nodes. ExternalDNS doesn\u2019t really care and supports both modes.</p>"},{"location":"docs/tutorials/gke-nginx/#default-backend","title":"Default Backend","text":"<p>The nginx controller uses a default backend that it serves when no Ingress rule matches. This is a separate Service that can be picked by you. We\u2019ll use the default backend that\u2019s used by other ingress controllers for that matter. Apply the following manifests to your cluster to deploy the default backend.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: default-http-backend\nspec:\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    app: default-http-backend\n\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: default-http-backend\nspec:\n  selector:\n    matchLabels:\n      app: default-http-backend\n  template:\n    metadata:\n      labels:\n        app: default-http-backend\n    spec:\n      containers:\n      - name: default-http-backend\n        image: gcr.io/google_containers/defaultbackend:1.3\n</code></pre>"},{"location":"docs/tutorials/gke-nginx/#without-a-separate-tcp-load-balancer","title":"Without a separate TCP load balancer","text":"<p>By default, the controller will update your Ingress objects with the public IPs of the nodes running your nginx controller instances. You should run multiple instances in case of pod or node failure. The controller will do leader election and will put multiple IPs as targets in your Ingress objects in that case. It could also make sense to run it as a DaemonSet. However, we\u2019ll just run a single replica. You have to open the respective ports on all of your worker nodes to allow nginx to receive traffic.</p> <pre><code>gcloud compute firewall-rules create \"allow-http\" --allow tcp:80 --source-ranges \"0.0.0.0/0\" --target-tags \"gke-external-dns-9488ba14-node\"\ngcloud compute firewall-rules create \"allow-https\" --allow tcp:443 --source-ranges \"0.0.0.0/0\" --target-tags \"gke-external-dns-9488ba14-node\"\n</code></pre> <p>Change <code>--target-tags</code> to the corresponding tags of your nodes. You can find them by describing your instances or by looking at the default firewall rules created by GKE for your cluster.</p> <p>Apply the following manifests to your cluster to deploy the nginx-based ingress controller. Note, how it receives a reference to the default backend\u2019s Service and that it listens on hostPorts. (You may have to use <code>hostNetwork: true</code> as well.)</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress-controller\nspec:\n  selector:\n    matchLabels:\n      app: nginx-ingress-controller\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress-controller\n    spec:\n      containers:\n      - name: nginx-ingress-controller\n        image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.3\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=default/default-http-backend\n        env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n</code></pre>"},{"location":"docs/tutorials/gke-nginx/#with-a-separate-tcp-load-balancer","title":"With a separate TCP load balancer","text":"<p>However, you can also have the ingress controller proxied by a Kubernetes Service. This will instruct the controller to populate this Service\u2019s external IP as the external IP of the Ingress. This exposes the nginx proxies via a Layer 4 load balancer (<code>type=LoadBalancer</code>) which is more reliable than the other method. With that approach, you can run as many nginx proxy instances on your cluster as you like or have them autoscaled. This is the preferred way of running the nginx controller.</p> <p>Apply the following manifests to your cluster. Note, how the controller is receiving an additional flag telling it which Service it should treat as its public endpoint and how it doesn\u2019t need hostPorts anymore.</p> <p>Apply the following manifests to run the controller in this mode.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-ingress-controller\nspec:\n  type: LoadBalancer\n  ports:\n  - name: http\n    port: 80\n    targetPort: 80\n  - name: https\n    port: 443\n    targetPort: 443\n  selector:\n    app: nginx-ingress-controller\n\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress-controller\nspec:\n  selector:\n    matchLabels:\n      app: nginx-ingress-controller\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress-controller\n    spec:\n      containers:\n      - name: nginx-ingress-controller\n        image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.3\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=default/default-http-backend\n        - --publish-service=default/nginx-ingress-controller\n        env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n        - containerPort: 443\n</code></pre>"},{"location":"docs/tutorials/gke-nginx/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Apply the following manifest file to deploy ExternalDNS.</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=ingress\n        - --domain-filter=external-dns-test.gcp.zalan.do\n        - --provider=google\n        - --google-project=zalando-external-dns-test\n        - --registry=txt\n        - --txt-owner-id=my-identifier\n</code></pre> <p>Use <code>--dry-run</code> if you want to be extra careful on the first run. Note, that you will not see any records created when you are running in dry-run mode. You can, however, inspect the logs and watch what would have been done.</p>"},{"location":"docs/tutorials/gke-nginx/#deploy-a-sample-application","title":"Deploy a sample application","text":"<p>Create the following sample application to test that ExternalDNS works.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nginx\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: via-ingress.external-dns-test.gcp.zalan.do\n    http:\n      paths:\n      - path: /\n        backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n        pathType: Prefix\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\nspec:\n  ports:\n  - port: 80\n    targetPort: 80\n  selector:\n    app: nginx\n\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n</code></pre> <p>After roughly two minutes check that a corresponding DNS record for your Ingress was created.</p> <pre><code>$ gcloud dns record-sets list \\\n    --zone \"external-dns-test-gcp-zalan-do\" \\\n    --name \"via-ingress.external-dns-test.gcp.zalan.do.\" \\\n    --type A\nNAME                                         TYPE  TTL  DATA\nvia-ingress.external-dns-test.gcp.zalan.do.  A     300  35.187.1.246\n</code></pre> <p>Let\u2019s check that we can resolve this DNS name as well.</p> <pre><code>dig +short @ns-cloud-e1.googledomains.com. via-ingress.external-dns-test.gcp.zalan.do.\n35.187.1.246\n</code></pre> <p>Try with <code>curl</code> as well.</p> <pre><code>$ curl via-ingress.external-dns-test.gcp.zalan.do\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n...\n&lt;/head&gt;\n&lt;body&gt;\n...\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"docs/tutorials/gke-nginx/#clean-up","title":"Clean up","text":"<p>Make sure to delete all Service and Ingress objects before terminating the cluster so all load balancers and DNS entries get cleaned up correctly.</p> <pre><code>kubectl delete service nginx-ingress-controller\nkubectl delete ingress nginx\n</code></pre> <p>Give ExternalDNS some time to clean up the DNS records for you. Then delete the managed zone and cluster.</p> <pre><code>gcloud dns managed-zones delete \"external-dns-test-gcp-zalan-do\"\ngcloud container clusters delete \"external-dns\"\n</code></pre> <p>Also delete the NS records for your removed zone from the parent zone.</p> <pre><code>$ gcloud dns record-sets transaction start --zone \"gcp-zalan-do\"\n$ gcloud dns record-sets transaction remove ns-cloud-e{1..4}.googledomains.com. \\\n    --name \"external-dns-test.gcp.zalan.do.\" --ttl 300 --type NS --zone \"gcp-zalan-do\"\n$ gcloud dns record-sets transaction execute --zone \"gcp-zalan-do\"\n</code></pre>"},{"location":"docs/tutorials/gke-nginx/#gke-with-workload-identity","title":"GKE with Workload Identity","text":"<p>The following instructions use GKE workload identity to provide ExternalDNS with the permissions it needs to manage DNS records. Workload identity is the Google-recommended way to provide GKE workloads access to GCP APIs.</p> <p>Create a GKE cluster with workload identity enabled and without the HttpLoadBalancing add-on.</p> <pre><code>$ gcloud container clusters create external-dns \\\n    --workload-metadata-from-node=GKE_METADATA_SERVER \\\n    --identity-namespace=zalando-external-dns-test.svc.id.goog \\\n    --addons=HorizontalPodAutoscaling\n</code></pre> <p>Create a GCP service account (GSA) for ExternalDNS and save its email address.</p> <pre><code>$ sa_name=\"Kubernetes external-dns\"\n$ gcloud iam service-accounts create sa-edns --display-name=\"$sa_name\"\n$ sa_email=$(gcloud iam service-accounts list --format='value(email)' \\\n    --filter=\"displayName:$sa_name\")\n</code></pre> <p>Bind the ExternalDNS GSA to the DNS admin role.</p> <pre><code>$ gcloud projects add-iam-policy-binding zalando-external-dns-test \\\n    --member=\"serviceAccount:$sa_email\" --role=roles/dns.admin\n</code></pre> <p>Link the ExternalDNS GSA to the Kubernetes service account (KSA) that external-dns will run under, i.e., the external-dns KSA in the external-dns namespaces.</p> <pre><code>$ gcloud iam service-accounts add-iam-policy-binding \"$sa_email\" \\\n    --member=\"serviceAccount:zalando-external-dns-test.svc.id.goog[external-dns/external-dns]\" \\\n    --role=roles/iam.workloadIdentityUser\n</code></pre> <p>Create a DNS zone which will contain the managed DNS records.</p> <pre><code>$ gcloud dns managed-zones create external-dns-test-gcp-zalan-do \\\n    --dns-name=external-dns-test.gcp.zalan.do. \\\n    --description=\"Automatically managed zone by ExternalDNS\"\n</code></pre> <p>Make a note of the nameservers that were assigned to your new zone.</p> <pre><code>$ gcloud dns record-sets list \\\n    --zone=external-dns-test-gcp-zalan-do \\\n    --name=external-dns-test.gcp.zalan.do. \\\n    --type NS\nNAME                             TYPE  TTL    DATA\nexternal-dns-test.gcp.zalan.do.  NS    21600  ns-cloud-e1.googledomains.com.,ns-cloud-e2.googledomains.com.,ns-cloud-e3.googledomains.com.,ns-cloud-e4.googledomains.com.\n</code></pre> <p>In this case it\u2019s <code>ns-cloud-{e1-e4}.googledomains.com.</code> but your\u2019s could slightly differ, e.g. <code>{a1-a4}</code>, <code>{b1-b4}</code> etc.</p> <p>Tell the parent zone where to find the DNS records for this zone by adding the corresponding NS records there. Assuming the parent zone is \u201cgcp-zalan-do\u201d and the domain is \u201cgcp.zalan.do\u201d and that it\u2019s also hosted at Google we would do the following.</p> <pre><code>$ gcloud dns record-sets transaction start --zone=gcp-zalan-do\n$ gcloud dns record-sets transaction add ns-cloud-e{1..4}.googledomains.com. \\\n    --name=external-dns-test.gcp.zalan.do. --ttl 300 --type NS --zone=gcp-zalan-do\n$ gcloud dns record-sets transaction execute --zone=gcp-zalan-do\n</code></pre> <p>Connect your <code>kubectl</code> client to the cluster you just created and bind your GCP user to the cluster admin role in Kubernetes.</p> <pre><code>$ gcloud container clusters get-credentials external-dns\n$ kubectl create clusterrolebinding cluster-admin-me \\\n    --clusterrole=cluster-admin --user=\"$(gcloud config get-value account)\"\n</code></pre>"},{"location":"docs/tutorials/gke-nginx/#deploy-ingress-nginx","title":"Deploy ingress-nginx","text":"<p>Follow the ingress-nginx GKE installation instructions to deploy it to the cluster.</p> <pre><code>$ kubectl apply -f \\\n    https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.35.0/deploy/static/provider/cloud/deploy.yaml\n</code></pre>"},{"location":"docs/tutorials/gke-nginx/#deploy-externaldns_1","title":"Deploy ExternalDNS","text":"<p>Apply the following manifest file to deploy external-dns.</p> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: external-dns\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n  namespace: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"services\", \"endpoints\", \"pods\"]\n    verbs: [\"get\", \"watch\", \"list\"]\n  - apiGroups: [\"extensions\", \"networking.k8s.io\"]\n    resources: [\"ingresses\"]\n    verbs: [\"get\", \"watch\", \"list\"]\n  - apiGroups: [\"\"]\n    resources: [\"nodes\"]\n    verbs: [\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n  - kind: ServiceAccount\n    name: external-dns\n    namespace: external-dns\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\n  namespace: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n        - args:\n            - --source=ingress\n            - --domain-filter=external-dns-test.gcp.zalan.do\n            - --provider=google\n            - --google-project=zalando-external-dns-test\n            - --registry=txt\n            - --txt-owner-id=my-identifier\n          image: registry.k8s.io/external-dns/external-dns:v0.15.1\n          name: external-dns\n      securityContext:\n        fsGroup: 65534\n        runAsUser: 65534\n      serviceAccountName: external-dns\n</code></pre> <p>Then add the proper workload identity annotation to the cert-manager service account.</p> <pre><code>$ kubectl annotate serviceaccount --namespace=external-dns external-dns \\\n    \"iam.gke.io/gcp-service-account=$sa_email\"\n</code></pre>"},{"location":"docs/tutorials/gke-nginx/#deploy-a-sample-application_1","title":"Deploy a sample application","text":"<p>Create the following sample application to test that ExternalDNS works.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nginx\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: via-ingress.external-dns-test.gcp.zalan.do\n    http:\n      paths:\n      - path: /\n        backend:\n          service:\n            name: nginx\n            port:\n              number: 80\n        pathType: Prefix\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\nspec:\n  ports:\n  - port: 80\n    targetPort: 80\n  selector:\n    app: nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n</code></pre> <p>After roughly two minutes check that a corresponding DNS record for your ingress was created.</p> <pre><code>$ gcloud dns record-sets list \\\n    --zone \"external-dns-test-gcp-zalan-do\" \\\n    --name \"via-ingress.external-dns-test.gcp.zalan.do.\" \\\n    --type A\nNAME                                         TYPE  TTL  DATA\nvia-ingress.external-dns-test.gcp.zalan.do.  A     300  35.187.1.246\n</code></pre> <p>Let\u2019s check that we can resolve this DNS name as well.</p> <pre><code>$ dig +short @ns-cloud-e1.googledomains.com. via-ingress.external-dns-test.gcp.zalan.do.\n35.187.1.246\n</code></pre> <p>Try with <code>curl</code> as well.</p> <pre><code>$ curl via-ingress.external-dns-test.gcp.zalan.do\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n...\n&lt;/head&gt;\n&lt;body&gt;\n...\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"docs/tutorials/gke-nginx/#clean-up_1","title":"Clean up","text":"<p>Make sure to delete all service and ingress objects before terminating the cluster so all load balancers and DNS entries get cleaned up correctly.</p> <pre><code>kubectl delete service --namespace=ingress-nginx ingress-nginx-controller\nkubectl delete ingress nginx\n</code></pre> <p>Give ExternalDNS some time to clean up the DNS records for you. Then delete the managed zone and cluster.</p> <pre><code>gcloud dns managed-zones delete external-dns-test-gcp-zalan-do\ngcloud container clusters delete external-dns\n</code></pre> <p>Also delete the NS records for your removed zone from the parent zone.</p> <pre><code>$ gcloud dns record-sets transaction start --zone gcp-zalan-do\n$ gcloud dns record-sets transaction remove ns-cloud-e{1..4}.googledomains.com. \\\n    --name=external-dns-test.gcp.zalan.do. --ttl 300 --type NS --zone=gcp-zalan-do\n$ gcloud dns record-sets transaction execute --zone=gcp-zalan-do\n</code></pre>"},{"location":"docs/tutorials/gke-nginx/#user-demo-how-to-blogs-and-examples","title":"User Demo How-To Blogs and Examples","text":"<ul> <li>Run external-dns on GKE with workload identity. See Kubernetes, ingress-nginx, cert-manager &amp; external-dns</li> </ul>"},{"location":"docs/tutorials/gke/","title":"GKE with default controller","text":"<p>This tutorial describes how to setup ExternalDNS for usage within a GKE (Google Kuberentes Engine) cluster. Make sure to use &gt;=0.11.0 version of ExternalDNS for this tutorial</p>"},{"location":"docs/tutorials/gke/#single-project-test-scenario-using-access-scopes","title":"Single project test scenario using access scopes","text":"<p>If you prefer to try-out ExternalDNS in one of the existing environments you can skip this step</p> <p>The following instructions use access scopes to provide ExternalDNS with the permissions it needs to manage DNS records within a single project, the organizing entity to allocate resources.</p> <p>Note that since these permissions are associated with the instance, all pods in the cluster will also have these permissions. As such, this approach is not suitable for anything but testing environments.</p> <p>This solution will only work when both CloudDNS and GKE are provisioned in the same project.  If the CloudDNS zone is in a different project, this solution will not work.</p>"},{"location":"docs/tutorials/gke/#configure-project-environment","title":"Configure Project Environment","text":"<p>Set up your environment to work with Google Cloud Platform. Fill in your variables as needed, e.g. target project.</p> <pre><code># set variables to the appropriate desired values\nPROJECT_ID=\"my-external-dns-test\"\nREGION=\"europe-west1\"\nZONE=\"europe-west1-d\"\nClOUD_BILLING_ACCOUNT=\"&lt;my-cloud-billing-account&gt;\"\n# set default settings for project\ngcloud config set project $PROJECT_ID\ngcloud config set compute/region $REGION\ngcloud config set compute/zone $ZONE\n# enable billing and APIs if not done already\ngcloud beta billing projects link $PROJECT_ID \\\n  --billing-account $BILLING_ACCOUNT\ngcloud services enable \"dns.googleapis.com\"\ngcloud services enable \"container.googleapis.com\"\n</code></pre>"},{"location":"docs/tutorials/gke/#create-gke-cluster","title":"Create GKE Cluster","text":"<pre><code>gcloud container clusters create $GKE_CLUSTER_NAME \\\n  --num-nodes 1 \\\n  --scopes \"https://www.googleapis.com/auth/ndev.clouddns.readwrite\"\n</code></pre> <p>[!WARNING] Note that this cluster will use the default compute engine GSA that contians the overly permissive project editor (<code>roles/editor</code>) role. So essentially, anything on the cluster could potentially grant escalated privileges. Also, as mentioned earlier, the access scope <code>ndev.clouddns.readwrite</code> will allow anything running on the cluster to have read/write permissions on all Cloud DNS zones within the same project.</p>"},{"location":"docs/tutorials/gke/#cloud-dns-zone","title":"Cloud DNS Zone","text":"<p>Create a DNS zone which will contain the managed DNS records. If using your own domain that was registered with a third-party domain registrar, you should point your domain\u2019s name servers to the values under the <code>nameServers</code> key. Please consult your registrar\u2019s documentation on how to do that. This tutorial will use example domain of <code>example.com</code>.</p> <pre><code>gcloud dns managed-zones create \"example-com\" --dns-name \"example.com.\" \\\n  --description \"Automatically managed zone by kubernetes.io/external-dns\"\n</code></pre> <p>Make a note of the nameservers that were assigned to your new zone.</p> <pre><code>gcloud dns record-sets list \\\n    --zone \"example-com\" --name \"example.com.\" --type NS\n</code></pre> <p>Outputs:</p> <pre><code>NAME          TYPE  TTL    DATA\nexample.com.  NS    21600  ns-cloud-e1.googledomains.com.,ns-cloud-e2.googledomains.com.,ns-cloud-e3.googledomains.com.,ns-cloud-e4.googledomains.com.\n</code></pre> <p>In this case it\u2019s <code>ns-cloud-{e1-e4}.googledomains.com.</code> but your\u2019s could slightly differ, e.g. <code>{a1-a4}</code>, <code>{b1-b4}</code> etc.</p>"},{"location":"docs/tutorials/gke/#cross-project-access-scenario-using-google-service-account","title":"Cross project access scenario using Google Service Account","text":"<p>More often, following best practices in regards to security and operations, Cloud DNS zones will be managed in a separate project from the Kubernetes cluster. This section shows how setup ExternalDNS to access Cloud DNS from a different project. These steps will also work for single project scenarios as well.</p> <p>ExternalDNS will need permissions to make changes to the Cloud DNS zone. There are three ways to configure the access needed:</p> <ul> <li>Worker Node Service Account</li> <li>Static Credentials</li> <li>Workload Identity</li> </ul>"},{"location":"docs/tutorials/gke/#setup-cloud-dns-and-gke","title":"Setup Cloud DNS and GKE","text":"<p>Below are examples on how you can configure Cloud DNS and GKE in separate projects, and then use one of the three methods to grant access to ExternalDNS.  Replace the environment variables to values that make sense in your environment.</p>"},{"location":"docs/tutorials/gke/#configure-projects","title":"Configure Projects","text":"<p>For this process, create projects with the appropriate APIs enabled.</p> <pre><code># set variables to appropriate desired values\nGKE_PROJECT_ID=\"my-workload-project\"\nDNS_PROJECT_ID=\"my-cloud-dns-project\"\nClOUD_BILLING_ACCOUNT=\"&lt;my-cloud-billing-account&gt;\"\n# enable billing and APIs for DNS project if not done already\ngcloud config set project $DNS_PROJECT_ID\ngcloud beta billing projects link $CLOUD_DNS_PROJECT \\\n  --billing-account $ClOUD_BILLING_ACCOUNT\ngcloud services enable \"dns.googleapis.com\"\n# enable billing and APIs for GKE project if not done already\ngcloud config set project $GKE_PROJECT_ID\ngcloud beta billing projects link $CLOUD_DNS_PROJECT \\\n  --billing-account $ClOUD_BILLING_ACCOUNT\ngcloud services enable \"container.googleapis.com\"\n</code></pre>"},{"location":"docs/tutorials/gke/#provisioning-cloud-dns","title":"Provisioning Cloud DNS","text":"<p>Create a Cloud DNS zone in the designated DNS project.</p> <pre><code>gcloud dns managed-zones create \"example-com\" --project $DNS_PROJECT_ID \\\n  --description \"example.com\" --dns-name=\"example.com.\" --visibility=public\n</code></pre> <p>If using your own domain that was registered with a third-party domain registrar, you should point your domain\u2019s name servers to the values under the <code>nameServers</code> key.  Please consult your registrar\u2019s documentation on how to do that. The example domain of <code>example.com</code> will be used for this tutorial.</p>"},{"location":"docs/tutorials/gke/#provisioning-a-gke-cluster-for-cross-project-access","title":"Provisioning a GKE cluster for cross project access","text":"<p>Create a GSA (Google Service Account) and grant it the minimal set of privileges required for GKE nodes:</p> <pre><code>GKE_CLUSTER_NAME=\"my-external-dns-cluster\"\nGKE_REGION=\"us-central1\"\nGKE_SA_NAME=\"worker-nodes-sa\"\nGKE_SA_EMAIL=\"$GKE_SA_NAME@${GKE_PROJECT_ID}.iam.gserviceaccount.com\"\n\nROLES=(\n  roles/logging.logWriter\n  roles/monitoring.metricWriter\n  roles/monitoring.viewer\n  roles/stackdriver.resourceMetadata.writer\n)\n\ngcloud iam service-accounts create $GKE_SA_NAME \\\n  --display-name $GKE_SA_NAME --project $GKE_PROJECT_ID\n\n# assign google service account to roles in GKE project\nfor ROLE in ${ROLES[*]}; do\n  gcloud projects add-iam-policy-binding $GKE_PROJECT_ID \\\n    --member \"serviceAccount:$GKE_SA_EMAIL\" \\\n    --role $ROLE\ndone\n</code></pre> <p>Create a cluster using this service account and enable workload identity:</p> <pre><code>gcloud container clusters create $GKE_CLUSTER_NAME \\\n  --project $GKE_PROJECT_ID --region $GKE_REGION --num-nodes 1 \\\n  --service-account \"$GKE_SA_EMAIL\" \\\n  --workload-pool \"$GKE_PROJECT_ID.svc.id.goog\"\n</code></pre>"},{"location":"docs/tutorials/gke/#workload-identity","title":"Workload Identity","text":"<p>Workload Identity allows workloads in your GKE cluster to authenticate directly to GCP using Kubernetes Service Accounts</p> <p>You have an option to chose from using the gcloud CLI or using Terraform.</p> gcloud CLITerraform <p>The below instructions assume you are using the default Kubernetes Service account name of <code>external-dns</code> in the namespace <code>external-dns</code></p> <p>Grant the Kubernetes service account DNS <code>roles/dns.admin</code> at project level</p> <pre><code>gcloud projects add-iam-policy-binding projects/DNS_PROJECT_ID \\\n    --role=roles/dns.admin \\\n    --member=principal://iam.googleapis.com/projects/PROJECT_NUMBER/locations/global/workloadIdentityPools/PROJECT_ID.svc.id.goog/subject/ns/external-dns/sa/external-dns \\\n    --condition=None\n</code></pre> <p>Replace the following:</p> <ul> <li><code>DNS_PROJECT_ID</code> : Project ID of your DNS project. If DNS is in the same project as your GKE cluster, use your GKE project.</li> <li><code>PROJECT_ID</code>: your Google Cloud project ID of your GKE Cluster</li> <li><code>PROJECT_NUMBER</code>: your numerical Google Cloud project number of your GKE cluster</li> </ul> <p>If you wish to change the namespace, replace</p> <ul> <li><code>ns/external-dns</code> with <code>ns/&lt;your namespace</code></li> <li><code>sa/external-dns</code> with <code>sa/&lt;your ksa&gt;</code></li> </ul> <p>The below instructions assume you are using the default Kubernetes Service account name of <code>external-dns</code> in the namespace <code>external-dns</code></p> <p>Create a file called <code>main.tf</code> and place in it the below. Note: If you\u2019re an experienced terraform user feel free to split these out in to different files</p> <pre><code>variable \"gke-project\" {\n  type        = string\n  description = \"Name of the project that the GKE cluster exists in\"\n  default     = \"GKE-PROJECT\"\n}\n\nvariable \"ksa_name\" {\n  type        = string\n  description = \"Name of the Kubernetes service account that will be accessing the DNS Zones\"\n  default     = \"external-dns\"\n}\n\nvariable \"kns_name\" {\n  type        = string\n  description = \"Name of the Kubernetes Namespace\"\n  default     = \"external-dns\"\n}\n\ndata \"google_project\" \"project\" {\n  project_id = var.gke-project\n}\n\nlocals {\n  member = \"principal://iam.googleapis.com/projects/${data.google_project.project.number}/locations/global/workloadIdentityPools/${var.gke-project}.svc.id.goog/subject/ns/${var.kns_name}/sa/${var.ksa_name}\"\n}\n\nresource \"google_project_iam_member\" \"external_dns\" {\n  member  = local.member\n  project = \"DNS-PROJECT\"\n  role    = \"roles/dns.reader\"\n}\n\nresource \"google_dns_managed_zone_iam_member\" \"member\" {\n  project      = \"DNS-PROJECT\"\n  managed_zone = \"ZONE-NAME\"\n  role         = \"roles/dns.admin\"\n  member       = local.member\n}\n</code></pre> <p>Replace the following</p> <ul> <li><code>GKE-PROJECT</code> : Project that contains your GKE cluster</li> <li><code>DNS-PROJECT</code> : Project that holds your DNS zones</li> </ul> <p>You can also change the below if you plan to use a different service account name and namespace</p> <ul> <li><code>variable \"ksa_name\"</code> : Name of the Kubernetes service account external-dns will use</li> <li><code>variable \"kns_name\"</code> : Name of the Kubernetes Name Space that will have external-dns installed to</li> </ul>"},{"location":"docs/tutorials/gke/#worker-node-service-account-method","title":"Worker Node Service Account method","text":"<p>In this method, the GSA (Google Service Account) that is associated with GKE worker nodes will be configured to have access to Cloud DNS.</p> <p>WARNING: This will grant access to modify the Cloud DNS zone records for all containers running on cluster, not just ExternalDNS, so use this option with caution.  This is not recommended for production environments.</p> <pre><code>GKE_SA_EMAIL=\"$GKE_SA_NAME@${GKE_PROJECT_ID}.iam.gserviceaccount.com\"\n\n# assign google service account to dns.admin role in the cloud dns project\ngcloud projects add-iam-policy-binding $DNS_PROJECT_ID \\\n  --member serviceAccount:$GKE_SA_EMAIL \\\n  --role roles/dns.admin\n</code></pre> <p>After this, follow the steps in Deploy ExternalDNS.  Make sure to set the <code>--google-project</code> flag to match the Cloud DNS project name.</p>"},{"location":"docs/tutorials/gke/#static-credentials","title":"Static Credentials","text":"<p>In this scenario, a new GSA (Google Service Account) is created that has access to the CloudDNS zone.  The credentials for this GSA are saved and installed as a Kubernetes secret that will be used by ExternalDNS.</p> <p>This allows only containers that have access to the secret, such as ExternalDNS to update records on the Cloud DNS Zone.</p>"},{"location":"docs/tutorials/gke/#create-gsa-for-use-with-static-credentials","title":"Create GSA for use with static credentials","text":"<pre><code>DNS_SA_NAME=\"external-dns-sa\"\nDNS_SA_EMAIL=\"$DNS_SA_NAME@${GKE_PROJECT_ID}.iam.gserviceaccount.com\"\n\n# create GSA used to access the Cloud DNS zone\ngcloud iam service-accounts create $DNS_SA_NAME --display-name $DNS_SA_NAME\n\n# assign google service account to dns.admin role in cloud-dns project\ngcloud projects add-iam-policy-binding $DNS_PROJECT_ID \\\n  --member serviceAccount:$DNS_SA_EMAIL --role \"roles/dns.admin\"\n</code></pre>"},{"location":"docs/tutorials/gke/#create-kubernetes-secret-using-static-credentials","title":"Create Kubernetes secret using static credentials","text":"<p>Generate static credentials from the ExternalDNS GSA.</p> <pre><code># download static credentials\ngcloud iam service-accounts keys create /local/path/to/credentials.json \\\n  --iam-account $DNS_SA_EMAIL\n</code></pre> <p>Create a Kubernetes secret with the credentials in the same namespace of ExternalDNS.</p> <pre><code>kubectl create secret generic \"external-dns\" --namespace ${EXTERNALDNS_NS:-\"default\"} \\\n  --from-file /local/path/to/credentials.json\n</code></pre> <p>After this, follow the steps in Deploy ExternalDNS.  Make sure to set the <code>--google-project</code> flag to match Cloud DNS project name. Make sure to uncomment out the section that mounts the secret to the ExternalDNS pods.</p>"},{"location":"docs/tutorials/gke/#deploy-external-dns","title":"Deploy External DNS","text":"<p>Deploy ExternalDNS with the following steps below, documented under Deploy ExternalDNS.  Set the <code>--google-project</code> flag to the Cloud DNS project name.</p>"},{"location":"docs/tutorials/gke/#update-externaldns-pods","title":"Update ExternalDNS pods","text":"<p>Only required if not enabled on all nodes</p> <p>If you have GKE Workload Identity enabled on all nodes in your cluster, the below step is not necessary</p> <p>Update the Pod spec to schedule the workloads on nodes that use Workload Identity and to use the annotated Kubernetes service account.</p> <pre><code>kubectl patch deployment \"external-dns\" \\\n  --namespace ${EXTERNALDNS_NS:-\"default\"} \\\n  --patch \\\n '{\"spec\": {\"template\": {\"spec\": {\"nodeSelector\": {\"iam.gke.io/gke-metadata-server-enabled\": \"true\"}}}}}'\n</code></pre> <p>After all of these steps you may see several messages with <code>googleapi: Error 403: Forbidden, forbidden</code>.  After several minutes when the token is refreshed, these error messages will go away, and you should see info messages, such as: <code>All records are already up to date</code>.</p>"},{"location":"docs/tutorials/gke/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Then apply the following manifests file to deploy ExternalDNS.</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n  labels:\n    app.kubernetes.io/name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\n  labels:\n    app.kubernetes.io/name: external-dns\nrules:\n  - apiGroups: [\"\"]\n    resources: [\"services\",\"endpoints\",\"pods\",\"nodes\"]\n    verbs: [\"get\",\"watch\",\"list\"]\n  - apiGroups: [\"extensions\",\"networking.k8s.io\"]\n    resources: [\"ingresses\"]\n    verbs: [\"get\",\"watch\",\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\n  labels:\n    app.kubernetes.io/name: external-dns\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n  - kind: ServiceAccount\n    name: external-dns\n    namespace: default # change if namespace is not 'default'\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\n  labels:\n    app.kubernetes.io/name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-dns\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n        - name: external-dns\n          image: registry.k8s.io/external-dns/external-dns:v0.15.1\n          args:\n            - --source=service\n            - --source=ingress\n            - --domain-filter=example.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n            - --provider=google\n            - --log-format=json # google cloud logs parses severity of the \"text\" log format incorrectly\n    #        - --google-project=my-cloud-dns-project # Use this to specify a project different from the one external-dns is running inside\n            - --google-zone-visibility=public # Use this to filter to only zones with this visibility. Set to either 'public' or 'private'. Omitting will match public and private zones\n            - --policy=upsert-only # would prevent ExternalDNS from deleting any records, omit to enable full synchronization\n            - --registry=txt\n            - --txt-owner-id=my-identifier\n      #     # uncomment below if static credentials are used\n      #     env:\n      #       - name: GOOGLE_APPLICATION_CREDENTIALS\n      #         value: /etc/secrets/service-account/credentials.json\n      #     volumeMounts:\n      #       - name: google-service-account\n      #         mountPath: /etc/secrets/service-account/\n      # volumes:\n      #   - name: google-service-account\n      #     secret:\n      #       secretName: external-dns\n</code></pre> <p>Create the deployment for ExternalDNS:</p> <pre><code>kubectl create --namespace \"default\" --filename externaldns.yaml\n</code></pre>"},{"location":"docs/tutorials/gke/#verify-externaldns-works","title":"Verify ExternalDNS works","text":"<p>The following will deploy a small nginx server that will be used to demonstrate that ExternalDNS is working.</p>"},{"location":"docs/tutorials/gke/#verify-using-an-external-load-balancer","title":"Verify using an external load balancer","text":"<p>Create the following sample application to test that ExternalDNS works.  This example will provision a L4 load balancer.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    # change nginx.example.com to match an appropriate value\n    external-dns.alpha.kubernetes.io/hostname: nginx.example.com\nspec:\n  type: LoadBalancer\n  ports:\n    - port: 80\n      targetPort: 80\n  selector:\n    app: nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - image: nginx\n          name: nginx\n          ports:\n            - containerPort: 80\n</code></pre> <p>Create the deployment and service objects:</p> <pre><code>kubectl create --namespace \"default\" --filename nginx.yaml\n</code></pre> <p>After roughly two minutes check that a corresponding DNS record for your service was created.</p> <pre><code>gcloud dns record-sets list --zone \"example-com\" --name \"nginx.example.com.\"\n</code></pre> <p>Example output:</p> <pre><code>NAME                TYPE  TTL  DATA\nnginx.example.com.  A     300  104.155.60.49\nnginx.example.com.  TXT   300  \"heritage=external-dns,external-dns/owner=my-identifier\"\n</code></pre> <p>Note created <code>TXT</code> record alongside <code>A</code> record. <code>TXT</code> record signifies that the corresponding <code>A</code> record is managed by ExternalDNS. This makes ExternalDNS safe for running in environments where there are other records managed via other means.</p> <p>Let\u2019s check that we can resolve this DNS name. We\u2019ll ask the nameservers assigned to your zone first.</p> <pre><code>dig +short @ns-cloud-e1.googledomains.com. nginx.example.com.\n104.155.60.49\n</code></pre> <p>Given you hooked up your DNS zone with its parent zone you can use <code>curl</code> to access your site.</p> <pre><code>curl nginx.example.com\n</code></pre>"},{"location":"docs/tutorials/gke/#verify-using-an-ingress","title":"Verify using an ingress","text":"<p>Let\u2019s check that Ingress works as well. Create the following Ingress.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: nginx\nspec:\n  rules:\n    - host: server.example.com\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: nginx\n                port:\n                  number: 80\n</code></pre> <p>Create the ingress objects with:</p> <pre><code>kubectl create --namespace \"default\" --filename ingress.yaml\n</code></pre> <p>Note that this will ingress object will use the default ingress controller that comes with GKE to create a L7 load balancer in addition to the L4 load balancer previously with the service object. To use only the L7 load balancer, update the service manafest to change the Service type to <code>NodePort</code> and remove the ExternalDNS annotation.</p> <p>After roughly two minutes check that a corresponding DNS record for your Ingress was created.</p> <pre><code>gcloud dns record-sets list \\\n    --zone \"example-com\" \\\n    --name \"server.example.com.\" \\\n</code></pre> <p>Output:</p> <pre><code>NAME                 TYPE  TTL  DATA\nserver.example.com.  A     300  130.211.46.224\nserver.example.com.  TXT   300  \"heritage=external-dns,external-dns/owner=my-identifier\"\n</code></pre> <p>Let\u2019s check that we can resolve this DNS name as well.</p> <pre><code>dig +short @ns-cloud-e1.googledomains.com. server.example.com.\n130.211.46.224\n</code></pre> <p>Try with <code>curl</code> as well.</p> <pre><code>curl server.example.com\n</code></pre>"},{"location":"docs/tutorials/gke/#clean-up","title":"Clean up","text":"<p>Make sure to delete all Service and Ingress objects before terminating the cluster so all load balancers get cleaned up correctly.</p> <pre><code>kubectl delete service nginx\nkubectl delete ingress nginx\n</code></pre> <p>Give ExternalDNS some time to clean up the DNS records for you. Then delete the managed zone and cluster.</p> <pre><code>gcloud dns managed-zones delete \"example-com\"\ngcloud container clusters delete \"external-dns\"\n</code></pre>"},{"location":"docs/tutorials/godaddy/","title":"GoDaddy","text":"<p>This tutorial describes how to set up ExternalDNS for use within a Kubernetes cluster using GoDaddy DNS.</p> <p>Make sure to use &gt;=0.6 version of ExternalDNS for this tutorial.</p>"},{"location":"docs/tutorials/godaddy/#creating-a-zone-with-godaddy-dns","title":"Creating a zone with GoDaddy DNS","text":"<p>If you are new to GoDaddy, we recommend you first read the following instructions for creating a zone.</p> <p>Creating a zone using the GoDaddy web console</p> <p>Creating a zone using the GoDaddy API</p>"},{"location":"docs/tutorials/godaddy/#creating-godaddy-api-key","title":"Creating GoDaddy API key","text":"<p>You first need to create an API Key.</p> <p>Using the GoDaddy documentation you will have your <code>API key</code> and <code>API secret</code></p>"},{"location":"docs/tutorials/godaddy/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Connect your <code>kubectl</code> client to the cluster with which you want to test ExternalDNS, and then apply one of the following manifest files for deployment:</p>"},{"location":"docs/tutorials/godaddy/#using-helm","title":"Using Helm","text":"<p>Create a values.yaml file to configure ExternalDNS to use GoDaddy as the DNS provider. This file should include the necessary environment variables:</p> <pre><code>provider:\n  name: godaddy\nextraArgs:\n  - --godaddy-api-key=YOUR_API_KEY\n  - --godaddy-api-secret=YOUR_API_SECRET\n</code></pre> <p>Be sure to replace YOUR_API_KEY and YOUR_API_SECRET with your actual GoDaddy API key and GoDaddy API secret.</p> <p>Finally, install the ExternalDNS chart with Helm using the configuration specified in your values.yaml file:</p> <pre><code>helm upgrade --install external-dns external-dns/external-dns --values values.yaml\n</code></pre>"},{"location":"docs/tutorials/godaddy/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=godaddy\n        - --txt-prefix=external-dns. # In case of multiple k8s cluster\n        - --txt-owner-id=owner-id # In case of multiple k8s cluster\n        - --godaddy-api-key=&lt;Your API Key&gt;\n        - --godaddy-api-secret=&lt;Your API secret&gt;\n</code></pre>"},{"location":"docs/tutorials/godaddy/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\",\"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"endpoints\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=godaddy\n        - --txt-prefix=external-dns. # In case of multiple k8s cluster\n        - --txt-owner-id=owner-id # In case of multiple k8s cluster\n        - --godaddy-api-key=&lt;Your API Key&gt;\n        - --godaddy-api-secret=&lt;Your API secret&gt;\n</code></pre>"},{"location":"docs/tutorials/godaddy/#deploying-an-nginx-service","title":"Deploying an Nginx Service","text":"<p>Create a service file called \u2018nginx.yaml\u2019 with the following contents:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: example.com\n    external-dns.alpha.kubernetes.io/ttl: \"120\" #optional\nspec:\n  selector:\n    app: nginx\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre> <p>A note about annotations</p> <p>Verify that the annotation on the service uses the same hostname as the GoDaddy DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019).</p> <p>The TTL annotation can be used to configure the TTL on DNS records managed by ExternalDNS and is optional. If this annotation is not set, the TTL on records managed by ExternalDNS will default to 10.</p> <p>ExternalDNS uses the hostname annotation to determine which services should be registered with DNS. Removing the hostname annotation will cause ExternalDNS to remove the corresponding DNS records.</p>"},{"location":"docs/tutorials/godaddy/#create-the-deployment-and-service","title":"Create the deployment and service","text":"<pre><code>kubectl create -f nginx.yaml\n</code></pre> <p>Depending on where you run your service, it may take some time for your cloud provider to create an external IP for the service. Once an external IP is assigned, ExternalDNS detects the new service IP address and synchronizes the GoDaddy DNS records.</p>"},{"location":"docs/tutorials/godaddy/#verifying-godaddy-dns-records","title":"Verifying GoDaddy DNS records","text":"<p>Use the GoDaddy web console or API to verify that the A record for your domain shows the external IP address of the services.</p>"},{"location":"docs/tutorials/godaddy/#cleanup","title":"Cleanup","text":"<p>Once you successfully configure and verify record management via ExternalDNS, you can delete the tutorial\u2019s example:</p> <pre><code>kubectl delete -f nginx.yaml\nkubectl delete -f externaldns.yaml\n</code></pre>"},{"location":"docs/tutorials/hostport/","title":"Headless Services","text":"<p>This tutorial describes how to setup ExternalDNS for usage in conjunction with a Headless service.</p>"},{"location":"docs/tutorials/hostport/#use-cases","title":"Use cases","text":"<p>The main use cases that inspired this feature is the necessity for fixed addressable hostnames with services, such as Kafka when trying to access them from outside the cluster. In this scenario, quite often, only the Node IP addresses are actually routable and as in systems like Kafka more direct connections are preferable.</p>"},{"location":"docs/tutorials/hostport/#setup","title":"Setup","text":"<p>We will go through a small example of deploying a simple Kafka with use of a headless service.</p>"},{"location":"docs/tutorials/hostport/#external-dns","title":"External DNS","text":"<p>A simple deploy could look like this:</p>"},{"location":"docs/tutorials/hostport/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --log-level=debug\n        - --source=service\n        - --source=ingress\n        - --namespace=dev\n        - --domain-filter=example.org.\n        - --provider=aws\n        - --registry=txt\n        - --txt-owner-id=dev.example.org\n</code></pre>"},{"location":"docs/tutorials/hostport/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --log-level=debug\n        - --source=service\n        - --source=ingress\n        - --namespace=dev\n        - --domain-filter=example.org.\n        - --provider=aws\n        - --registry=txt\n        - --txt-owner-id=dev.example.org\n</code></pre>"},{"location":"docs/tutorials/hostport/#kafka-stateful-set","title":"Kafka Stateful Set","text":"<p>First lets deploy a Kafka Stateful set, a simple example(a lot of stuff is missing) with a headless service called <code>ksvc</code></p> <pre><code>apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\nspec:\n  serviceName: ksvc\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        component: kafka\n    spec:\n      containers:\n      - name:  kafka\n        image: confluent/kafka\n        ports:\n        - containerPort: 9092\n          hostPort: 9092\n          name: external\n        command:\n        - bash\n        - -c\n        - \" export DOMAIN=$(hostname -d) &amp;&amp; \\\n            export KAFKA_BROKER_ID=$(echo $HOSTNAME|rev|cut -d '-' -f 1|rev) &amp;&amp; \\\n            export KAFKA_ZOOKEEPER_CONNECT=$ZK_CSVC_SERVICE_HOST:$ZK_CSVC_SERVICE_PORT &amp;&amp; \\\n            export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://$HOSTNAME.example.org:9092 &amp;&amp; \\\n            /etc/confluent/docker/run\"\n        volumeMounts:\n        - name: datadir\n          mountPath: /var/lib/kafka\n  volumeClaimTemplates:\n  - metadata:\n      name: datadir\n      annotations:\n          volume.beta.kubernetes.io/storage-class: st1\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage:  500Gi\n</code></pre> <p>Very important here, is to set the <code>hostPort</code>(only works if the PodSecurityPolicy allows it)! and in case your app requires an actual hostname inside the container, unlike Kafka, which can advertise on another address, you have to set the hostname yourself.</p>"},{"location":"docs/tutorials/hostport/#headless-service","title":"Headless Service","text":"<p>Now we need to define a headless service to use to expose the Kafka pods. There are generally two approaches to use expose the nodeport of a Headless service:</p> <ol> <li>Add <code>--fqdn-template={{name}}.example.org</code></li> <li>Use a full annotation</li> </ol> <p>If you go with #1, you just need to define the headless service, here is an example of the case #2:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: ksvc\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname:  example.org\nspec:\n  ports:\n  - port: 9092\n    name: external\n  clusterIP: None\n  selector:\n    component: kafka\n</code></pre> <p>This will create 3 dns records:</p> <pre><code>kafka-0.example.org\nkafka-1.example.org\nkafka-2.example.org\n</code></pre> <p>If you set <code>--fqdn-template={{name}}.example.org</code> you can omit the annotation. Generally it is a better approach to use  <code>--fqdn-template={{name}}.example.org</code>, because then you would get the service name inside the generated A records:</p> <pre><code>kafka-0.ksvc.example.org\nkafka-1.ksvc.example.org\nkafka-2.ksvc.example.org\n</code></pre>"},{"location":"docs/tutorials/hostport/#using-pods-hostips-as-targets","title":"Using pods\u2019 HostIPs as targets","text":"<p>Add the following annotation to your <code>Service</code>:</p> <pre><code>external-dns.alpha.kubernetes.io/endpoints-type: HostIP\n</code></pre> <p>external-dns will now publish the value of the <code>.status.hostIP</code> field of the pods backing your <code>Service</code>.</p>"},{"location":"docs/tutorials/hostport/#using-node-external-ips-as-targets","title":"Using node external IPs as targets","text":"<p>Add the following annotation to your <code>Service</code>:</p> <pre><code>external-dns.alpha.kubernetes.io/endpoints-type: NodeExternalIP\n</code></pre> <p>external-dns will now publish the node external IP (<code>.status.addresses</code> entries of with <code>type: NodeExternalIP</code>) of the nodes on which the pods backing your <code>Service</code> are running.</p>"},{"location":"docs/tutorials/hostport/#using-pod-annotations-to-specify-target-ips","title":"Using pod annotations to specify target IPs","text":"<p>Add the following annotation to the pods backing your <code>Service</code>:</p> <pre><code>external-dns.alpha.kubernetes.io/target: \"1.2.3.4\"\n</code></pre> <p>external-dns will publish the IP specified in the annotation of each pod instead of using the podIP advertised by Kubernetes.</p> <p>This can be useful e.g. if you are NATing public IPs onto your pod IPs and want to publish these in DNS.</p>"},{"location":"docs/tutorials/ibmcloud/","title":"IBMCloud","text":"<p>This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using IBMCloud DNS.</p> <p>This tutorial uses IBMCloud CLI for all IBM Cloud commands and assumes that the Kubernetes cluster was created via IBM Cloud Kubernetes Service and <code>kubectl</code> commands are being run on an orchestration node.</p>"},{"location":"docs/tutorials/ibmcloud/#creating-a-ibmcloud-dns-zone","title":"Creating a IBMCloud DNS zone","text":"<p>The IBMCloud provider for ExternalDNS will find suitable zones for domains it manages; it will not automatically create zones. For public zone, This tutorial assume that the IBMCloud Internet Services was provisioned and the cis cli plugin was installed with IBMCloud CLI For private zone, This tutorial assume that the IBMCloud DNS Services was provisioned and the dns cli plugin was installed with IBMCloud CLI</p>"},{"location":"docs/tutorials/ibmcloud/#public-zone","title":"Public Zone","text":"<p>For this tutorial, we create public zone named <code>example.com</code> on IBMCloud Internet Services instance <code>external-dns-public</code></p> <pre><code>ibmcloud cis domain-add example.com -i external-dns-public\n</code></pre> <p>Follow step to active your zone</p>"},{"location":"docs/tutorials/ibmcloud/#private-zone","title":"Private Zone","text":"<p>For this tutorial, we create private zone named <code>example.com</code> on IBMCloud DNS Services instance <code>external-dns-private</code></p> <pre><code>ibmcloud dns zone-create example.com -i external-dns-private\n</code></pre>"},{"location":"docs/tutorials/ibmcloud/#creating-configuration-file","title":"Creating configuration file","text":"<p>The preferred way to inject the configuration file is by using a Kubernetes secret. The secret should contain an object named azure.json with content similar to this:</p> <pre><code>{\n  \"apiKey\": \"1234567890abcdefghijklmnopqrstuvwxyz\",\n  \"instanceCrn\": \"crn:v1:bluemix:public:internet-svcs:global:a/bcf1865e99742d38d2d5fc3fb80a5496:b950da8a-5be6-4691-810e-36388c77b0a3::\"\n}\n</code></pre> <p>You can create or find the <code>apiKey</code> in your ibmcloud IAM \u2192 API Keys page</p> <p>You can find the <code>instanceCrn</code> in your service instance details</p> <p>Now you can create a file named \u2018ibmcloud.json\u2019 with values gathered above and with the structure of the example above. Use this file to create a Kubernetes secret:</p> <pre><code>kubectl create secret generic ibmcloud-config-file --from-file=/local/path/to/ibmcloud.json\n</code></pre>"},{"location":"docs/tutorials/ibmcloud/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Connect your <code>kubectl</code> client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.</p>"},{"location":"docs/tutorials/ibmcloud/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=ibmcloud\n        - --ibmcloud-proxied # (optional) enable the proxy feature of IBMCloud\n        volumeMounts:\n        - name: ibmcloud-config-file\n          mountPath: /etc/kubernetes\n          readOnly: true\n      volumes:\n      - name: ibmcloud-config-file\n        secret:\n          secretName: ibmcloud-config-file\n          items:\n          - key: externaldns-config.json\n            path: ibmcloud.json\n</code></pre>"},{"location":"docs/tutorials/ibmcloud/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\", \"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=ibmcloud\n        - --ibmcloud-proxied # (optional) enable the proxy feature of IBMCloud public zone\n        volumeMounts:\n        - name: ibmcloud-config-file\n          mountPath: /etc/kubernetes\n          readOnly: true\n      volumes:\n      - name: ibmcloud-config-file\n        secret:\n          secretName: ibmcloud-config-file\n          items:\n          - key: externaldns-config.json\n            path: ibmcloud.json\n</code></pre>"},{"location":"docs/tutorials/ibmcloud/#deploying-an-nginx-service","title":"Deploying an Nginx Service","text":"<p>Create a service file called <code>nginx.yaml</code> with the following contents:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: www.example.com\n    external-dns.alpha.kubernetes.io/ttl: \"120\" #optional\nspec:\n  selector:\n    app: nginx\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre> <p>Note the annotation on the service; use the hostname as the IBMCloud DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019).</p> <p>By setting the TTL annotation on the service, you have to pass a valid TTL, which must be 120 or above. This annotation is optional, if you won\u2019t set it, it will be 1 (automatic) which is 300.</p> <p>ExternalDNS uses this annotation to determine what services should be registered with DNS.  Removing the annotation will cause ExternalDNS to remove the corresponding DNS records.</p> <p>Create the deployment and service:</p> <pre><code>kubectl create -f nginx.yaml\n</code></pre> <p>Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service.</p> <p>Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the IBMCloud DNS records.</p>"},{"location":"docs/tutorials/ibmcloud/#verifying-ibmcloud-dns-records","title":"Verifying IBMCloud DNS records","text":"<p>Run the following command to view the A records:</p>"},{"location":"docs/tutorials/ibmcloud/#public-zone_1","title":"Public Zone","text":"<pre><code># Get the domain ID with below command on IBMCloud Internet Services instance `external-dns-public`\n$ ibmcloud cis domains -i external-dns-public\n# Get the records with domain ID\n$ ibmcloud cis dns-records DOMAIN_ID  -i external-dns-public\n</code></pre>"},{"location":"docs/tutorials/ibmcloud/#private-zone_1","title":"Private Zone","text":"<pre><code># Get the domain ID with below command on IBMCloud DNS Services instance `external-dns-private`\n$ ibmcloud dns zones -i external-dns-private\n# Get the records with domain ID\n$ ibmcloud dns resource-records ZONE_ID  -i external-dns-public\n</code></pre> <p>This should show the external IP address of the service as the A record for your domain.</p>"},{"location":"docs/tutorials/ibmcloud/#cleanup","title":"Cleanup","text":"<p>Now that we have verified that ExternalDNS will automatically manage IBMCloud DNS records, we can delete the tutorial\u2019s example:</p> <pre><code>kubectl delete -f nginx.yaml\nkubectl delete -f externaldns.yaml\n</code></pre>"},{"location":"docs/tutorials/ibmcloud/#setting-proxied-records-on-public-zone","title":"Setting proxied records on public zone","text":"<p>Using the <code>external-dns.alpha.kubernetes.io/ibmcloud-proxied: \"true\"</code> annotation on your ingress or service, you can specify if the proxy feature of IBMCloud public DNS should be enabled for that record. This setting will override the global <code>--ibmcloud-proxied</code> setting.</p>"},{"location":"docs/tutorials/ibmcloud/#active-priviate-zone-with-vpc-allocated","title":"Active priviate zone with VPC allocated","text":"<p>By default, IBMCloud DNS Services don\u2019t active your private zone with new zone added. With External DNS, you can use <code>external-dns.alpha.kubernetes.io/ibmcloud-vpc: \"crn:v1:bluemix:public:is:us-south:a/bcf1865e99742d38d2d5fc3fb80a5496::vpc:r006-74353823-a60d-42e4-97c5-5e2551278435\"</code> annotation on your ingress or service. It will active your private zone with in specific VPC for that record created in. This setting won\u2019t work if the private zone was active already.</p> <p>Note: the annotaion value is the VPC CRN, every IBM Cloud service have a valid CRN.</p>"},{"location":"docs/tutorials/kops-dns-controller/","title":"kOps dns-controller","text":"<p>kOps includes a dns-controller that is primarily used to bootstrap the cluster, but can also be used for provisioning DNS entries for Services and Ingress.</p> <p>ExternalDNS can be used as a drop-in replacement for dns-controller if you are running a non-gossip cluster. The flag <code>--compatibility kops-dns-controller</code> enables the dns-controller behaviour.</p>"},{"location":"docs/tutorials/kops-dns-controller/#annotations","title":"Annotations","text":"<p>In kops-dns-controller compatibility mode, ExternalDNS supports two additional annotations:</p> <ul> <li> <p><code>dns.alpha.kubernetes.io/external</code> which is used to define a DNS record for accessing the resource publicly (i.e. public IPs)</p> </li> <li> <p><code>dns.alpha.kubernetes.io/internal</code> which is used to define a DNS record for accessing the resource from outside the cluster but inside the cloud, i.e. it will typically use internal IPs for instances.</p> </li> </ul> <p>These annotations may both be comma-separated lists of names.</p>"},{"location":"docs/tutorials/kops-dns-controller/#dns-record-mappings","title":"DNS record mappings","text":"<p>The DNS record mappings try to \u201cdo the right thing\u201d, but what this means is different for each resource type.</p>"},{"location":"docs/tutorials/kops-dns-controller/#pods","title":"Pods","text":"<p>For the external annotation, ExternalDNS will map a Pod to the external IPs of the Node.</p> <p>For the internal annotation, ExternalDNS will map a Pod to the internal IPs of the Node.</p> <p>Annotations added to Pods will always result in an A record being created.</p>"},{"location":"docs/tutorials/kops-dns-controller/#services","title":"Services","text":"<ul> <li> <p>For a Service of Type=LoadBalancer, ExternalDNS looks at Status.LoadBalancer.Ingress. It will create CNAMEs to hostnames,   and A records for IP addresses. It will do this for both internal and external names</p> </li> <li> <p>For a Service of Type=NodePort, ExternalDNS will create A records for the Node\u2019s internal/external IP addresses, as appropriate.</p> </li> </ul>"},{"location":"docs/tutorials/kube-ingress-aws/","title":"kube-ingress-aws-controller","text":"<p>This tutorial describes how to use ExternalDNS with the kube-ingress-aws-controller.</p>"},{"location":"docs/tutorials/kube-ingress-aws/#setting-up-externaldns-and-kube-ingress-aws-controller","title":"Setting up ExternalDNS and kube-ingress-aws-controller","text":"<p>Follow the AWS tutorial to setup ExternalDNS for use in Kubernetes clusters running in AWS. Specify the <code>source=ingress</code> argument so that ExternalDNS will look for hostnames in Ingress objects. In addition, you may wish to limit which Ingress objects are used as an ExternalDNS source via the <code>ingress-class</code> argument, but this is not required.</p> <p>For help setting up the Kubernetes Ingress AWS Controller, that can create ALBs and NLBs, follow the Setup Guide.</p>"},{"location":"docs/tutorials/kube-ingress-aws/#optional-routegroup","title":"Optional RouteGroup","text":"<p>RouteGroup is a CRD, that enables you to do complex routing with Skipper.</p> <p>First, you have to apply the RouteGroup CRD to your cluster:</p> <pre><code>kubectl apply -f https://github.com/zalando/skipper/blob/HEAD/dataclients/kubernetes/deploy/apply/routegroups_crd.yaml\n</code></pre> <p>You have to grant all controllers: Skipper, kube-ingress-aws-controller and external-dns to read the routegroup resource and kube-ingress-aws-controller to update the status field of a routegroup. This depends on your RBAC policies, in case you use RBAC, you can use this for all 3 controllers:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: kube-ingress-aws-controller\nrules:\n- apiGroups:\n  - extensions\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - extensions\n  - networking.k8s.io\n  resources:\n  - ingresses/status\n  verbs:\n  - patch\n  - update\n- apiGroups:\n  - zalando.org\n  resources:\n  - routegroups\n  verbs:\n  - get\n  - list\n  - watch\n- apiGroups:\n  - zalando.org\n  resources:\n  - routegroups/status\n  verbs:\n  - patch\n  - update\n</code></pre> <p>See also current RBAC yaml files:</p> <ul> <li>kube-ingress-aws-controller</li> <li>skipper</li> <li>external-dns</li> </ul>"},{"location":"docs/tutorials/kube-ingress-aws/#deploy-an-example-application","title":"Deploy an example application","text":"<p>Create the following sample \u201cechoserver\u201d application to demonstrate how ExternalDNS works with ingress objects, that were created by kube-ingress-aws-controller.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echoserver\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: echoserver\n  template:\n    metadata:\n      labels:\n        app: echoserver\n    spec:\n      containers:\n      - image: gcr.io/google_containers/echoserver:1.4\n        imagePullPolicy: Always\n        name: echoserver\n        ports:\n        - containerPort: 8080\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: echoserver\nspec:\n  ports:\n    - port: 80\n      targetPort: 8080\n      protocol: TCP\n  type: ClusterIP\n  selector:\n    app: echoserver\n</code></pre> <p>Note that the Service object is of type <code>ClusterIP</code>, because we will target Skipper and do the HTTP routing in Skipper. We don\u2019t need a Service of type <code>LoadBalancer</code> here, since we will be using a shared skipper-ingress for all Ingress. Skipper use <code>hostNetwork</code> to be able to get traffic from AWS LoadBalancers EC2 network. ALBs or NLBs, will be created based on need and will be shared across all ingress as default.</p>"},{"location":"docs/tutorials/kube-ingress-aws/#ingress-examples","title":"Ingress examples","text":"<p>Create the following Ingress to expose the echoserver application to the Internet.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: echoserver\nspec:\n  ingressClassName: skipper\n  rules:\n  - host: echoserver.mycluster.example.org\n    http: &amp;echoserver_root\n      paths:\n      - path: /\n        backend:\n          service:\n            name: echoserver\n            port:\n              number: 80\n        pathType: Prefix\n  - host: echoserver.example.org\n    http: *echoserver_root\n</code></pre> <p>The above should result in the creation of an (ipv4) ALB in AWS which will forward traffic to skipper which will forward to the echoserver application.</p> <p>If the <code>--source=ingress</code> argument is specified, then ExternalDNS will create DNS records based on the hosts specified in ingress objects. The above example would result in two alias records (A and AAAA) being created for each of the domains: <code>echoserver.mycluster.example.org</code> and <code>echoserver.example.org</code>. All four records alias the ALB that is associated with the Ingress object. As the ALB is IPv4 only, the AAAA alias records have no effect.</p> <p>If you would like ExternalDNS to not create AAAA records at all, you can add the following command line parameter: <code>--exclude-record-types=AAAA</code>. Please be aware, this will disable AAAA record creation even for dualstack enabled load balancers.</p> <p>Note that the above example makes use of the YAML anchor feature to avoid having to repeat the http section for multiple hosts that use the exact same paths. If this Ingress object will only be fronting one backend Service, we might instead create the following:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: echoserver.mycluster.example.org, echoserver.example.org\n  name: echoserver\nspec:\n  ingressClassName: skipper\n  rules:\n  - http:\n      paths:\n      - path: /\n        backend:\n          service:\n            name: echoserver\n            port:\n              number: 80\n        pathType: Prefix\n</code></pre> <p>In the above example we create a default path that works for any hostname, and make use of the <code>external-dns.alpha.kubernetes.io/hostname</code> annotation to create multiple aliases for the resulting ALB.</p>"},{"location":"docs/tutorials/kube-ingress-aws/#nlbs","title":"NLBs","text":"<p>AWS has NLBs and kube-ingress-aws-controller is able to create NLBs instead of ALBs. The Kubernetes Ingress AWS controller supports the <code>zalando.org/aws-load-balancer-type</code> annotation (which defaults to <code>alb</code>) to determine this. If this annotation is set to <code>nlb</code> then ExternalDNS will create an NLB instead of an ALB.</p> <p>Example:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    zalando.org/aws-load-balancer-type: nlb\n  name: echoserver\nspec:\n  ingressClassName: skipper\n  rules:\n  - host: echoserver.example.org\n    http:\n      paths:\n      - path: /\n        backend:\n          service:\n            name: echoserver\n            port:\n              number: 80\n        pathType: Prefix\n</code></pre> <p>The above Ingress object will result in the creation of an NLB. A successful create, you can observe in the ingress <code>status</code> field, that is written by kube-ingress-aws-controller:</p> <pre><code>status:\n  loadBalancer:\n    ingress:\n    - hostname: kube-ing-lb-atedkrlml7iu-1681027139.$region.elb.amazonaws.com\n</code></pre> <p>ExternalDNS will create A and AAAA alias records for: <code>echoserver.example.org</code>. ExternalDNS will use these alias records to automatically maintain IP addresses of the NLB.</p>"},{"location":"docs/tutorials/kube-ingress-aws/#dualstack-load-balancers","title":"Dualstack Load Balancers","text":"<p>AWS supports both IPv4 and \u201cdualstack\u201d (both IPv4 and IPv6) interfaces for ALBs and NLBs. The Kubernetes Ingress AWS controller supports the <code>alb.ingress.kubernetes.io/ip-address-type</code> annotation (which defaults to <code>ipv4</code>) to determine this. ExternalDNS creates both A and AAAA alias DNS records by default, regardless of this annotation. It\u2019s possible to create only A records with the following command line parameter: <code>--exclude-record-types=AAAA</code></p> <p>Example:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    alb.ingress.kubernetes.io/ip-address-type: dualstack\n  name: echoserver\nspec:\n  ingressClassName: skipper\n  rules:\n  - host: echoserver.example.org\n    http:\n      paths:\n      - path: /\n        backend:\n          service:\n            name: echoserver\n            port:\n              number: 80\n        pathType: Prefix\n</code></pre> <p>The above Ingress object will result in the creation of an ALB with a dualstack interface.</p>"},{"location":"docs/tutorials/kube-ingress-aws/#routegroup-optional","title":"RouteGroup (optional)","text":"<p>Kube-ingress-aws-controller, Skipper and external-dns support RouteGroups. External-dns needs to be started with <code>--source=skipper-routegroup</code> parameter in order to work on RouteGroup objects.</p> <p>Here we can not show all RouteGroup capabilities, but we show one simple example with an application and a custom https redirect.</p> <pre><code>apiVersion: zalando.org/v1\nkind: RouteGroup\nmetadata:\n  name: my-route-group\nspec:\n  backends:\n  - name: my-backend\n    type: service\n    serviceName: my-service\n    servicePort: 80\n  - name: redirectShunt\n    type: shunt\n  defaultBackends:\n  - backendName: my-service\n  routes:\n  - pathSubtree: /\n  - pathSubtree: /\n    predicates:\n    - Header(\"X-Forwarded-Proto\", \"http\")\n    filters:\n    - redirectTo(302, \"https:\")\n    backends:\n    - redirectShunt\n</code></pre>"},{"location":"docs/tutorials/linode/","title":"Linode","text":"<p>This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using Linode DNS Manager.</p> <p>Make sure to use &gt;=0.5.5 version of ExternalDNS for this tutorial.</p>"},{"location":"docs/tutorials/linode/#managing-dns-with-linode","title":"Managing DNS with Linode","text":"<p>If you want to learn about how to use Linode DNS Manager read the following tutorials:</p> <p>An Introduction to Managing DNS, and general documentation</p>"},{"location":"docs/tutorials/linode/#creating-linode-credentials","title":"Creating Linode Credentials","text":"<p>Generate a new oauth token by following the instructions at Access-and-Authentication</p> <p>The environment variable <code>LINODE_TOKEN</code> will be needed to run ExternalDNS with Linode.</p>"},{"location":"docs/tutorials/linode/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Connect your <code>kubectl</code> client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.</p>"},{"location":"docs/tutorials/linode/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=linode\n        env:\n        - name: LINODE_TOKEN\n          value: \"YOUR_LINODE_API_KEY\"\n</code></pre>"},{"location":"docs/tutorials/linode/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=linode\n        env:\n        - name: LINODE_TOKEN\n          value: \"YOUR_LINODE_API_KEY\"\n</code></pre>"},{"location":"docs/tutorials/linode/#deploying-an-nginx-service","title":"Deploying an Nginx Service","text":"<p>Create a service file called \u2018nginx.yaml\u2019 with the following contents:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: my-app.example.com\nspec:\n  selector:\n    app: nginx\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre> <p>Note the annotation on the service; use the same hostname as the Linode DNS zone created above.</p> <p>ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records.</p> <p>Create the deployment and service:</p> <pre><code>kubectl create -f nginx.yaml\n</code></pre> <p>Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service.</p> <p>Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Linode DNS records.</p>"},{"location":"docs/tutorials/linode/#verifying-linode-dns-records","title":"Verifying Linode DNS records","text":"<p>Check your Linode UI to view the records for your Linode DNS zone.</p> <p>Click on the zone for the one created above if a different domain was used.</p> <p>This should show the external IP address of the service as the A record for your domain.</p>"},{"location":"docs/tutorials/linode/#cleanup","title":"Cleanup","text":"<p>Now that we have verified that ExternalDNS will automatically manage Linode DNS records, we can delete the tutorial\u2019s example:</p> <pre><code>kubectl delete service -f nginx.yaml\nkubectl delete service -f externaldns.yaml\n</code></pre>"},{"location":"docs/tutorials/ns1/","title":"NS1","text":"<p>This tutorial describes how to setup ExternalDNS for use within a Kubernetes cluster using NS1 DNS.</p> <p>Make sure to use &gt;=0.5 version of ExternalDNS for this tutorial.</p>"},{"location":"docs/tutorials/ns1/#creating-a-zone-with-ns1-dns","title":"Creating a zone with NS1 DNS","text":"<p>If you are new to NS1, we recommend you first read the following instructions for creating a zone.</p> <p>Creating a zone using the NS1 portal</p> <p>Creating a zone using the NS1 API</p>"},{"location":"docs/tutorials/ns1/#creating-ns1-credentials","title":"Creating NS1 Credentials","text":"<p>All NS1 products are API-first, meaning everything that can be done on the portal\u2014including managing zones and records, data sources and feeds, and account settings and users\u2014can be done via API.</p> <p>The NS1 API is a standard REST API with JSON responses. The environment var <code>NS1_APIKEY</code> will be needed to run ExternalDNS with NS1.</p>"},{"location":"docs/tutorials/ns1/#to-add-or-delete-an-api-key","title":"To add or delete an API key","text":"<ol> <li> <p>Log into the NS1 portal at my.nsone.net.</p> </li> <li> <p>Click your username in the upper-right corner, and navigate to Account Settings &gt; Users &amp; Teams.</p> </li> <li> <p>Navigate to the API Keys tab, and click Add Key.</p> </li> <li> <p>Enter the name of the application and modify permissions and settings as desired. Once complete, click Create Key. The new API key appears in the list.</p> </li> </ol> <p>[!NOTE] Set the permissions for your API keys just as you would for a user or team associated with your organization\u2019s NS1 account. For more information, refer to the article Creating and Managing API Keys in the NS1 Knowledge Base.</p>"},{"location":"docs/tutorials/ns1/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Connect your <code>kubectl</code> client to the cluster with which you want to test ExternalDNS, and then apply one of the following manifest files for deployment:</p> <p>Begin by creating a Kubernetes secret to securely store your NS1 API key. This key will enable ExternalDNS to authenticate with NS1:</p> <pre><code>kubectl create secret generic NS1_APIKEY --from-literal=NS1_API_KEY=YOUR_NS1_API_KEY\n</code></pre> <p>Ensure to replace YOUR_NS1_API_KEY with your actual NS1 API key.</p> <p>Then apply one of the following manifests file to deploy ExternalDNS.</p>"},{"location":"docs/tutorials/ns1/#using-helm","title":"Using Helm","text":"<p>Create a values.yaml file to configure ExternalDNS to use NS1 as the DNS provider. This file should include the necessary environment variables:</p> <pre><code>provider:\n  name: ns1\nenv:\n  - name: NS1_APIKEY\n    valueFrom:\n      secretKeyRef:\n        name: NS1_APIKEY\n        key: NS1_API_KEY\n</code></pre> <p>Finally, install the ExternalDNS chart with Helm using the configuration specified in your values.yaml file:</p> <pre><code>helm upgrade --install external-dns external-dns/external-dns --values values.yaml\n</code></pre>"},{"location":"docs/tutorials/ns1/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=ns1\n        env:\n       - name: NS1_APIKEY\n          valueFrom:\n            secretKeyRef:\n              name: NS1_APIKEY\n              key: NS1_API_KEY\n</code></pre>"},{"location":"docs/tutorials/ns1/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=ns1\n        env:\n       - name: NS1_APIKEY\n          valueFrom:\n            secretKeyRef:\n              name: NS1_APIKEY\n              key: NS1_API_KEY\n</code></pre>"},{"location":"docs/tutorials/ns1/#deploying-an-nginx-service","title":"Deploying an Nginx Service","text":"<p>Create a service file called \u2018nginx.yaml\u2019 with the following contents:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: example.com\n    external-dns.alpha.kubernetes.io/ttl: \"120\" #optional\nspec:\n  selector:\n    app: nginx\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre> <p>A note about annotations</p> <p>Verify that the annotation on the service uses the same hostname as the NS1 DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019).</p> <p>The TTL annotation can be used to configure the TTL on DNS records managed by ExternalDNS and is optional. If this annotation is not set, the TTL on records managed by ExternalDNS will default to 10.</p> <p>ExternalDNS uses the hostname annotation to determine which services should be registered with DNS. Removing the hostname annotation will cause ExternalDNS to remove the corresponding DNS records.</p>"},{"location":"docs/tutorials/ns1/#create-the-deployment-and-service","title":"Create the deployment and service","text":"<pre><code>kubectl create -f nginx.yaml\n</code></pre> <p>Depending on where you run your service, it may take some time for your cloud provider to create an external IP for the service. Once an external IP is assigned, ExternalDNS detects the new service IP address and synchronizes the NS1 DNS records.</p>"},{"location":"docs/tutorials/ns1/#verifying-ns1-dns-records","title":"Verifying NS1 DNS records","text":"<p>Use the NS1 portal or API to verify that the A record for your domain shows the external IP address of the services.</p>"},{"location":"docs/tutorials/ns1/#cleanup","title":"Cleanup","text":"<p>Once you successfully configure and verify record management via ExternalDNS, you can delete the tutorial\u2019s example:</p> <pre><code>kubectl delete -f nginx.yaml\nkubectl delete -f externaldns.yaml\n</code></pre>"},{"location":"docs/tutorials/oracle/","title":"Oracle Cloud Infrastructure","text":"<p>This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using OCI DNS.</p> <p>Make sure to use the latest version of ExternalDNS for this tutorial.</p>"},{"location":"docs/tutorials/oracle/#creating-an-oci-dns-zone","title":"Creating an OCI DNS Zone","text":"<p>Create a DNS zone which will contain the managed DNS records. Let\u2019s use <code>example.com</code> as a reference here.  Make note of the OCID of the compartment in which you created the zone; you\u2019ll need to provide that later.</p> <p>For more information about OCI DNS see the documentation here.</p>"},{"location":"docs/tutorials/oracle/#using-private-oci-dns-zones","title":"Using Private OCI DNS Zones","text":"<p>By default, the ExternalDNS OCI provider is configured to use Global OCI DNS Zones. If you want to use Private OCI DNS Zones, add the following argument to the ExternalDNS controller:</p> <pre><code>--oci-zone-scope=PRIVATE\n</code></pre> <p>To use both Global and Private OCI DNS Zones, set the OCI Zone Scope to be empty:</p> <pre><code>--oci-zone-scope=\n</code></pre>"},{"location":"docs/tutorials/oracle/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Connect your <code>kubectl</code> client to the cluster you want to test ExternalDNS with. The OCI provider supports two authentication options: key-based and instance principals.</p>"},{"location":"docs/tutorials/oracle/#key-based","title":"Key-based","text":"<p>We first need to create a config file containing the information needed to connect with the OCI API.</p> <p>Create a new file (oci.yaml) and modify the contents to match the example below. Be sure to adjust the values to match your own credentials, and the OCID of the compartment containing the zone:</p> <pre><code>auth:\n  region: us-phoenix-1\n  tenancy: ocid1.tenancy.oc1...\n  user: ocid1.user.oc1...\n  key: |\n    -----BEGIN RSA PRIVATE KEY-----\n    -----END RSA PRIVATE KEY-----\n  fingerprint: af:81:71:8e...\n  # Omit if there is not a password for the key\n  passphrase: Tx1jRk...\ncompartment: ocid1.compartment.oc1...\n</code></pre> <p>Create a secret using the config file above:</p> <pre><code>kubectl create secret generic external-dns-config --from-file=oci.yaml\n</code></pre>"},{"location":"docs/tutorials/oracle/#oci-iam-instance-principal","title":"OCI IAM Instance Principal","text":"<p>If you\u2019re running ExternalDNS within OCI, you can use OCI IAM instance principals to authenticate with OCI.  This obviates the need to create the secret with your credentials.  You\u2019ll need to ensure an OCI IAM policy exists with a statement granting the <code>manage dns</code> permission on zones and records in the target compartment to the dynamic group covering your instance running ExternalDNS. E.g.:</p> <pre><code>Allow dynamic-group &lt;dynamic-group-name&gt; to manage dns in compartment id &lt;target-compartment-OCID&gt;\n</code></pre> <p>You\u2019ll also need to add the <code>--oci-auth-instance-principal</code> flag to enable this type of authentication. Finally, you\u2019ll need to add the <code>--oci-compartment-ocid=ocid1.compartment.oc1...</code> flag to provide the OCID of the compartment containing the zone to be managed.</p> <p>For more information about OCI IAM instance principals, see the documentation here. For more information about OCI IAM policy details for the DNS service, see the documentation here.</p>"},{"location":"docs/tutorials/oracle/#oci-iam-workload-identity","title":"OCI IAM Workload Identity","text":"<p>If you\u2019re running ExternalDNS within an OCI Container Engine for Kubernetes (OKE) cluster, you can use OCI IAM Workload Identity to authenticate with OCI. You\u2019ll need to ensure an OCI IAM policy exists with a statement granting the <code>manage dns</code> permission on zones and records in the target compartment covering your OKE cluster running ExternalDNS. E.g.:</p> <pre><code>Allow any-user to manage dns in compartment &lt;compartment-name&gt; where all {request.principal.type='workload',request.principal.cluster_id='&lt;cluster-ocid&gt;',request.principal.service_account='external-dns'}\n</code></pre> <p>You\u2019ll also need to create a new file (oci.yaml) and modify the contents to match the example below. Be sure to adjust the values to match your region and the OCID of the compartment containing the zone:</p> <pre><code>auth:\n  region: us-phoenix-1\n  useWorkloadIdentity: true\ncompartment: ocid1.compartment.oc1...\n</code></pre> <p>Create a secret using the config file above:</p> <pre><code>kubectl create secret generic external-dns-config --from-file=oci.yaml\n</code></pre>"},{"location":"docs/tutorials/oracle/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<p>Apply the following manifest to deploy ExternalDNS.</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service\n        - --source=ingress\n        - --provider=oci\n        - --policy=upsert-only # prevent ExternalDNS from deleting any records, omit to enable full synchronization\n        - --txt-owner-id=my-identifier\n        # Specifies the OCI DNS Zone scope, defaults to GLOBAL.\n        # May be GLOBAL, PRIVATE, or an empty value to specify both GLOBAL and PRIVATE OCI DNS Zones\n        # - --oci-zone-scope=GLOBAL\n        # Specifies the zone cache duration, defaults to 0s. If set to 0s, the zone cache is disabled.\n        # Use of zone caching is recommended to reduce the amount of requests sent to OCI DNS.\n        # - --oci-zones-cache-duration=0s\n        volumeMounts:\n          - name: config\n            mountPath: /etc/kubernetes/\n      volumes:\n      - name: config\n        secret:\n          secretName: external-dns-config\n</code></pre>"},{"location":"docs/tutorials/oracle/#verify-externaldns-works-service-example","title":"Verify ExternalDNS works (Service example)","text":"<p>Create the following sample application to test that ExternalDNS works.</p> <p>For services ExternalDNS will look for the annotation <code>external-dns.alpha.kubernetes.io/hostname</code> on the service and use the corresponding value.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: example.com\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    name: http\n    targetPort: 80\n  selector:\n    app: nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n          name: http\n</code></pre> <p>Apply the manifest above and wait roughly two minutes and check that a corresponding DNS record for your service was created.</p> <pre><code>kubectl apply -f nginx.yaml\n</code></pre>"},{"location":"docs/tutorials/ovh/","title":"OVHcloud","text":"<p>This tutorial describes how to setup ExternalDNS for use within a Kubernetes cluster using OVH DNS.</p> <p>Make sure to use &gt;=0.6 version of ExternalDNS for this tutorial.</p>"},{"location":"docs/tutorials/ovh/#creating-a-zone-with-ovh-dns","title":"Creating a zone with OVH DNS","text":"<p>If you are new to OVH, we recommend you first read the following instructions for creating a zone.</p> <p>Creating a zone using the OVH manager</p> <p>Creating a zone using the OVH API</p>"},{"location":"docs/tutorials/ovh/#creating-ovh-credentials","title":"Creating OVH Credentials","text":"<p>You first need to create an OVH application.</p> <p>Using the OVH documentation you will have your <code>Application key</code> and <code>Application secret</code></p> <p>And you will need to generate your consumer key, here the permissions needed :</p> <ul> <li>GET on <code>/domain/zone</code></li> <li>GET on <code>/domain/zone/*/record</code></li> <li>GET on <code>/domain/zone/*/record/*</code></li> <li>POST on <code>/domain/zone/*/record</code></li> <li>DELETE on <code>/domain/zone/*/record/*</code></li> <li>GET on <code>/domain/zone/*/soa</code></li> <li>POST on <code>/domain/zone/*/refresh</code></li> </ul> <p>You can use the following <code>curl</code> request to generate &amp; validated your <code>Consumer key</code></p> <pre><code>curl -XPOST -H \"X-Ovh-Application: &lt;ApplicationKey&gt;\" -H \"Content-type: application/json\" https://eu.api.ovh.com/1.0/auth/credential -d '{\n  \"accessRules\": [\n    {\n      \"method\": \"GET\",\n      \"path\": \"/domain/zone\"\n    },\n    {\n      \"method\": \"GET\",\n      \"path\": \"/domain/zone/*/soa\"\n    },\n    {\n      \"method\": \"GET\",\n      \"path\": \"/domain/zone/*/record\"\n    },\n    {\n      \"method\": \"GET\",\n      \"path\": \"/domain/zone/*/record/*\"\n    },\n    {\n      \"method\": \"POST\",\n      \"path\": \"/domain/zone/*/record\"\n    },\n    {\n      \"method\": \"DELETE\",\n      \"path\": \"/domain/zone/*/record/*\"\n    },\n    {\n      \"method\": \"POST\",\n      \"path\": \"/domain/zone/*/refresh\"\n    }\n  ],\n  \"redirection\":\"https://github.com/kubernetes-sigs/external-dns/blob/HEAD/docs/tutorials/ovh.md#creating-ovh-credentials\"\n}'\n</code></pre>"},{"location":"docs/tutorials/ovh/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Connect your <code>kubectl</code> client to the cluster with which you want to test ExternalDNS, and then apply one of the following manifest files for deployment:</p>"},{"location":"docs/tutorials/ovh/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=ovh\n        env:\n        - name: OVH_APPLICATION_KEY\n          value: \"YOUR_OVH_APPLICATION_KEY\"\n        - name: OVH_APPLICATION_SECRET\n          value: \"YOUR_OVH_APPLICATION_SECRET\"\n        - name: OVH_CONSUMER_KEY\n          value: \"YOUR_OVH_CONSUMER_KEY_AFTER_VALIDATED_LINK\"\n</code></pre>"},{"location":"docs/tutorials/ovh/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"endpoints\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=ovh\n        env:\n        - name: OVH_APPLICATION_KEY\n          value: \"YOUR_OVH_APPLICATION_KEY\"\n        - name: OVH_APPLICATION_SECRET\n          value: \"YOUR_OVH_APPLICATION_SECRET\"\n        - name: OVH_CONSUMER_KEY\n          value: \"YOUR_OVH_CONSUMER_KEY_AFTER_VALIDATED_LINK\"\n</code></pre>"},{"location":"docs/tutorials/ovh/#deploying-an-nginx-service","title":"Deploying an Nginx Service","text":"<p>Create a service file called \u2018nginx.yaml\u2019 with the following contents:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: example.com\n    external-dns.alpha.kubernetes.io/ttl: \"120\" #optional\nspec:\n  selector:\n    app: nginx\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre> <p>A note about annotations</p> <p>Verify that the annotation on the service uses the same hostname as the OVH DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019).</p> <p>The TTL annotation can be used to configure the TTL on DNS records managed by ExternalDNS and is optional. If this annotation is not set, the TTL on records managed by ExternalDNS will default to 10.</p> <p>ExternalDNS uses the hostname annotation to determine which services should be registered with DNS. Removing the hostname annotation will cause ExternalDNS to remove the corresponding DNS records.</p>"},{"location":"docs/tutorials/ovh/#create-the-deployment-and-service","title":"Create the deployment and service","text":"<pre><code>kubectl create -f nginx.yaml\n</code></pre> <p>Depending on where you run your service, it may take some time for your cloud provider to create an external IP for the service. Once an external IP is assigned, ExternalDNS detects the new service IP address and synchronizes the OVH DNS records.</p>"},{"location":"docs/tutorials/ovh/#verifying-ovh-dns-records","title":"Verifying OVH DNS records","text":"<p>Use the OVH manager or API to verify that the A record for your domain shows the external IP address of the services.</p>"},{"location":"docs/tutorials/ovh/#cleanup","title":"Cleanup","text":"<p>Once you successfully configure and verify record management via ExternalDNS, you can delete the tutorial\u2019s example:</p> <pre><code>kubectl delete -f nginx.yaml\nkubectl delete -f externaldns.yaml\n</code></pre>"},{"location":"docs/tutorials/pdns/","title":"PowerDNS","text":""},{"location":"docs/tutorials/pdns/#prerequisites","title":"Prerequisites","text":"<p>The provider has been written for and tested against PowerDNS v4.1.x and thus requires PowerDNS Auth Server &gt;= 4.1.x</p> <p>PowerDNS provider support was added via this PR, thus you need to use external-dns version &gt;= v0.5</p> <p>The PDNS provider expects that your PowerDNS instance is already setup and functional. It expects that zones, you wish to add records to, already exist and are configured correctly. It does not add, remove or configure new zones in anyway.</p>"},{"location":"docs/tutorials/pdns/#feature-support","title":"Feature Support","text":"<p>The PDNS provider currently does not support:</p> <ul> <li>Dry running a configuration is not supported</li> </ul>"},{"location":"docs/tutorials/pdns/#deployment","title":"Deployment","text":"<p>Deploying external DNS for PowerDNS is actually nearly identical to deploying it for other providers. This is what a sample <code>deployment.yaml</code> looks like:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      # Only use if you're also using RBAC\n      # serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # or ingress or both\n        - --provider=pdns\n        - --pdns-server={{ pdns-api-url }}\n        - --pdns-server-id={{ pdns-server-id }}\n        - --pdns-api-key={{ pdns-http-api-key }}\n        - --txt-owner-id={{ owner-id-for-this-external-dns }}\n        - --domain-filter=external-dns-test.my-org.com # will make ExternalDNS see only the zones matching provided domain; omit to process all available zones in PowerDNS\n        - --log-level=debug\n        - --interval=30s\n</code></pre>"},{"location":"docs/tutorials/pdns/#domain-filter-domain-filter","title":"Domain Filter (<code>--domain-filter</code>)","text":"<p>When the <code>--domain-filter</code> argument is specified, external-dns will only create DNS records for host names (specified in ingress objects and services with the external-dns annotation) related to zones that match the <code>--domain-filter</code> argument in the external-dns deployment manifest.</p> <p>eg. <code>--domain-filter=example.org</code> will allow for zone <code>example.org</code> and any zones in PowerDNS that ends in <code>.example.org</code>, including <code>an.example.org</code>, ie. the subdomains of example.org.</p> <p>eg. <code>--domain-filter=.example.org</code> will allow only zones that end in <code>.example.org</code>, ie. the subdomains of example.org but not the <code>example.org</code> zone itself.</p> <p>The filter can also match parent zones. For example <code>--domain-filter=a.example.com</code> will allow for zone <code>example.com</code>. If you want to match parent zones, you cannot pre-pend your filter with a \u201c.\u201d, eg. <code>--domain-filter=.example.com</code> will not attempt to match parent zones.</p>"},{"location":"docs/tutorials/pdns/#regex-domain-filter-regex-domain-filter","title":"Regex Domain Filter (<code>--regex-domain-filter</code>)","text":"<p><code>--regex-domain-filter</code> limits possible domains and target zone with a regex. It overrides domain filters and can be specified only once.</p>"},{"location":"docs/tutorials/pdns/#rbac","title":"RBAC","text":"<p>If your cluster is RBAC enabled, you also need to setup the following, before you can run external-dns:</p> <pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n</code></pre>"},{"location":"docs/tutorials/pdns/#testing-and-verification","title":"Testing and Verification","text":"<p>Important!: Remember to change <code>example.com</code> with your own domain throughout the following text.</p> <p>Spin up a simple \u201cHello World\u201d HTTP server with the following spec (<code>kubectl apply -f</code>):</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo\nspec:\n  selector:\n    matchLabels:\n      app: echo\n  template:\n    metadata:\n      labels:\n        app: echo\n    spec:\n      containers:\n      - image: hashicorp/http-echo\n        name: echo\n        ports:\n        - containerPort: 5678\n        args:\n          - -text=\"Hello World\"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: echo\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: echo.example.com\nspec:\n  selector:\n    app: echo\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 5678\n</code></pre> <p>Important!: Don\u2019t run dig, nslookup or similar immediately (until you\u2019ve confirmed the record exists). You\u2019ll get hit by negative DNS caching, which is hard to flush.</p> <p>Run the following to make sure everything is in order:</p> <pre><code>kubectl get services echo\nkubectl get endpoints echo\n</code></pre> <p>Make sure everything looks correct, i.e the service is defined and receives a public IP, and that the endpoint also has a pod IP.</p> <p>Once that\u2019s done, wait about 30s-1m (interval for external-dns to kick in), then do:</p> <pre><code>curl -H \"X-API-Key: ${PDNS_API_KEY}\" ${PDNS_API_URL}/api/v1/servers/localhost/zones/example.com. | jq '.rrsets[] | select(.name | contains(\"echo\"))'\n</code></pre> <p>Once the API shows the record correctly, you can double check your record using:</p> <pre><code>dig @${PDNS_FQDN} echo.example.com.\n</code></pre>"},{"location":"docs/tutorials/pdns/#using-crd-source-to-manage-dns-records-in-powerdns","title":"Using CRD source to manage DNS records in PowerDNS","text":"<p>Please refer to the CRD source documentation for more information.</p>"},{"location":"docs/tutorials/pihole/","title":"Pi-hole","text":"<p>This tutorial describes how to setup ExternalDNS to sync records with Pi-hole\u2019s Custom DNS. Pi-hole has an internal list it checks last when resolving requests. This list can contain any number of arbitrary A, AAAA or CNAME records. There is a pseudo-API exposed that ExternalDNS is able to use to manage these records.</p> <p>NOTE: Your Pi-hole must be running version 5.9 or newer.</p>"},{"location":"docs/tutorials/pihole/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>You can skip to the manifest if authentication is disabled on your Pi-hole instance or you don\u2019t want to use secrets.</p> <p>If your Pi-hole server\u2019s admin dashboard is protected by a password, you\u2019ll likely want to create a secret first containing its value. This is optional since you do retain the option to pass it as a flag with <code>--pihole-password</code>.</p> <p>You can create the secret with:</p> <pre><code>kubectl create secret generic pihole-password \\\n    --from-literal EXTERNAL_DNS_PIHOLE_PASSWORD=supersecret\n</code></pre> <p>Replacing \u201csupersecret\u201d with the actual password to your Pi-hole server.</p>"},{"location":"docs/tutorials/pihole/#externaldns-manifest","title":"ExternalDNS Manifest","text":"<p>Apply the following manifest to deploy ExternalDNS, editing values for your environment accordingly. Be sure to change the namespace in the <code>ClusterRoleBinding</code> if you are using a namespace other than default.</p> <pre><code>---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\",\"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        # If authentication is disabled and/or you didn't create\n        # a secret, you can remove this block.\n        envFrom:\n        - secretRef:\n            # Change this if you gave the secret a different name\n            name: pihole-password\n        args:\n        - --source=service\n        - --source=ingress\n        # Pihole only supports A/AAAA/CNAME records so there is no mechanism to track ownership.\n        # You don't need to set this flag, but if you leave it unset, you will receive warning\n        # logs when ExternalDNS attempts to create TXT records.\n        - --registry=noop\n        # IMPORTANT: If you have records that you manage manually in Pi-hole, set\n        # the policy to upsert-only so they do not get deleted.\n        - --policy=upsert-only\n        - --provider=pihole\n        # Change this to the actual address of your Pi-hole web server\n        - --pihole-server=http://pihole-web.pihole.svc.cluster.local\n      securityContext:\n        fsGroup: 65534 # For ExternalDNS to be able to read Kubernetes token files\n</code></pre>"},{"location":"docs/tutorials/pihole/#arguments","title":"Arguments","text":"<ul> <li><code>--pihole-server (env: EXTERNAL_DNS_PIHOLE_SERVER)</code> - The address of the Pi-hole web server</li> <li><code>--pihole-password (env: EXTERNAL_DNS_PIHOLE_PASSWORD)</code> - The password to the Pi-hole web server (if enabled)</li> <li><code>--pihole-tls-skip-verify (env: EXTERNAL_DNS_PIHOLE_TLS_SKIP_VERIFY)</code> - Skip verification of any TLS certificates served by the Pi-hole web server.</li> </ul>"},{"location":"docs/tutorials/pihole/#verify-externaldns-works","title":"Verify ExternalDNS Works","text":""},{"location":"docs/tutorials/pihole/#ingress-example","title":"Ingress Example","text":"<p>Create an Ingress resource. ExternalDNS will use the hostname specified in the Ingress object.</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: foo\nspec:\n  ingressClassName: nginx\n  rules:\n  - host: foo.bar.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: foo\n            port:\n              number: 80\n</code></pre>"},{"location":"docs/tutorials/pihole/#service-example","title":"Service Example","text":"<p>The below sample application can be used to verify Services work. For services ExternalDNS will look for the annotation <code>external-dns.alpha.kubernetes.io/hostname</code> on the service and use the corresponding value.</p> <pre><code>---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: nginx.external-dns-test.homelab.com\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    name: http\n    targetPort: 80\n  selector:\n    app: nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n          name: http\n</code></pre> <p>You can then query your Pi-hole to see if the record was created.</p> <p>Change <code>@192.168.100.2</code> to the actual address of your DNS server</p> <pre><code>$ dig +short @192.168.100.2  nginx.external-dns-test.homelab.com\n\n192.168.100.129\n</code></pre>"},{"location":"docs/tutorials/plural/","title":"Plural","text":"<p>This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using Plural DNS.</p> <p>Make sure to use &gt;=0.12.3 version of ExternalDNS for this tutorial.</p>"},{"location":"docs/tutorials/plural/#creating-plural-credentials","title":"Creating Plural Credentials","text":"<p>A secret containing the a Plural access token is needed for this provider. You can get a token for your user here.</p> <p>To create the secret you can run <code>kubectl create secret generic plural-env --from-literal=PLURAL_ACCESS_TOKEN=&lt;replace-with-your-access-token&gt;</code>.</p>"},{"location":"docs/tutorials/plural/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Connect your <code>kubectl</code> client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.</p>"},{"location":"docs/tutorials/plural/#using-helm","title":"Using Helm","text":"<p>Create a values.yaml file to configure ExternalDNS to use plural DNS as the DNS provider. This file should include the necessary environment variables:</p> <pre><code>provider:\n  name: plural\nextraArgs:\n  - --plural-cluster=example-plural-cluster\n  - --plural-provider=aws # gcp, azure, equinix and kind are also possible\nenv:\n  - name: PLURAL_ACCESS_TOKEN\n    valueFrom:\n      secretKeyRef:\n        name: PLURAL_ACCESS_TOKEN\n        key: plural-env\n  - name: PLURAL_ENDPOINT\n    value: https://app.plural.sh\n</code></pre> <p>Finally, install the ExternalDNS chart with Helm using the configuration specified in your values.yaml file:</p> <pre><code>helm upgrade --install external-dns external-dns/external-dns --values values.yaml\n</code></pre>"},{"location":"docs/tutorials/plural/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=plural\n        - --plural-cluster=example-plural-cluster\n        - --plural-provider=aws # gcp, azure, equinix and kind are also possible\n        env:\n        - name: PLURAL_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: PLURAL_ACCESS_TOKEN\n              name: plural-env\n        - name: PLURAL_ENDPOINT # (optional) use an alternative endpoint for Plural; defaults to https://app.plural.sh\n          value: https://app.plural.sh\n</code></pre>"},{"location":"docs/tutorials/plural/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\", \"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=plural\n        - --plural-cluster=example-plural-cluster\n        - --plural-provider=aws # gcp, azure, equinix and kind are also possible\n        env:\n        - name: PLURAL_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: PLURAL_ACCESS_TOKEN\n              name: plural-env\n        - name: PLURAL_ENDPOINT # (optional) use an alternative endpoint for Plural; defaults to https://app.plural.sh\n          value: https://app.plural.sh\n</code></pre>"},{"location":"docs/tutorials/plural/#deploying-an-nginx-service","title":"Deploying an Nginx Service","text":"<p>Create a service file called \u2018nginx.yaml\u2019 with the following contents:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: example.com\nspec:\n  selector:\n    app: nginx\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre> <p>Note the annotation on the service; use the same hostname as the Plural DNS zone created above. The annotation may also be a subdomain of the DNS zone (e.g. \u2018www.example.com\u2019).</p> <p>By setting the TTL annotation on the service, you have to pass a valid TTL, which must be 120 or above. This annotation is optional, if you won\u2019t set it, it will be 1 (automatic) which is 300.</p> <p>ExternalDNS uses this annotation to determine what services should be registered with DNS.  Removing the annotation will cause ExternalDNS to remove the corresponding DNS records.</p> <p>Create the deployment and service:</p> <pre><code>kubectl create -f nginx.yaml\n</code></pre> <p>Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service.</p> <p>Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Plural DNS records.</p>"},{"location":"docs/tutorials/plural/#verifying-plural-dns-records","title":"Verifying Plural DNS records","text":"<p>Check your Plural domain overview to view the domains associated with your Plural account. There you can view the records for each domain.</p> <p>The records should show the external IP address of the service as the A record for your domain.</p>"},{"location":"docs/tutorials/plural/#cleanup","title":"Cleanup","text":"<p>Now that we have verified that ExternalDNS will automatically manage Plural DNS records, we can delete the tutorial\u2019s example:</p> <p>```sh kubectl delete -f nginx.yaml kubectl delete -f externaldns.yaml</p>"},{"location":"docs/tutorials/rfc2136/","title":"RFC2136 provider","text":"<p>This tutorial describes how to use the RFC2136 with either BIND or Windows DNS.</p>"},{"location":"docs/tutorials/rfc2136/#using-with-bind","title":"Using with BIND","text":"<p>To use external-dns with BIND: generate/procure a key, configure DNS and add a deployment of external-dns.</p>"},{"location":"docs/tutorials/rfc2136/#server-credentials","title":"Server credentials","text":"<ul> <li>RFC2136 was developed for and tested with BIND DNS server. This documentation assumes that you already have a configured and working server. If you don\u2019t, please check BIND documents or tutorials.</li> <li>If your DNS is provided for you, ask for a TSIG key authorized to update and transfer the zone you wish to update. The key will look something like below. Skip the next steps wrt BIND setup.</li> </ul> <pre><code>key \"externaldns-key\" {\n algorithm hmac-sha256;\n secret \"96Ah/a2g0/nLeFGK+d/0tzQcccf9hCEIy34PoXX2Qg8=\";\n};\n</code></pre> <ul> <li>If you are your own DNS administrator create a TSIG key. Use <code>tsig-keygen -a hmac-sha256 externaldns</code> or on older distributions <code>dnssec-keygen -a HMAC-SHA256 -b 256 -n HOST externaldns</code>. You will end up with a key printed to standard out like above (or in the case of dnssec-keygen in a file called <code>Kexternaldns......key</code>).</li> </ul>"},{"location":"docs/tutorials/rfc2136/#bind-configuration","title":"BIND Configuration","text":"<p>If you do not administer your own DNS, skip to RFC provider configuration</p> <ul> <li>Edit your named.conf file (or appropriate included file) and add/change the following.</li> <li>Make sure You are listening on the right interfaces. At least whatever   interface external-dns will be communicating over and the interface that   faces the internet.</li> <li>Add the key that you generated/was given to you above. Copy paste the four   lines that you got (not the same as the example key) into your file.</li> <li>Create a zone for kubernetes. If you already have a zone, skip to the next   step. (I put the zone in it\u2019s own subdirectory because named,   which shouldn\u2019t be running as root, needs to create a journal file and the   default zone directory isn\u2019t writeable by named).</li> </ul> <pre><code>zone \"k8s.example.org\" {\n    type master;\n    file \"/etc/bind/pri/k8s/k8s.zone\";\n};\n</code></pre> <ul> <li>Add your key to both transfer and update. For instance with our previous   zone.</li> </ul> <pre><code>zone \"k8s.example.org\" {\n    type master;\n    file \"/etc/bind/pri/k8s/k8s.zone\";\n    allow-transfer {\n        key \"externaldns-key\";\n    };\n    update-policy {\n        grant externaldns-key zonesub ANY;\n    };\n};\n</code></pre> <ul> <li>Create a zone file (k8s.zone):</li> </ul> <pre><code>$TTL 60 ; 1 minute\nk8s.example.org         IN SOA  k8s.example.org. root.k8s.example.org. (\n                                16         ; serial\n                                60         ; refresh (1 minute)\n                                60         ; retry (1 minute)\n                                60         ; expire (1 minute)\n                                60         ; minimum (1 minute)\n                                )\n                        NS      ns.k8s.example.org.\nns                      A       123.456.789.012\n</code></pre> <ul> <li>Reload (or restart) named</li> </ul>"},{"location":"docs/tutorials/rfc2136/#using-external-dns","title":"Using external-dns","text":"<p>To use external-dns add an ingress or a LoadBalancer service with a host that is part of the domain-filter. For example both of the following would produce A records.</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: svc.example.org\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 80\n  selector:\n    app: nginx\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n    name: my-ingress\nspec:\n    rules:\n    - host: ingress.example.org\n      http:\n          paths:\n          - path: /\n            backend:\n                serviceName: my-service\n                servicePort: 8000\n</code></pre>"},{"location":"docs/tutorials/rfc2136/#custom-ttl","title":"Custom TTL","text":"<p>The default DNS record TTL (Time-To-Live) is 0 seconds. You can customize this value by setting the annotation <code>external-dns.alpha.kubernetes.io/ttl</code>. e.g., modify the service manifest YAML file above:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: nginx.external-dns-test.my-org.com\n    external-dns.alpha.kubernetes.io/ttl: 60\nspec:\n    ...\n</code></pre> <p>This will set the DNS record\u2019s TTL to 60 seconds.</p> <p>A default TTL for all records can be set using the the flag with a time in seconds, minutes or hours, such as <code>--rfc2136-min-ttl=60s</code></p> <p>There are other annotation that can affect the generation of DNS records, but these are beyond the scope of this tutorial and are covered in the main documentation.</p>"},{"location":"docs/tutorials/rfc2136/#generate-reverse-dns-records","title":"Generate reverse DNS records","text":"<p>If you want to generate reverse DNS records for your services, you have to enable the functionality using the <code>--rfc2136-create-ptr</code> flag. You have also to add the zone to the list of zones managed by ExternalDNS via the <code>--rfc2136-zone</code> and <code>--domain-filter</code> flags. An example of a valid configuration is the following:</p> <pre><code>--domain-filter=157.168.192.in-addr.arpa --rfc2136-zone=157.168.192.in-addr.arpa\n</code></pre> <p>PTR record tracking is managed by the A/AAAA record so you can\u2019t create PTR records for already generated A/AAAA records.</p>"},{"location":"docs/tutorials/rfc2136/#test-with-external-dns-installed-on-local-machine-optional","title":"Test with external-dns installed on local machine (optional)","text":"<p>You may install external-dns and test on a local machine by running:</p> <pre><code>external-dns --txt-owner-id k8s --provider rfc2136 \\\n  --rfc2136-host=192.168.0.1 --rfc2136-port=53 \\\n  --rfc2136-zone=k8s.example.org \\\n  --rfc2136-tsig-secret=96Ah/a2g0/nLeFGK+d/0tzQcccf9hCEIy34PoXX2Qg8= \\\n  --rfc2136-tsig-secret-alg=hmac-sha256 \\\n  --rfc2136-tsig-keyname=externaldns-key \\\n  --rfc2136-tsig-axfr \\\n  --source ingress --once \\\n  --domain-filter=k8s.example.org --dry-run\n</code></pre> <ul> <li>host should be the IP of your master DNS server.</li> <li>tsig-secret should be changed to match your secret.</li> <li>tsig-keyname needs to match the keyname you used (if you changed it).</li> <li>domain-filter can be used as shown to filter the domains you wish to update.</li> </ul>"},{"location":"docs/tutorials/rfc2136/#rfc2136-provider-configuration","title":"RFC2136 provider configuration","text":"<p>In order to use external-dns with your cluster you need to add a deployment with access to your ingress and service resources. The following are two example manifests with and without RBAC respectively.</p> <ul> <li>With RBAC:</li> </ul> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: external-dns\n  labels:\n    name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\n  namespace: external-dns\nrules:\n- apiGroups:\n  - \"\"\n  resources:\n  - services\n  - endpoints\n  - pods\n  - nodes\n  verbs:\n  - get\n  - watch\n  - list\n- apiGroups:\n  - extensions\n  - networking.k8s.io\n  resources:\n  - ingresses\n  verbs:\n  - get\n  - list\n  - watch\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n  namespace: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\n  namespace: external-dns\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: external-dns\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\n  namespace: external-dns\nspec:\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --registry=txt\n        - --txt-prefix=external-dns-\n        - --txt-owner-id=k8s\n        - --provider=rfc2136\n        - --rfc2136-host=192.168.0.1\n        - --rfc2136-port=53\n        - --rfc2136-zone=k8s.example.org\n        - --rfc2136-zone=k8s.your-zone.org\n        - --rfc2136-tsig-secret=96Ah/a2g0/nLeFGK+d/0tzQcccf9hCEIy34PoXX2Qg8=\n        - --rfc2136-tsig-secret-alg=hmac-sha256\n        - --rfc2136-tsig-keyname=externaldns-key\n        - --rfc2136-tsig-axfr\n        - --source=ingress\n        - --domain-filter=k8s.example.org\n</code></pre> <ul> <li>Without RBAC:</li> </ul> <pre><code>apiVersion: v1\nkind: Namespace\nmetadata:\n  name: external-dns\n  labels:\n    name: external-dns\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\n  namespace: external-dns\nspec:\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --registry=txt\n        - --txt-prefix=external-dns-\n        - --txt-owner-id=k8s\n        - --provider=rfc2136\n        - --rfc2136-host=192.168.0.1\n        - --rfc2136-port=53\n        - --rfc2136-zone=k8s.example.org\n        - --rfc2136-zone=k8s.your-zone.org\n        - --rfc2136-tsig-secret=96Ah/a2g0/nLeFGK+d/0tzQcccf9hCEIy34PoXX2Qg8=\n        - --rfc2136-tsig-secret-alg=hmac-sha256\n        - --rfc2136-tsig-keyname=externaldns-key\n        - --rfc2136-tsig-axfr\n        - --source=ingress\n        - --domain-filter=k8s.example.org\n</code></pre>"},{"location":"docs/tutorials/rfc2136/#microsoft-dns","title":"Microsoft DNS","text":"<p>While <code>external-dns</code> was not developed or tested against Microsoft DNS, it can be configured to work against it. YMMV.</p>"},{"location":"docs/tutorials/rfc2136/#secure-updates-using-rfc3645-gss-tsig","title":"Secure Updates Using RFC3645 (GSS-TSIG)","text":""},{"location":"docs/tutorials/rfc2136/#dns-side-configuration","title":"DNS-side configuration","text":"<ol> <li>Create a DNS zone</li> <li>Enable secure dynamic updates for the zone</li> <li>Enable Zone Transfers to all servers and/or other domains</li> <li>Create a user with permissions to create/update/delete records in that zone</li> </ol> <p>If you see any error messages which indicate that <code>external-dns</code> was somehow not able to fetch existing DNS records from your DNS server, this could mean that you forgot about step 3.</p>"},{"location":"docs/tutorials/rfc2136/#kerberos-configuration","title":"Kerberos Configuration","text":"<p>DNS with secure updates relies upon a valid Kerberos configuration running within the <code>external-dns</code> container. At this time, you will need to create a ConfigMap for the <code>external-dns</code> container to use and mount it in your deployment. Below is an example of a working Kerberos configuration inside a ConfigMap definition.  This may be different depending on many factors in your environment:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  creationTimestamp: null\n  name: krb5.conf\ndata:\n  krb5.conf: |\n    [logging]\n    default = FILE:/var/log/krb5libs.log\n    kdc = FILE:/var/log/krb5kdc.log\n    admin_server = FILE:/var/log/kadmind.log\n\n    [libdefaults]\n    dns_lookup_realm = false\n    ticket_lifetime = 24h\n    renew_lifetime = 7d\n    forwardable = true\n    rdns = false\n    pkinit_anchors = /etc/pki/tls/certs/ca-bundle.crt\n    default_ccache_name = KEYRING:persistent:%{uid}\n\n    default_realm = YOUR-REALM.COM\n\n    [realms]\n    YOUR-REALM.COM = {\n      kdc = dc1.yourdomain.com\n      admin_server = dc1.yourdomain.com\n    }\n\n    [domain_realm]\n    yourdomain.com = YOUR-REALM.COM\n    .yourdomain.com = YOUR-REALM.COM\n</code></pre> <p>In most cases, the realm name will probably be the same as the domain name, so you can simply replace <code>YOUR-REALM.COM</code> with something like <code>YOURDOMAIN.COM</code>.</p> <p>Once the ConfigMap is created, the container <code>external-dns</code> container needs to be told to mount that ConfigMap as a volume at the default Kerberos configuration location.  The pod spec should include a similar configuration to the following:</p> <pre><code>...\n    volumeMounts:\n    - mountPath: /etc/krb5.conf\n      name: kerberos-config-volume\n      subPath: krb5.conf\n...\n  volumes:\n  - configMap:\n      defaultMode: 420\n      name: krb5.conf\n    name: kerberos-config-volume\n...\n</code></pre>"},{"location":"docs/tutorials/rfc2136/#external-dns-configuration","title":"<code>external-dns</code> configuration","text":"<p>You\u2019ll want to configure <code>external-dns</code> similarly to the following:</p> <pre><code>...\n        - --provider=rfc2136\n        - --rfc2136-gss-tsig\n        - --rfc2136-host=dns-host.yourdomain.com\n        - --rfc2136-port=53\n        - --rfc2136-zone=your-zone.com\n        - --rfc2136-zone=your-secondary-zone.com\n        - --rfc2136-kerberos-username=your-domain-account\n        - --rfc2136-kerberos-password=your-domain-password\n        - --rfc2136-kerberos-realm=your-domain.com\n        - --rfc2136-tsig-axfr # needed to enable zone transfers, which is required for deletion of records.\n...\n</code></pre> <p>As noted above, the <code>--rfc2136-kerberos-realm</code> flag is completely optional and won\u2019t be necessary in many cases. Most likely, you will only need it if you see errors similar to this: <code>KRB Error: (68) KDC_ERR_WRONG_REALM Reserved for future use</code>.</p> <p>The flag <code>--rfc2136-host</code> can be set to the host\u2019s domain name or IP address. However, it also determines the name of the Kerberos principal which is used during authentication. This means that Active Directory might only work if this is set to a specific domain name, possibly leading to errors like this: <code>KDC_ERR_S_PRINCIPAL_UNKNOWN Server not found in Kerberos database</code>. To fix this, try setting <code>--rfc2136-host</code> to the \u201cactual\u201d hostname of your DNS server.</p>"},{"location":"docs/tutorials/rfc2136/#insecure-updates","title":"Insecure Updates","text":""},{"location":"docs/tutorials/rfc2136/#dns-side-configuration_1","title":"DNS-side configuration","text":"<ol> <li>Create a DNS zone</li> <li>Enable insecure dynamic updates for the zone</li> <li>Enable Zone Transfers to all servers and/or other domains</li> </ol>"},{"location":"docs/tutorials/rfc2136/#external-dns-configuration_1","title":"<code>external-dns</code> configuration","text":"<p>You\u2019ll want to configure <code>external-dns</code> similarly to the following:</p> <pre><code>...\n        - --provider=rfc2136\n        - --rfc2136-host=192.168.0.1\n        - --rfc2136-port=53\n        - --rfc2136-zone=k8s.example.org\n        - --rfc2136-zone=k8s.your-zone.org\n        - --rfc2136-insecure\n        - --rfc2136-tsig-axfr # needed to enable zone transfers, which is required for deletion of records.\n...\n</code></pre>"},{"location":"docs/tutorials/rfc2136/#dns-over-tls-rfcs-7858-and-9103","title":"DNS Over TLS (RFCs 7858 and 9103)","text":"<p>If your DNS server does zone transfers over TLS, you can instruct <code>external-dns</code> to connect over TLS with the following flags:</p> <ul> <li><code>--rfc2136-use-tls</code> Will enable TLS for both zone transfers and for updates.</li> <li><code>--tls-ca=&lt;cert-file&gt;</code> Is the path to a file containing certificate(s) that can be used to verify the DNS server</li> <li><code>--tls-client-cert=&lt;client-cert-file&gt;</code> and</li> <li><code>--tls-client-cert-key=&lt;client-key-file&gt;</code> Set the client certificate and key for mutual verification</li> <li><code>--rfc2136-skip-tls-verify</code> Disables verification of the certificate supplied by the DNS server.</li> </ul> <p>It is currently not supported to do only zone transfers over TLS, but not the updates. They are enabled and disabled together.</p>"},{"location":"docs/tutorials/rfc2136/#configuring-rfc2136-provider-with-multiple-hosts-and-load-balancing","title":"Configuring RFC2136 Provider with Multiple Hosts and Load Balancing","text":"<p>This section describes how to configure the RFC2136 provider in ExternalDNS to support multiple DNS servers and load balancing options.</p>"},{"location":"docs/tutorials/rfc2136/#enhancements-overview","title":"Enhancements Overview","text":"<p>The RFC2136 provider now supports multiple DNS hosts and introduces load balancing options to distribute DNS update requests evenly across available DNS servers. This helps prevent a single server from becoming a bottleneck in environments with multiple DNS servers.</p>"},{"location":"docs/tutorials/rfc2136/#configuration-steps","title":"Configuration Steps","text":"<ol> <li> <p>Allow Multiple Hosts for <code>--rfc2136-host</code></p> <ul> <li>Modify the <code>--rfc2136-host</code> command-line option to accept multiple hosts.</li> <li>Example: <code>--rfc2136-host=\"dns-host-1.yourdomain.com\" --rfc2136-host=\"dns-host-2.yourdomain.com\"</code></li> </ul> </li> <li> <p>Introduce Load Balancing Options</p> <ul> <li>Add a new command-line option <code>--rfc2136-load-balancing-strategy</code> to specify the load balancing strategy.</li> <li>Supported options:<ul> <li><code>round-robin</code>: Distributes DNS updates evenly across all specified hosts in a round-robin manner.</li> <li><code>random</code>: Randomly selects a host for each DNS update.</li> <li><code>disabled</code> (default): Uses the first host in the list as the primary, only moving to the next host if a failure occurs.</li> </ul> </li> </ul> </li> </ol>"},{"location":"docs/tutorials/rfc2136/#example-configuration","title":"Example Configuration","text":"<pre><code>external-dns \\\n  --provider=rfc2136 \\\n  --rfc2136-host=\"dns-host-1.yourdomain.com\" \\\n  --rfc2136-host=\"dns-host-2.yourdomain.com\" \\\n  --rfc2136-host=\"dns-host-3.yourdomain.com\" \\\n  --rfc2136-load-balancing-strategy=\"round-robin\" \\\n  --rfc2136-port=53 \\\n  --rfc2136-zone=example.com \\\n  --rfc2136-tsig-secret-alg=hmac-sha256 \\\n  --rfc2136-tsig-keyname=example-key \\\n  --rfc2136-tsig-secret=example-secret \\\n  --rfc2136-insecure\n</code></pre>"},{"location":"docs/tutorials/rfc2136/#benefits","title":"Benefits","text":"<ul> <li>Distributes the load of DNS updates across multiple data centers, preventing any single DC from becoming a bottleneck.</li> <li>Provides flexibility to choose different load balancing strategies based on the environment and requirements.</li> <li>Improves the resilience and reliability of DNS updates by introducing a retry mechanism with a list of hosts.</li> </ul>"},{"location":"docs/tutorials/scaleway/","title":"Scaleway","text":"<p>This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using Scaleway DNS.</p> <p>Make sure to use &gt;=0.7.4 version of ExternalDNS for this tutorial.</p> <p>Warning: Scaleway DNS is currently in Public Beta and may not be suited for production usage.</p>"},{"location":"docs/tutorials/scaleway/#importing-a-domain-into-scaleway-dns","title":"Importing a Domain into Scaleway DNS","text":"<p>In order to use your domain, you need to import it into Scaleway DNS. If it\u2019s not already done, you can follow this documentation</p> <p>Once the domain is imported you can either use the root zone, or create a subzone to use.</p> <p>In this example we will use <code>example.com</code> as an example.</p>"},{"location":"docs/tutorials/scaleway/#creating-scaleway-credentials","title":"Creating Scaleway Credentials","text":"<p>To use ExternalDNS with Scaleway DNS, you need to create an API token (composed of the Access Key and the Secret Key). You can either use existing ones or you can create a new token, as explained in How to generate an API token or directly by going to the credentials page.</p> <p>Scaleway provider supports configuring credentials using profiles or supplying it directly with environment variables.</p>"},{"location":"docs/tutorials/scaleway/#configuration-using-a-config-file","title":"Configuration using a config file","text":"<p>You can supply the credentials through a config file:</p> <ol> <li>Create the config file. Check out Scaleway docs for instructions</li> <li>Mount it as a Secret into the Pod</li> <li>Configure environment variable <code>SCW_PROFILE</code> to match the profile name in the config file</li> <li>Configure environment variable <code>SCW_CONFIG_PATH</code> to match the location of the mounted config file</li> </ol>"},{"location":"docs/tutorials/scaleway/#configuration-using-environment-variables","title":"Configuration using environment variables","text":"<p>Two environment variables are needed to run ExternalDNS with Scaleway DNS:</p> <ul> <li><code>SCW_ACCESS_KEY</code> which is the Access Key.</li> <li><code>SCW_SECRET_KEY</code> which is the Secret Key.</li> </ul>"},{"location":"docs/tutorials/scaleway/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Connect your <code>kubectl</code> client to the cluster you want to test ExternalDNS with. Then apply one of the following manifests file to deploy ExternalDNS.</p> <p>The following example are suited for development. For a production usage, prefer secrets over environment, and use a tagged release.</p>"},{"location":"docs/tutorials/scaleway/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: external-dns\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=scaleway\n        env:\n        - name: SCW_ACCESS_KEY\n          value: \"&lt;your access key&gt;\"\n        - name: SCW_SECRET_KEY\n          value: \"&lt;your secret key&gt;\"\n        ### Set if configuring using a config file. Make sure to create the Secret first.\n        # - name: SCW_PROFILE\n        #   value: \"&lt;profile name&gt;\"\n        # - name: SCW_CONFIG_PATH\n        #   value: /etc/scw/config.yaml\n    #     volumeMounts:\n    #     - name: scw-config\n    #       mountPath: /etc/scw/config.yaml\n    #       readOnly: true\n    # volumes:\n    # - name: scw-config\n    #   secret:\n    #     secretName: scw-config\n    ###\n</code></pre>"},{"location":"docs/tutorials/scaleway/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\",\"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: external-dns\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains; change to match the zone created above.\n        - --provider=scaleway\n        env:\n        - name: SCW_ACCESS_KEY\n          value: \"&lt;your access key&gt;\"\n        - name: SCW_SECRET_KEY\n          value: \"&lt;your secret key&gt;\"\n        ### Set if configuring using a config file. Make sure to create the Secret first.\n        # - name: SCW_PROFILE\n        #   value: \"&lt;profile name&gt;\"\n        # - name: SCW_CONFIG_PATH\n        #   value: /etc/scw/config.yaml\n    #     volumeMounts:\n    #     - name: scw-config\n    #       mountPath: /etc/scw/config.yaml\n    #       readOnly: true\n    # volumes:\n    # - name: scw-config\n    #   secret:\n    #     secretName: scw-config\n    ###\n</code></pre>"},{"location":"docs/tutorials/scaleway/#deploying-an-nginx-service","title":"Deploying an Nginx Service","text":"<p>Create a service file called \u2018nginx.yaml\u2019 with the following contents:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: my-app.example.com\nspec:\n  selector:\n    app: nginx\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre> <p>Note the annotation on the service; use the same hostname as the Scaleway DNS zone created above.</p> <p>ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records.</p> <p>Create the deployment and service:</p> <pre><code>kubectl create -f nginx.yaml\n</code></pre> <p>Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service.</p> <p>Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the Scaleway DNS records.</p>"},{"location":"docs/tutorials/scaleway/#verifying-scaleway-dns-records","title":"Verifying Scaleway DNS records","text":"<p>Check your Scaleway DNS UI to view the records for your Scaleway DNS zone.</p> <p>Click on the zone for the one created above if a different domain was used.</p> <p>This should show the external IP address of the service as the A record for your domain.</p>"},{"location":"docs/tutorials/scaleway/#cleanup","title":"Cleanup","text":"<p>Now that we have verified that ExternalDNS will automatically manage Scaleway DNS records, we can delete the tutorial\u2019s example:</p> <pre><code>kubectl delete service -f nginx.yaml\nkubectl delete service -f externaldns.yaml\n</code></pre>"},{"location":"docs/tutorials/security-context/","title":"Running ExternalDNS with limited privileges","text":"<p>You can run ExternalDNS with reduced privileges since <code>v0.5.6</code> using the following <code>SecurityContext</code>.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - ... # your arguments here\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 65534\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop: [\"ALL\"]\n</code></pre>"},{"location":"docs/tutorials/tencentcloud/","title":"Tencent Cloud","text":""},{"location":"docs/tutorials/tencentcloud/#external-dns-version","title":"External Dns Version","text":"<ul> <li>Make sure to use &gt;=0.13.1 version of ExternalDNS for this tutorial</li> </ul>"},{"location":"docs/tutorials/tencentcloud/#set-up-privatedns-or-dnspod","title":"Set up PrivateDns or DNSPod","text":"<p>Tencent Cloud DNSPod Service is the domain name resolution and management service for public access. Tencent Cloud PrivateDNS Service is the domain name resolution and management service for VPC internal access.</p> <ul> <li>If you want to use internal dns service in Tencent Cloud.</li> </ul> <ol> <li>Set up the args <code>--tencent-cloud-zone-type=private</code></li> <li>Create a DNS domain in PrivateDNS console. DNS domain which will contain the managed DNS records.</li> </ol> <ul> <li>If you want to use public dns service in Tencent Cloud.</li> </ul> <ol> <li>Set up the args <code>--tencent-cloud-zone-type=public</code></li> <li>Create a Domain in DnsPod console. DNS domain which will contain the managed DNS records.</li> </ol>"},{"location":"docs/tutorials/tencentcloud/#set-up-cam-for-api-key","title":"Set up CAM for API Key","text":"<p>In Tencent CAM Console. you may get the secretId and secretKey pair. make sure the key pair has those Policy.</p> <pre><code>{\n    \"version\": \"2.0\",\n    \"statement\": [\n        {\n            \"effect\": \"allow\",\n            \"action\": [\n                \"dnspod:ModifyRecord\",\n                \"dnspod:DeleteRecord\",\n                \"dnspod:CreateRecord\",\n                \"dnspod:DescribeRecordList\",\n                \"dnspod:DescribeDomainList\"\n            ],\n            \"resource\": [\n                \"*\"\n            ]\n        },\n        {\n            \"effect\": \"allow\",\n            \"action\": [\n                \"privatedns:DescribePrivateZoneList\",\n                \"privatedns:DescribePrivateZoneRecordList\",\n                \"privatedns:CreatePrivateZoneRecord\",\n                \"privatedns:DeletePrivateZoneRecord\",\n                \"privatedns:ModifyPrivateZoneRecord\"\n            ],\n            \"resource\": [\n                \"*\"\n            ]\n        }\n    ]\n}\n</code></pre>"},{"location":"docs/tutorials/tencentcloud/#deploy-externaldns","title":"Deploy ExternalDNS","text":""},{"location":"docs/tutorials/tencentcloud/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: external-dns\ndata:\n  tencent-cloud.json: |\n    {\n      \"regionId\": \"ap-shanghai\",\n      \"secretId\": \"******\",\n      \"secretKey\": \"******\",\n      \"vpcId\": \"vpc-******\",\n      \"internetEndpoint\": false  # Default: false. Access the Tencent API through the intranet. If you need to deploy on the public network, you need to change to true\n    }\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - args:\n        - --source=service\n        - --source=ingress\n        - --domain-filter=external-dns-test.com # will make ExternalDNS see only the hosted zones matching provided domain, omit to process all available hosted zones\n        - --provider=tencentcloud\n        - --policy=sync # set `upsert-only` would prevent ExternalDNS from deleting any records\n        - --tencent-cloud-zone-type=private # only look at private hosted zones. set `public` to use the public dns service.\n        - --tencent-cloud-config-file=/etc/kubernetes/tencent-cloud.json\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        imagePullPolicy: Always\n        name: external-dns\n        resources: {}\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        volumeMounts:\n        - mountPath: /etc/kubernetes\n          name: config-volume\n          readOnly: true\n      dnsPolicy: ClusterFirst\n      hostAliases:\n      - hostnames:\n        - privatedns.internal.tencentcloudapi.com\n        - dnspod.internal.tencentcloudapi.com\n        ip: 169.254.0.95\n      restartPolicy: Always\n      schedulerName: default-scheduler\n      securityContext: {}\n      serviceAccount: external-dns\n      serviceAccountName: external-dns\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: tencent-cloud.json\n            path: tencent-cloud.json\n          name: external-dns\n        name: config-volume\n</code></pre>"},{"location":"docs/tutorials/tencentcloud/#example","title":"Example","text":""},{"location":"docs/tutorials/tencentcloud/#service","title":"Service","text":"<pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: nginx.external-dns-test.com\n    external-dns.alpha.kubernetes.io/internal-hostname: nginx-internal.external-dns-test.com\n    external-dns.alpha.kubernetes.io/ttl: \"600\"\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 80\n    name: http\n    targetPort: 80\n  selector:\n    app: nginx\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n          name: http\n</code></pre> <p><code>nginx.external-dns-test.com</code> will record to the Loadbalancer VIP. <code>nginx-internal.external-dns-test.com</code> will record to the ClusterIP. all of the DNS Record ttl will be 600.</p> <p>[!WARNING] This makes ExternalDNS safe for running in environments where there are other records managed via other means.</p>"},{"location":"docs/tutorials/transip/","title":"TransIP","text":"<p>This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using TransIP.</p> <p>Make sure to use &gt;=0.5.14 version of ExternalDNS for this tutorial, have at least 1 domain registered at TransIP and enabled the API.</p>"},{"location":"docs/tutorials/transip/#enable-transip-api-and-prepare-your-api-key","title":"Enable TransIP API and prepare your API key","text":"<p>To use the TransIP API you need an account at TransIP and enable API usage as described in the knowledge base. With the private key generated by the API, we create a kubernetes secret:</p> <pre><code>kubectl create secret generic transip-api-key --from-file=transip-api-key=/path/to/private.key\n</code></pre>"},{"location":"docs/tutorials/transip/#deploy-externaldns","title":"Deploy ExternalDNS","text":"<p>Below are example manifests, for both cluster without or with RBAC enabled. Don\u2019t forget to replace <code>YOUR_TRANSIP_ACCOUNT_NAME</code> with your TransIP account name. In these examples, an example domain-filter is defined. Such a filter can be used to prevent ExternalDNS from touching any domain not listed in the filter. Refer to the docs for any other command-line parameters you might want to use.</p>"},{"location":"docs/tutorials/transip/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains\n        - --provider=transip\n        - --transip-account=YOUR_TRANSIP_ACCOUNT_NAME\n        - --transip-keyfile=/transip/transip-api-key\n        volumeMounts:\n        - mountPath: /transip\n          name: transip-api-key\n          readOnly: true\n      volumes:\n      - name: transip-api-key\n        secret:\n          secretName: transip-api-key\n</code></pre>"},{"location":"docs/tutorials/transip/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\",\"networking.k8s.io\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"watch\", \"list\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service # ingress is also possible\n        - --domain-filter=example.com # (optional) limit to only example.com domains\n        - --provider=transip\n        - --transip-account=YOUR_TRANSIP_ACCOUNT_NAME\n        - --transip-keyfile=/transip/transip-api-key\n        volumeMounts:\n        - mountPath: /transip\n          name: transip-api-key\n          readOnly: true\n      volumes:\n      - name: transip-api-key\n        secret:\n          secretName: transip-api-key\n</code></pre>"},{"location":"docs/tutorials/transip/#deploying-an-nginx-service","title":"Deploying an Nginx Service","text":"<p>Create a service file called \u2018nginx.yaml\u2019 with the following contents:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: my-app.example.com\nspec:\n  selector:\n    app: nginx\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre> <p>Note the annotation on the service; this is the name ExternalDNS will create and manage DNS records for.</p> <p>ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records.</p> <p>Create the deployment and service:</p> <pre><code>kubectl create -f nginx.yaml\n</code></pre> <p>Depending where you run your service it can take a little while for your cloud provider to create an external IP for the service.</p> <p>Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and synchronize the TransIP DNS records.</p>"},{"location":"docs/tutorials/transip/#verifying-transip-dns-records","title":"Verifying TransIP DNS records","text":"<p>Check your TransIP Control Panel to view the records for your TransIP DNS zone.</p> <p>Click on the zone for the one created above if a different domain was used.</p> <p>This should show the external IP address of the service as the A record for your domain.</p>"},{"location":"docs/tutorials/ultradns/","title":"UltraDNS","text":"<p>This tutorial describes how to setup ExternalDNS for usage within a Kubernetes cluster using UltraDNS.</p> <p>For this tutorial, please make sure that you are using a version &gt; 0.7.2 of ExternalDNS.</p>"},{"location":"docs/tutorials/ultradns/#managing-dns-with-ultradns","title":"Managing DNS with UltraDNS","text":"<p>If you would like to read-up on the UltraDNS service, you can find additional details here: Introduction to UltraDNS</p> <p>Before proceeding, please create a new DNS Zone that you will create your records in for this tutorial process. For the examples in this tutorial, we will be using <code>example.com</code> as our Zone.</p>"},{"location":"docs/tutorials/ultradns/#setting-up-ultradns-credentials","title":"Setting Up UltraDNS Credentials","text":"<p>The following environment variables will be needed to run ExternalDNS with UltraDNS.</p> <p><code>ULTRADNS_USERNAME</code>,<code>ULTRADNS_PASSWORD</code>, &amp;<code>ULTRADNS_BASEURL</code> <code>ULTRADNS_ACCOUNTNAME</code>(optional variable).</p>"},{"location":"docs/tutorials/ultradns/#deploying-externaldns","title":"Deploying ExternalDNS","text":"<p>Connect your <code>kubectl</code> client to the cluster you want to test ExternalDNS with. Then, apply one of the following manifests file to deploy ExternalDNS.</p> <ul> <li>Note: We are assuming the zone is already present within UltraDNS.</li> <li>Note: While creating CNAMES as target endpoints, the <code>--txt-prefix</code> option is mandatory.</li> </ul>"},{"location":"docs/tutorials/ultradns/#manifest-for-clusters-without-rbac-enabled","title":"Manifest (for clusters without RBAC enabled)","text":"<pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service\n        - --source=ingress # ingress is also possible\n        - --domain-filter=example.com # (Recommended) We recommend to use this filter as it minimize the time to propagate changes, as there are less number of zones to look into..\n        - --provider=ultradns\n        - --txt-prefix=txt-\n        env:\n        - name: ULTRADNS_USERNAME\n          value: \"\"\n        - name: ULTRADNS_PASSWORD  # The password is required to be BASE64 encrypted.\n          value: \"\"\n        - name: ULTRADNS_BASEURL\n          value: \"https://api.ultradns.com/\"\n        - name: ULTRADNS_ACCOUNTNAME\n          value: \"\"\n</code></pre>"},{"location":"docs/tutorials/ultradns/#manifest-for-clusters-with-rbac-enabled","title":"Manifest (for clusters with RBAC enabled)","text":"<pre><code>apiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: external-dns\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: external-dns\nrules:\n- apiGroups: [\"\"]\n  resources: [\"services\",\"endpoints\",\"pods\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"extensions\"]\n  resources: [\"ingresses\"]\n  verbs: [\"get\",\"watch\",\"list\"]\n- apiGroups: [\"\"]\n  resources: [\"nodes\"]\n  verbs: [\"list\",\"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: external-dns-viewer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: external-dns\nsubjects:\n- kind: ServiceAccount\n  name: external-dns\n  namespace: default\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: external-dns\nspec:\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: external-dns\n  template:\n    metadata:\n      labels:\n        app: external-dns\n    spec:\n      serviceAccountName: external-dns\n      containers:\n      - name: external-dns\n        image: registry.k8s.io/external-dns/external-dns:v0.15.1\n        args:\n        - --source=service\n        - --source=ingress\n        - --domain-filter=example.com #(Recommended) We recommend to use this filter as it minimize the time to propagate changes, as there are less number of zones to look into..\n        - --provider=ultradns\n        - --txt-prefix=txt-\n        env:\n        - name: ULTRADNS_USERNAME\n          value: \"\"\n        - name: ULTRADNS_PASSWORD # The password is required to be BASE64 encrypted.\n          value: \"\"\n        - name: ULTRADNS_BASEURL\n          value: \"https://api.ultradns.com/\"\n        - name: ULTRADNS_ACCOUNTNAME\n          value: \"\"\n</code></pre>"},{"location":"docs/tutorials/ultradns/#deploying-an-nginx-service","title":"Deploying an Nginx Service","text":"<p>Create a service file called \u2018nginx.yaml\u2019 with the following contents:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: my-app.example.com.\nspec:\n  selector:\n    app: nginx\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n</code></pre> <p>Please note the annotation on the service. Use the same hostname as the UltraDNS zone created above.</p> <p>ExternalDNS uses this annotation to determine what services should be registered with DNS. Removing the annotation will cause ExternalDNS to remove the corresponding DNS records.</p>"},{"location":"docs/tutorials/ultradns/#creating-the-deployment-and-service","title":"Creating the Deployment and Service","text":"<pre><code>kubectl create -f nginx.yaml\nkubectl create -f external-dns.yaml\n</code></pre> <p>Depending on where you run your service from, it can take a few minutes for your cloud provider to create an external IP for the service.</p> <p>Once the service has an external IP assigned, ExternalDNS will notice the new service IP address and will synchronize the UltraDNS records.</p>"},{"location":"docs/tutorials/ultradns/#verifying-ultradns-records","title":"Verifying UltraDNS Records","text":"<p>Please verify on the UltraDNS UI that the records are created under the zone \u201cexample.com\u201d.</p> <p>For more information on UltraDNS UI, refer to (https://docs.ultradns.com/Content/MSP_User_Guide/Content/User%20Guides/MSP_User_Guide/Navigation/Moving%20Around%20the%20UI.htm#_Toc2780722).</p> <p>Select the zone that was created above (or select the appropriate zone if a different zone was used.)</p> <p>The external IP address will be displayed as a CNAME record for your zone.</p>"},{"location":"docs/tutorials/ultradns/#cleaning-up-the-deployment-and-service","title":"Cleaning Up the Deployment and Service","text":"<p>Now that we have verified that ExternalDNS will automatically manage your UltraDNS records, you can delete example zones that you created in this tutorial:</p> <pre><code>kubectl delete service -f nginx.yaml\nkubectl delete service -f externaldns.yaml\n</code></pre>"},{"location":"docs/tutorials/ultradns/#examples-to-manage-your-records","title":"Examples to Manage your Records","text":""},{"location":"docs/tutorials/ultradns/#creating-multiple-a-records-target","title":"Creating Multiple A Records Target","text":"<ul> <li>First, you want to create a service file called \u2018apple-banana-echo.yaml\u2019</li> </ul> <pre><code>---\nkind: Pod\napiVersion: v1\nmetadata:\n  name: example-app\n  labels:\n    app: apple\nspec:\n  containers:\n    - name: example-app\n      image: hashicorp/http-echo\n      args:\n        - \"-text=apple\"\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: example-service\nspec:\n  selector:\n    app: apple\n  ports:\n    - port: 5678 # Default port for image\n</code></pre> <ul> <li>Then, create service file called \u2018expose-apple-banana-app.yaml\u2019 to expose the services. For more information to deploy ingress controller, refer to (https://kubernetes.github.io/ingress-nginx/deploy/)</li> </ul> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  annotations:\n    ingress.kubernetes.io/rewrite-target: /\n    ingress.kubernetes.io/scheme: internet-facing\n    external-dns.alpha.kubernetes.io/hostname: apple.example.com.\n    external-dns.alpha.kubernetes.io/target: 10.10.10.1,10.10.10.23\nspec:\n  rules:\n  - http:\n      paths:\n        - path: /apple\n          pathType: Prefix\n          backend:\n            service:\n              name: example-service\n              port:\n                number: 5678\n</code></pre> <ul> <li>Then, create the deployment and service:</li> </ul> <pre><code>kubectl create -f apple-banana-echo.yaml\nkubectl create -f expose-apple-banana-app.yaml\nkubectl create -f external-dns.yaml\n</code></pre> <ul> <li>Depending on where you run your service from, it can take a few minutes for your cloud provider to create an external IP for the service.</li> <li>Please verify on the UltraDNS UI that the records have been created under the zone \u201cexample.com\u201d.</li> <li>Finally, you will need to clean up the deployment and service. Please verify on the UI afterwards that the records have been deleted from the zone \u201cexample.com\u201d:</li> </ul> <pre><code>kubectl delete -f apple-banana-echo.yaml\nkubectl delete -f expose-apple-banana-app.yaml\nkubectl delete -f external-dns.yaml\n</code></pre>"},{"location":"docs/tutorials/ultradns/#creating-cname-record","title":"Creating CNAME Record","text":"<ul> <li>Please note, that prior to deploying the external-dns service, you will need to add the option \u2013txt-prefix=txt- into external-dns.yaml. If this not provided, your records will not be created.</li> <li>First, create a service file called \u2018apple-banana-echo.yaml\u2019</li> <li> <p>Config File Example \u2013 kubernetes cluster is on-premise not on cloud</p> <pre><code>---\nkind: Pod\napiVersion: v1\nmetadata:\n  name: example-app\n  labels:\n    app: apple\nspec:\n  containers:\n    - name: example-app\n      image: hashicorp/http-echo\n      args:\n        - \"-text=apple\"\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: example-service\nspec:\n  selector:\n    app: apple\n  ports:\n    - port: 5678 # Default port for image\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  annotations:\n    ingress.kubernetes.io/rewrite-target: /\n    ingress.kubernetes.io/scheme: internet-facing\n    external-dns.alpha.kubernetes.io/hostname: apple.example.com.\n    external-dns.alpha.kubernetes.io/target: apple.cname.com.\nspec:\n  rules:\n  - http:\n      paths:\n        - path: /apple\n          backend:\n            service:\n              name: example-service\n              port:\n                number: 5678\n</code></pre> </li> <li> <p>Config File Example \u2013 Kubernetes cluster service from different cloud vendors</p> <pre><code>---\nkind: Pod\napiVersion: v1\nmetadata:\n  name: example-app\n  labels:\n    app: apple\nspec:\n  containers:\n    - name: example-app\n      image: hashicorp/http-echo\n      args:\n        - \"-text=apple\"\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: example-service\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: my-app.example.com.\nspec:\n  selector:\n    app: apple\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 5678\n      targetPort: 5678\n</code></pre> </li> <li> <p>Then, create the deployment and service:</p> </li> </ul> <pre><code>kubectl create -f apple-banana-echo.yaml\nkubectl create -f external-dns.yaml\n</code></pre> <ul> <li>Depending on where you run your service from, it can take a few minutes for your cloud provider to create an external IP for the service.</li> <li>Please verify on the UltraDNS UI, that the records have been created under the zone \u201cexample.com\u201d.</li> <li>Finally, you will need to clean up the deployment and service. Please verify on the UI afterwards that the records have been deleted from the zone \u201cexample.com\u201d:</li> </ul> <pre><code>kubectl delete -f apple-banana-echo.yaml\nkubectl delete -f external-dns.yaml\n</code></pre>"},{"location":"docs/tutorials/ultradns/#creating-multiple-types-of-records","title":"Creating Multiple Types Of Records","text":"<ul> <li>Please note, that prior to deploying the external-dns service, you will need to add the option \u2013txt-prefix=txt- into external-dns.yaml. Since you will also be created a CNAME record, If this not provided, your records will not be created.</li> <li>First, create a service file called \u2018apple-banana-echo.yaml\u2019</li> <li> <p>Config File Example \u2013 kubernetes cluster is on-premise not on cloud</p> <pre><code>---\nkind: Pod\napiVersion: v1\nmetadata:\n  name: example-app\n  labels:\n    app: apple\nspec:\n  containers:\n    - name: example-app\n      image: hashicorp/http-echo\n      args:\n        - \"-text=apple\"\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: example-service\nspec:\n  selector:\n    app: apple\n  ports:\n    - port: 5678 # Default port for image\n---\nkind: Pod\napiVersion: v1\nmetadata:\n  name: example-app1\n  labels:\n    app: apple1\nspec:\n  containers:\n    - name: example-app1\n      image: hashicorp/http-echo\n      args:\n        - \"-text=apple\"\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: example-service1\nspec:\n  selector:\n    app: apple1\n  ports:\n    - port: 5679 # Default port for image\n---\nkind: Pod\napiVersion: v1\nmetadata:\n  name: example-app2\n  labels:\n    app: apple2\nspec:\n  containers:\n    - name: example-app2\n      image: hashicorp/http-echo\n      args:\n        - \"-text=apple\"\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: example-service2\nspec:\n  selector:\n    app: apple2\n  ports:\n    - port: 5680 # Default port for image\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  annotations:\n    ingress.kubernetes.io/rewrite-target: /\n    ingress.kubernetes.io/scheme: internet-facing\n    external-dns.alpha.kubernetes.io/hostname: apple.example.com.\n    external-dns.alpha.kubernetes.io/target: apple.cname.com.\nspec:\n  rules:\n  - http:\n      paths:\n        - path: /apple\n          backend:\n            service:\n              name: example-service\n              port:\n                number: 5678\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress1\n  annotations:\n    ingress.kubernetes.io/rewrite-target: /\n    ingress.kubernetes.io/scheme: internet-facing\n    external-dns.alpha.kubernetes.io/hostname: apple-banana.example.com.\n    external-dns.alpha.kubernetes.io/target: 10.10.10.3\nspec:\n  rules:\n  - http:\n      paths:\n        - path: /apple\n          backend:\n            service:\n              name: example-service1\n              port:\n                number: 5679\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress2\n  annotations:\n    ingress.kubernetes.io/rewrite-target: /\n    ingress.kubernetes.io/scheme: internet-facing\n    external-dns.alpha.kubernetes.io/hostname: banana.example.com.\n    external-dns.alpha.kubernetes.io/target: 10.10.10.3,10.10.10.20\nspec:\n  rules:\n  - http:\n      paths:\n        - path: /apple\n          backend:\n            service:\n              name: example-service2\n              port:\n                number: 5680\n</code></pre> </li> <li> <p>Config File Example \u2013 Kubernetes cluster service from different cloud vendors</p> <pre><code>---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - image: nginx\n        name: nginx\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx\n  annotations:\n    external-dns.alpha.kubernetes.io/hostname: my-app.example.com.\nspec:\n  selector:\n    app: nginx\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 80\n---\nkind: Pod\napiVersion: v1\nmetadata:\n  name: example-app\n  labels:\n    app: apple\nspec:\n  containers:\n    - name: example-app\n      image: hashicorp/http-echo\n      args:\n        - \"-text=apple\"\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: example-service\nspec:\n  selector:\n    app: apple\n  ports:\n    - port: 5678 # Default port for image\n---\nkind: Pod\napiVersion: v1\nmetadata:\n  name: example-app1\n  labels:\n    app: apple1\nspec:\n  containers:\n    - name: example-app1\n      image: hashicorp/http-echo\n      args:\n        - \"-text=apple\"\n---\napiVersion: extensions/v1beta1\nkind: Service\napiVersion: v1\nmetadata:\n  name: example-service1\nspec:\n  selector:\n    app: apple1\n  ports:\n    - port: 5679 # Default port for image\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress\n  annotations:\n    ingress.kubernetes.io/rewrite-target: /\n    ingress.kubernetes.io/scheme: internet-facing\n    external-dns.alpha.kubernetes.io/hostname: apple.example.com.\n    external-dns.alpha.kubernetes.io/target: 10.10.10.3,10.10.10.25\nspec:\n  rules:\n  - http:\n      paths:\n        - path: /apple\n          backend:\n            service:\n              name: example-service\n              port:\n                number: 5678\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: example-ingress1\n  annotations:\n    ingress.kubernetes.io/rewrite-target: /\n    ingress.kubernetes.io/scheme: internet-facing\n    external-dns.alpha.kubernetes.io/hostname: apple-banana.example.com.\n    external-dns.alpha.kubernetes.io/target: 10.10.10.3\nspec:\n  rules:\n  - http:\n      paths:\n        - path: /apple\n          backend:\n            service:\n              name: example-service1\n              port:\n                number: 5679\n</code></pre> </li> <li> <p>Then, create the deployment and service:</p> </li> </ul> <pre><code>kubectl create -f apple-banana-echo.yaml\nkubectl create -f external-dns.yaml\n</code></pre> <ul> <li>Depending on where you run your service from, it can take a few minutes for your cloud provider to create an external IP for the service.</li> <li>Please verify on the UltraDNS UI, that the records have been created under the zone \u201cexample.com\u201d.</li> <li>Finally, you will need to clean up the deployment and service. Please verify on the UI afterwards that the records have been deleted from the zone \u201cexample.com\u201d:</li> </ul> <p><code>console kubectl delete -f apple-banana-echo.yaml kubectl delete -f external-dns.yaml</code></p>"},{"location":"docs/tutorials/webhook-provider/","title":"Webhook provider","text":"<p>The \u201cWebhook\u201d provider allows integrating ExternalDNS with DNS providers through an HTTP interface. The Webhook provider implements the <code>Provider</code> interface. Instead of implementing code specific to a provider, it implements an HTTP client that sends requests to an HTTP API. The idea behind it is that providers can be implemented in separate programs: these programs expose an HTTP API that the Webhook provider interacts with. The ideal setup for providers is to run as a sidecar in the same pod of the ExternalDNS container, listening only on localhost. This is not strictly a requirement, but we do not recommend other setups.</p>"},{"location":"docs/tutorials/webhook-provider/#architectural-diagram","title":"Architectural diagram","text":""},{"location":"docs/tutorials/webhook-provider/#api-guarantees","title":"API guarantees","text":"<p>Providers implementing the HTTP API have to keep in sync with changes to the JSON serialization of Go types <code>plan.Changes</code>, <code>endpoint.Endpoint</code>, and <code>endpoint.DomainFilter</code>. Given the maturity of the project, we do not expect to make significant changes to those types, but can\u2019t exclude the possibility that changes will need to happen. We commit to publishing changes to those in the release notes, to ensure that providers implementing the API can keep providers up to date quickly.</p>"},{"location":"docs/tutorials/webhook-provider/#implementation-requirements","title":"Implementation requirements","text":"<p>The following table represents the methods to implement mapped to their HTTP method and route.</p>"},{"location":"docs/tutorials/webhook-provider/#provider-endpoints","title":"Provider endpoints","text":"Provider method HTTP Method Route Description Negotiate GET / Negotiate <code>DomainFilter</code> Records GET /records Get records AdjustEndpoints POST /adjustendpoints Provider specific adjustments of records ApplyChanges POST /records Apply record <p>OpenAPI spec is here.</p> <p>ExternalDNS will also make requests to the <code>/</code> endpoint for negotiation and for deserialization of the <code>DomainFilter</code>.</p> <p>The server needs to respond to those requests by reading the <code>Accept</code> header and responding with a corresponding <code>Content-Type</code> header specifying the supported media type format and version.</p> <p>The default recommended port for the provider endpoints is <code>8888</code>, and should listen only on <code>localhost</code> (ie: only accessible for external-dns).</p> <p>NOTE: only <code>5xx</code> responses will be retried and only <code>20x</code> will be considered as successful. All status codes different from those will be considered a failure on ExternalDNS\u2019s side.</p>"},{"location":"docs/tutorials/webhook-provider/#exposed-endpoints","title":"Exposed endpoints","text":"Provider method HTTP Method Route Description K8s probe GET /healthz Used by <code>livenessProbe</code> and <code>readinessProbe</code> Open Metrics GET /metrics Optional endpoint to expose Open Metrics <p>The default recommended port for the exposed endpoints is <code>8080</code>, and it should be bound to all interfaces (<code>0.0.0.0</code>)</p>"},{"location":"docs/tutorials/webhook-provider/#custom-annotations","title":"Custom Annotations","text":"<p>The Webhook provider supports custom annotations for DNS records. This feature allows users to define additional configuration options for DNS records managed by the Webhook provider. Custom annotations are defined using the annotation format <code>external-dns.alpha.kubernetes.io/webhook-&lt;custom-annotation&gt;</code>.</p> <p>Custom annotations can be used to influence DNS record creation and updates. Providers implementing the Webhook API should document the custom annotations they support and how they affect DNS record management.</p>"},{"location":"docs/tutorials/webhook-provider/#provider-registry","title":"Provider registry","text":"<p>To simplify the discovery of providers, we will accept pull requests that will add links to providers in this documentation. This list will only serve the purpose of simplifying finding providers and will not constitute an official endorsement of any of the externally implemented providers unless otherwise stated.</p>"},{"location":"docs/tutorials/webhook-provider/#run-an-externaldns-in-tree-provider-as-a-webhook","title":"Run an ExternalDNS in-tree provider as a webhook","text":"<p>To test the Webhook provider and provide a reference implementation, we added the functionality to run ExternalDNS as a webhook. To run the AWS provider as a webhook, you need the following flags:</p> <pre><code>- --webhook-server\n- --provider=aws\n- --source=ingress\n</code></pre> <p>The value of the <code>--source</code> flag is ignored in this mode.</p> <p>This will start the AWS provider as an HTTP server exposed only on localhost. In a separate process/container, run ExternalDNS with <code>--provider=webhook</code>. This is the same setup that we recommend for other providers and a good way to test the Webhook provider.</p>"}]}